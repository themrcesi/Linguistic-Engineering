{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Índice<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-documentos\" data-toc-modified-id=\"Carga-de-documentos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Carga de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocesado\" data-toc-modified-id=\"Preprocesado-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preprocesado</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bigramas\" data-toc-modified-id=\"Bigramas-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Bigramas</a></span></li></ul></li></ul></li><li><span><a href=\"#Glosario\" data-toc-modified-id=\"Glosario-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-de-keywords\" data-toc-modified-id=\"Extracción-de-keywords-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extracción de keywords</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-propia\" data-toc-modified-id=\"Extracción-propia-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Extracción propia</a></span></li><li><span><a href=\"#Gensim\" data-toc-modified-id=\"Gensim-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Gensim</a></span></li><li><span><a href=\"#Kmeans\" data-toc-modified-id=\"Kmeans-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Kmeans</a></span></li></ul></li><li><span><a href=\"#Formación-del-glosario\" data-toc-modified-id=\"Formación-del-glosario-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Formación del glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automatizado\" data-toc-modified-id=\"Automatizado-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Automatizado</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificador\" data-toc-modified-id=\"Clasificador-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clasificador</a></span><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-glosarios\" data-toc-modified-id=\"Carga-de-glosarios-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Carga de glosarios</a></span></li><li><span><a href=\"#Bigramas-de-test-data\" data-toc-modified-id=\"Bigramas-de-test-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bigramas de test data</a></span></li><li><span><a href=\"#Modelos\" data-toc-modified-id=\"Modelos-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Naive Bayes</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3.3.3.1\"><span class=\"toc-item-num\">3.3.3.1&nbsp;&nbsp;</span>TFIDF</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Clasificación-de-documentos\" data-toc-modified-id=\"Clasificación-de-documentos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clasificación de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li><li><span><a href=\"#Clasificación\" data-toc-modified-id=\"Clasificación-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Clasificación</a></span></li></ul></li><li><span><a href=\"#Evaluación-de-modelos\" data-toc-modified-id=\"Evaluación-de-modelos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluación de modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:39.790333Z",
     "start_time": "2020-12-12T18:58:38.668332Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.summarization import keywords\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# Operatos\n",
    "from operator import itemgetter\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy_spanish_lemmatizer import SpacyCustomLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# utils\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:39.794333Z",
     "start_time": "2020-12-12T18:58:39.791333Z"
    }
   },
   "outputs": [],
   "source": [
    "path_health = \"../documents/health\"\n",
    "path_politics = \"../documents/politics\"\n",
    "path_sports = \"../documents/sports\"\n",
    "path_documents = \"../documents\"\n",
    "path_stopwords = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:39.799334Z",
     "start_time": "2020-12-12T18:58:39.795333Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    return path.split(\"\\\\\")[-1], open(path,encoding='utf-8').read(), path.split(\"\\\\\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:40.266332Z",
     "start_time": "2020-12-12T18:58:39.800333Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = Parallel(n_jobs = -1)(delayed(load_document)(path) for path in glob.glob(path_documents+\"/*/*.txt\"))\n",
    "documents = pd.DataFrame(documents, columns=[\"doc_name\", \"text\", \"class\"])\n",
    "documents['text'] = documents['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:40.278333Z",
     "start_time": "2020-12-12T18:58:40.267333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health\n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health\n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health\n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health\n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:41.185335Z",
     "start_time": "2020-12-12T18:58:40.279333Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\&)|(\\%)|(\\$)|(\\€)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "    \n",
    "nlp = spacy.load(\"es\")\n",
    "lemmatizer = SpacyCustomLemmatizer()\n",
    "\n",
    "def load_stopwords(path):\n",
    "    return [line.strip() for line in open(path_stopwords, encoding = \"utf-8\").readlines()]\n",
    "\n",
    "STOP_WORDS = set(load_stopwords(path_stopwords))\n",
    "\n",
    "def delete_stop_words(doc):\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return clean\n",
    "\n",
    "def preprocess_document(document):\n",
    "    document = REPLACE_NO_SPACE.sub(NO_SPACE, document.lower())\n",
    "    document = REPLACE_WITH_SPACE.sub(SPACE, document)\n",
    "    # tokens = wordpunct_tokenize(document)\n",
    "    # tokens = delete_proper_nouns(tokens)\n",
    "    return document\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tokens = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "# TODO: REVISAR ESTO\n",
    "\n",
    "def delete_proper_nouns(tokens):\n",
    "    # Tag the tokens with their type - ie are they nouns or not\n",
    "    lTokens = pos_tag(tokens)\n",
    "    # find all the proper nouns and print them out\n",
    "    lTagDict = findtags('NNP', lTokens)\n",
    "    return [token.lower() for token in tokens if token not in lTagDict]\n",
    "    \n",
    "def findtags(tag_prefix, tagged_text):\n",
    "    \"\"\"\n",
    "    Find tokens matching the specified tag_prefix\n",
    "    \"\"\"\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    print(cfd.conditions())\n",
    "    return [list(cfd[tag].keys()) for tag in cfd.conditions()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:41.519335Z",
     "start_time": "2020-12-12T18:58:41.186334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "      <td>aceptémoslo de una vez perder peso de manera r...</td>\n",
       "      <td>[aceptémoslo, perder, peso, rápida, indolora, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "      <td>sin tiempo para hacer recuento de daños irrump...</td>\n",
       "      <td>[recuento, daños, irrumpe, ola, virus, golpear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "      <td>mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>[gente, mostrar, redes, sociales, versión, fot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "      <td>una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>[faceta, clave, frenética, lucha, global, pfiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "      <td>la curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>[curva, contagios, coronavirus, mantiene, espa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class  \\\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health   \n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health   \n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health   \n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health   \n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health   \n",
       "\n",
       "                                        preprocesado  \\\n",
       "0  aceptémoslo de una vez perder peso de manera r...   \n",
       "1  sin tiempo para hacer recuento de daños irrump...   \n",
       "2  mucha gente intenta mostrar en las redes socia...   \n",
       "3  una faceta clave en la frenética lucha global ...   \n",
       "4  la curva de contagios de coronavirus se mantie...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [aceptémoslo, perder, peso, rápida, indolora, ...  \n",
       "1  [recuento, daños, irrumpe, ola, virus, golpear...  \n",
       "2  [gente, mostrar, redes, sociales, versión, fot...  \n",
       "3  [faceta, clave, frenética, lucha, global, pfiz...  \n",
       "4  [curva, contagios, coronavirus, mantiene, espa...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[\"preprocesado\"] = documents[\"text\"].apply(lambda x: preprocess_document(x))\n",
    "documents[\"tokens\"] = documents[\"preprocesado\"].apply(lambda x: delete_stop_words(x))\n",
    "# documents[\"lematizado\"] = documents[\"preprocesado\"].apply(lambda x: lemmatize(x))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:41.529333Z",
     "start_time": "2020-12-12T18:58:41.520334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ==> 45 documents\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_health = documents[documents[\"class\"] == \"health\"].iloc[:15]\n",
    "train_politics = documents[documents[\"class\"] == \"politics\"].iloc[:15]\n",
    "train_sports = documents[documents[\"class\"] == \"sports\"].iloc[:15]\n",
    "\n",
    "train_data = pd.concat([train_health, train_politics, train_sports])\n",
    "print(f\"Training data ==> {len(train_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:41.535334Z",
     "start_time": "2020-12-12T18:58:41.530334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data ==> 105 documents\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_health = documents[documents[\"class\"] == \"health\"].iloc[15:]\n",
    "test_politics = documents[documents[\"class\"] == \"politics\"].iloc[15:]\n",
    "test_sports = documents[documents[\"class\"] == \"sports\"].iloc[15:]\n",
    "\n",
    "test_data = pd.concat([test_health, test_politics, test_sports])\n",
    "test_data.reset_index(inplace = True)\n",
    "print(f\"Testing data ==> {len(test_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:41.540334Z",
     "start_time": "2020-12-12T18:58:41.536333Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(documents, threshold):\n",
    "    token_ = [doc.split(\" \") for doc in documents]\n",
    "    bigram = Phrases(token_, min_count=1, threshold=threshold, delimiter=b' ')\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    bigram_token = []\n",
    "    for sent in token_:\n",
    "        for bigram in bigram_phraser[sent]:\n",
    "            if len(bigram.split(\" \")) > 1: # comprobamos que realmente es un bigrama\n",
    "                bigram_token.append(bigram) \n",
    "    return bigram_token\n",
    "           \n",
    "def check_bigram(x, bigrams):\n",
    "    if x.find(\"jamón serrano\") != -1 or x.find(\"jamón\") != -1:\n",
    "        print(x)\n",
    "    return [bigram for bigram in bigrams if x.find(bigram) != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:42.401332Z",
     "start_time": "2020-12-12T18:58:41.541333Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_sports = get_bigrams(train_sports[\"preprocesado\"].values, 50)\n",
    "bigrams_health = get_bigrams(train_health[\"preprocesado\"].values, 50)\n",
    "bigrams_politics = get_bigrams(train_politics[\"preprocesado\"].values, 50)\n",
    "bigrams = get_bigrams(train_data[\"preprocesado\"].values, 50)\n",
    "\n",
    "\n",
    "train_sports[\"bigrams\"] = train_sports[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_sports))\n",
    "train_health[\"bigrams\"] = train_health[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_health))\n",
    "train_politics[\"bigrams\"] = train_politics[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_politics))\n",
    "\n",
    "train_sports[\"tokens + bigrams\"] = train_sports[\"tokens\"] + train_sports[\"bigrams\"]\n",
    "train_health[\"tokens + bigrams\"] = train_health[\"tokens\"] + train_health[\"bigrams\"]\n",
    "train_politics[\"tokens + bigrams\"] = train_politics[\"tokens\"] + train_politics[\"bigrams\"]\n",
    "\n",
    "train_data[\"bigrams\"] = train_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "train_data[\"tokens + bigrams\"] = train_data[\"tokens\"] + train_data[\"bigrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:42.405333Z",
     "start_time": "2020-12-12T18:58:42.402334Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_dir = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:42.413333Z",
     "start_time": "2020-12-12T18:58:42.406333Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_tfidf_keywords(df, k):\n",
    "    tokens = df[\"tokens + bigrams\"].values\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    bow = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    bow_tfidf = tfidf[bow]\n",
    "    tfidf_dic = {dictionary.get(id): value for doc in bow_tfidf for id, value in doc}\n",
    "    tfidf_list = [k for k, v in sorted(tfidf_dic.items(), key=lambda item: item[1], reverse = True)]\n",
    "    return tfidf_list[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:42.519332Z",
     "start_time": "2020-12-12T18:58:42.414334Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health = get_k_tfidf_keywords(train_health, 100)\n",
    "keywords_tfidf_politics = get_k_tfidf_keywords(train_politics, 100)\n",
    "keywords_tfidf_sports = get_k_tfidf_keywords(train_sports, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:42.523337Z",
     "start_time": "2020-12-12T18:58:42.520333Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(d1, d2, d3):\n",
    "\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    \n",
    "    for key in deleted:\n",
    "        try:\n",
    "            d1.remove(key)\n",
    "        except:\n",
    "            print(f\"D1 no tiene {key}\")\n",
    "        try:\n",
    "            d2.remove(key)\n",
    "        except:\n",
    "            print(f\"D2 no tiene {key}\")\n",
    "        try:\n",
    "            d3.remove(key)\n",
    "        except:\n",
    "            print(f\"D3 no tiene {key}\")\n",
    "            \n",
    "    return d1, d2, d3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:42.529334Z",
     "start_time": "2020-12-12T18:58:42.524336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 no tiene defensa\n",
      "D1 no tiene récord\n"
     ]
    }
   ],
   "source": [
    "keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports = remove_duplicates(keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:42.533335Z",
     "start_time": "2020-12-12T18:58:42.530334Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_gensim_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens + bigrams\"].apply(lambda x: \" \".join(x))\n",
    "    data['joined'] = data.joined.astype(str)\n",
    "    data = \" \".join(data[\"joined\"].values)\n",
    "    return [key[0] for key in keywords(data, scores=True, words=k, pos_filter=('NNP', 'JJ', \"NNPS\", \"VB\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:43.716334Z",
     "start_time": "2020-12-12T18:58:42.534333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 no tiene presidente\n",
      "D3 no tiene casos\n",
      "D2 no tiene horas\n",
      "D1 no tiene real\n",
      "D1 no tiene situacion\n",
      "D3 no tiene sin\n",
      "D3 no tiene trata\n",
      "D2 no tiene puntos\n",
      "D2 no tiene punto\n",
      "D3 no tiene persona\n",
      "D3 no tiene personas\n",
      "D1 no tiene espana\n",
      "D1 no tiene partidos\n",
      "D2 no tiene grupo\n",
      "D1 no tiene partido\n",
      "D3 no tiene pandemia\n",
      "D3 no tiene dia\n"
     ]
    }
   ],
   "source": [
    "keywords_gensim_health = get_k_gensim_keywords(train_health, 100)\n",
    "keywords_gensim_politics = get_k_gensim_keywords(train_politics, 100)\n",
    "keywords_gensim_sports = get_k_gensim_keywords(train_sports, 100)\n",
    "\n",
    "keywords_gensim_health, keywords_k_gensim_politics, keywords_k_gensim_sports = remove_duplicates(keywords_gensim_health, keywords_gensim_politics, keywords_gensim_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:43.722334Z",
     "start_time": "2020-12-12T18:58:43.717334Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_kmeans_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    k_means_data = data[\"joined\"].values\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(k_means_data)\n",
    "    \n",
    "    model = KMeans(n_clusters=3, init='k-means++', max_iter=1000, n_init=1, random_state = 5, algorithm=\"full\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    keywords_kmeans_politics = [terms[ind] for ind in order_centroids[0, :k]]\n",
    "    keywords_kmeans_health = [terms[ind] for ind in order_centroids[1, :k]]\n",
    "    keywords_kmeans_sports = [terms[ind] for ind in order_centroids[2, :k]]\n",
    "    \n",
    "    return keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:43.846334Z",
     "start_time": "2020-12-12T18:58:43.723334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n",
      "D3 no tiene presidente\n",
      "D3 no tiene pedro sánchez\n",
      "D1 no tiene forma\n",
      "D1 no tiene madrid\n",
      "D3 no tiene psoe\n",
      "D3 no tiene gobierno\n",
      "D3 no tiene sánchez\n",
      "D3 no tiene partido\n",
      "D1 no tiene infarto\n",
      "D3 no tiene fiscal\n",
      "D3 no tiene pedro\n",
      "D1 no tiene estudio\n",
      "D3 no tiene españa\n",
      "D1 no tiene mundo\n",
      "D3 no tiene rey\n",
      "D3 no tiene real\n",
      "88 83 94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['equipo',\n",
       " 'jugador',\n",
       " 'liga',\n",
       " 'jordan',\n",
       " 'nba',\n",
       " 'temporada',\n",
       " 'goles',\n",
       " 'vaccaro',\n",
       " 'gasol',\n",
       " 'contrato',\n",
       " 'base',\n",
       " 'millones',\n",
       " 'metabolismo',\n",
       " 'juego',\n",
       " 'lakers',\n",
       " 'haaland',\n",
       " 'facebook',\n",
       " 'cuerpo',\n",
       " 'mercado',\n",
       " 'auténtica',\n",
       " 'año',\n",
       " 'entrenador',\n",
       " 'jugar',\n",
       " 'hombre',\n",
       " 'escolta',\n",
       " 'competir',\n",
       " 'duro',\n",
       " 'atlético',\n",
       " 'puntos',\n",
       " 'masa',\n",
       " 'volver',\n",
       " 'pívot',\n",
       " 'competición',\n",
       " 'nike',\n",
       " 'magra',\n",
       " 'masa magra',\n",
       " 'gente',\n",
       " 'raptors',\n",
       " 'lesión',\n",
       " 'redes sociales',\n",
       " 'temporadas',\n",
       " 'curry',\n",
       " 'warriors',\n",
       " 'thompson',\n",
       " 'jugadores',\n",
       " 'franquicia',\n",
       " 'gol',\n",
       " 'suns',\n",
       " 'realmente',\n",
       " 'redes',\n",
       " 'investigadores',\n",
       " 'energía',\n",
       " 'recién',\n",
       " 'champions',\n",
       " 'campazzo',\n",
       " 'ricky',\n",
       " 'the',\n",
       " 'marc',\n",
       " 'campeones',\n",
       " 'llegado',\n",
       " 'convirtió',\n",
       " 'tasa',\n",
       " 'dólares',\n",
       " 'vida',\n",
       " 'bla',\n",
       " 'metabólica',\n",
       " 'tasa metabólica',\n",
       " 'sociales',\n",
       " 'tantos',\n",
       " 'pasar',\n",
       " 'personalidad',\n",
       " 'dortmund',\n",
       " 'hablando',\n",
       " 'difícil',\n",
       " 'fútbol',\n",
       " 'sal',\n",
       " 'michael jordan',\n",
       " 'pasa',\n",
       " 'draft',\n",
       " 'firmar',\n",
       " 'partidos',\n",
       " 'historia',\n",
       " 'acuerdo',\n",
       " 'bolt',\n",
       " 'barça',\n",
       " 'entrenamiento',\n",
       " 'rehabilitación',\n",
       " 'puerta',\n",
       " 'estrella',\n",
       " 'asistencias',\n",
       " 'michael',\n",
       " 'técnico',\n",
       " 'anthony',\n",
       " 'thunder']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = get_k_kmeans_keywords(train_data, 100)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))\n",
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = remove_duplicates(keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))\n",
    "keywords_kmeans_sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formación del glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:43.851332Z",
     "start_time": "2020-12-12T18:58:43.847335Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_relevant_keywords(d1, d2, d3):\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    return deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:43.862332Z",
     "start_time": "2020-12-12T18:58:43.852332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics ==>  {'desahucio', 'eta', 'pablo iglesias', 'código', 'armonización', 'generales', 'casa real', 'felipe gonzález', 'texto', 'injurias', 'gonzález', 'erc', 'marín', 'presupuestos', 'jiménez becerril', 'vivienda', 'vox', 'azurmendi', 'instituciones', 'penal', 'iceta', 'jueves', 'mal', 'sociedad', 'ciudadanos', 'ministro', 'ayuntamiento', 'callar', 'dsn', 'notas', 'proyecto', 'regulación', 'navidades', 'diputados', 'código penal', 'atención', 'informes', 'comunidades', 'alberto', 'ley', 'claro', 'armonización fiscal', 'becerril', 'congreso', 'eutanasia', 'independentistas', 'bildu', 'ascen', 'presupuestos generales', 'acuerdos', 'militares', 'pnv'}\n",
      "Sports ==>  {'jugador', 'historia', 'gasol', 'pista', 'temporadas', 'atlético', 'curry', 'franquicia', 'balonmano', 'bla', 'michael jordan', 'vaccaro', 'jordan', 'rehabilitación', 'gol', 'thompson', 'situaciones', 'equipo', 'temporada', 'campazzo', 'realmente', 'lakers', 'duro', 'equipos', 'estrella', 'haaland', 'nike', 'juego', 'grupos', 'marc', 'competir', 'goles', 'campeones', 'alfredo', 'entrenador', 'mercado', 'barça', 'escolta', 'pasa', 'dortmund', 'ricky', 'firmar', 'jornet', 'jugadores', 'liga', 'hablando', 'warriors', 'bolt', 'lesión', 'hombre', 'base', 'volver'}\n",
      "Health ==>  {'riesgo', 'ensayo', 'caso', 'tratamiento', 'estudios', 'fallecidos', 'centro', 'teléfonos', 'contagios', 'vacuna', 'alergia', 'suicidio', 'enfermedad', 'salud', 'moderna', 'plasma', 'frente', 'teléfonos móviles', 'fruta', 'virus', 'muertes', 'sanidad', 'coronavirus', 'sistema', 'alimentos', 'tipo', 'anticuerpos', 'importante', 'comida', 'plato', 'alergias', 'infarto', 'tuberculosis', 'expertos', 'covid', 'sangre', 'miocardio', 'suicidios', 'tasa', 'facebook', 'móviles', 'metabolismo', 'pecho'}\n"
     ]
    }
   ],
   "source": [
    "relevant_keywords_politics = check_relevant_keywords(keywords_kmeans_politics, keywords_gensim_politics, keywords_tfidf_politics)\n",
    "relevant_keywords_health = check_relevant_keywords(keywords_kmeans_health, keywords_gensim_health, keywords_tfidf_health)\n",
    "relevant_keywords_sports = check_relevant_keywords(keywords_kmeans_sports, keywords_gensim_sports, keywords_tfidf_sports)\n",
    "\n",
    "print(\"Politics ==> \", relevant_keywords_politics)\n",
    "print(\"Sports ==> \", relevant_keywords_sports)\n",
    "print(\"Health ==> \", relevant_keywords_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:43.867334Z",
     "start_time": "2020-12-12T18:58:43.865333Z"
    }
   },
   "outputs": [],
   "source": [
    "path_keys_health = \"../keywords/keys_health.txt\"\n",
    "path_keys_sports = \"../keywords/keys_sports.txt\"\n",
    "path_keys_politics = \"../keywords/keys_politics.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:43.874333Z",
     "start_time": "2020-12-12T18:58:43.869332Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_health = [key.strip() for key in open(path_keys_health, encoding=\"utf-8\").readlines()]\n",
    "keys_sports = [key.strip() for key in open(path_keys_sports, encoding=\"utf-8\").readlines()]\n",
    "keys_politics = [key.strip() for key in open(path_keys_politics, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "keys_dic = {-1: \"unknown\", 0: \"health\", 1: \"sports\", 2: \"politics\"}\n",
    "inverted_keys_dic = {\"unknown\": -1, \"health\": 0, \"sports\": 1, \"politics\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramas de test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.435333Z",
     "start_time": "2020-12-12T18:58:43.875333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>tokens + bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>health_23.txt</td>\n",
       "      <td>Hace unos días Alejandro Díez, madrileño de 24...</td>\n",
       "      <td>health</td>\n",
       "      <td>hace unos días alejandro díez madrileño de  añ...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "      <td>[se trata, este tipo, se trata, ha sido, es de...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>health_24.txt</td>\n",
       "      <td>Casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>health</td>\n",
       "      <td>casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "      <td>[sobre todo, se trata, frente al, segunda ola,...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>health_25.txt</td>\n",
       "      <td>Un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>health</td>\n",
       "      <td>un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "      <td>[frente al, frente al, cada vez, new york, más...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>health_26.txt</td>\n",
       "      <td>Los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>health</td>\n",
       "      <td>los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "      <td>[muy probable, se trata, sino también, sin emb...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>health_27.txt</td>\n",
       "      <td>El estrés de la rutina diaria, las preocupacio...</td>\n",
       "      <td>health</td>\n",
       "      <td>el estrés de la rutina diaria las preocupacion...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "      <td>[sin embargo, más allá, sin embargo, muchas pe...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>145</td>\n",
       "      <td>sports_50.txt</td>\n",
       "      <td>La cuarentena que cumplen algunas gimnastas, l...</td>\n",
       "      <td>sports</td>\n",
       "      <td>la cuarentena que cumplen algunas gimnastas la...</td>\n",
       "      <td>[cuarentena, cumplen, gimnastas, dificultades,...</td>\n",
       "      <td>[muchos casos, muchos casos, las autoridades, ...</td>\n",
       "      <td>[cuarentena, cumplen, gimnastas, dificultades,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>146</td>\n",
       "      <td>sports_6.txt</td>\n",
       "      <td>El Sevilla obtuvo en Krasnodar su billete para...</td>\n",
       "      <td>sports</td>\n",
       "      <td>el sevilla obtuvo en krasnodar su billete para...</td>\n",
       "      <td>[sevilla, obtuvo, krasnodar, billete, octavos,...</td>\n",
       "      <td>[todas las, todas las, todas las, todas las, t...</td>\n",
       "      <td>[sevilla, obtuvo, krasnodar, billete, octavos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>147</td>\n",
       "      <td>sports_7.txt</td>\n",
       "      <td>Ronald Koeman decidió dar una oportunidad a Ca...</td>\n",
       "      <td>sports</td>\n",
       "      <td>ronald koeman decidió dar una oportunidad a ca...</td>\n",
       "      <td>[ronald, koeman, decidió, oportunidad, carles,...</td>\n",
       "      <td>[apostar por, apostar por, apostar por, frente...</td>\n",
       "      <td>[ronald, koeman, decidió, oportunidad, carles,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>148</td>\n",
       "      <td>sports_8.txt</td>\n",
       "      <td>\\nChiellini, Bonucci, Barzagli, Zambrotta...la...</td>\n",
       "      <td>sports</td>\n",
       "      <td>\\nchiellini bonucci barzagli zambrottala lista...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "      <td>[frente al, sin embargo, frente al, sin embarg...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>149</td>\n",
       "      <td>sports_9.txt</td>\n",
       "      <td>Darko Brasanac es la principal novedad en la c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>darko brasanac es la principal novedad en la c...</td>\n",
       "      <td>[darko, brasanac, principal, novedad, convocat...</td>\n",
       "      <td>[frente al, sin embargo, frente al, sin embarg...</td>\n",
       "      <td>[darko, brasanac, principal, novedad, convocat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       doc_name                                               text  \\\n",
       "0       15  health_23.txt  Hace unos días Alejandro Díez, madrileño de 24...   \n",
       "1       16  health_24.txt  Casi todos los planes contra el coronavirus un...   \n",
       "2       17  health_25.txt  Un correcto descanso nocturno no sólo es impor...   \n",
       "3       18  health_26.txt  Los problemas de sueño son cada vez más frecue...   \n",
       "4       19  health_27.txt  El estrés de la rutina diaria, las preocupacio...   \n",
       "..     ...            ...                                                ...   \n",
       "100    145  sports_50.txt  La cuarentena que cumplen algunas gimnastas, l...   \n",
       "101    146   sports_6.txt  El Sevilla obtuvo en Krasnodar su billete para...   \n",
       "102    147   sports_7.txt  Ronald Koeman decidió dar una oportunidad a Ca...   \n",
       "103    148   sports_8.txt  \\nChiellini, Bonucci, Barzagli, Zambrotta...la...   \n",
       "104    149   sports_9.txt  Darko Brasanac es la principal novedad en la c...   \n",
       "\n",
       "      class                                       preprocesado  \\\n",
       "0    health  hace unos días alejandro díez madrileño de  añ...   \n",
       "1    health  casi todos los planes contra el coronavirus un...   \n",
       "2    health  un correcto descanso nocturno no sólo es impor...   \n",
       "3    health  los problemas de sueño son cada vez más frecue...   \n",
       "4    health  el estrés de la rutina diaria las preocupacion...   \n",
       "..      ...                                                ...   \n",
       "100  sports  la cuarentena que cumplen algunas gimnastas la...   \n",
       "101  sports  el sevilla obtuvo en krasnodar su billete para...   \n",
       "102  sports  ronald koeman decidió dar una oportunidad a ca...   \n",
       "103  sports  \\nchiellini bonucci barzagli zambrottala lista...   \n",
       "104  sports  darko brasanac es la principal novedad en la c...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [días, alejandro, díez, madrileño, años, levan...   \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...   \n",
       "2    [correcto, descanso, nocturno, importante, sen...   \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...   \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...   \n",
       "..                                                 ...   \n",
       "100  [cuarentena, cumplen, gimnastas, dificultades,...   \n",
       "101  [sevilla, obtuvo, krasnodar, billete, octavos,...   \n",
       "102  [ronald, koeman, decidió, oportunidad, carles,...   \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...   \n",
       "104  [darko, brasanac, principal, novedad, convocat...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    [se trata, este tipo, se trata, ha sido, es de...   \n",
       "1    [sobre todo, se trata, frente al, segunda ola,...   \n",
       "2    [frente al, frente al, cada vez, new york, más...   \n",
       "3    [muy probable, se trata, sino también, sin emb...   \n",
       "4    [sin embargo, más allá, sin embargo, muchas pe...   \n",
       "..                                                 ...   \n",
       "100  [muchos casos, muchos casos, las autoridades, ...   \n",
       "101  [todas las, todas las, todas las, todas las, t...   \n",
       "102  [apostar por, apostar por, apostar por, frente...   \n",
       "103  [frente al, sin embargo, frente al, sin embarg...   \n",
       "104  [frente al, sin embargo, frente al, sin embarg...   \n",
       "\n",
       "                                      tokens + bigrams  \n",
       "0    [días, alejandro, díez, madrileño, años, levan...  \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...  \n",
       "2    [correcto, descanso, nocturno, importante, sen...  \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...  \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...  \n",
       "..                                                 ...  \n",
       "100  [cuarentena, cumplen, gimnastas, dificultades,...  \n",
       "101  [sevilla, obtuvo, krasnodar, billete, octavos,...  \n",
       "102  [ronald, koeman, decidió, oportunidad, carles,...  \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...  \n",
       "104  [darko, brasanac, principal, novedad, convocat...  \n",
       "\n",
       "[105 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"bigrams\"] = test_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "test_data[\"tokens + bigrams\"] = test_data[\"tokens\"] + test_data[\"bigrams\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.438336Z",
     "start_time": "2020-12-12T18:58:44.436334Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.444333Z",
     "start_time": "2020-12-12T18:58:44.439333Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(glossary for glossary in glossaries)\n",
    "dictionary.save('keys.dict')  # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.449334Z",
     "start_time": "2020-12-12T18:58:44.445333Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, docs, dictionary):\n",
    "        self.docs = docs\n",
    "        self.dict = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dict.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.454335Z",
     "start_time": "2020-12-12T18:58:44.450334Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = MyCorpus(glossaries, dictionary)\n",
    "corpora.MmCorpus.serialize(\"keys.mm\", bow, metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.460334Z",
     "start_time": "2020-12-12T18:58:44.455333Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "index = Similarity(index_temp, bow, num_features=len(dictionary))  # create index\n",
    "index.save(\"keys.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.465334Z",
     "start_time": "2020-12-12T18:58:44.461334Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tfidf = models.TfidfModel(bow,smartirs=\"lpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.470333Z",
     "start_time": "2020-12-12T18:58:44.466333Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_document_tfidf(model, dictionary, bow, index, documents, i, verbose = False):\n",
    "    \"\"\"\n",
    "    Given a specific document, computes the ranking of the classes and returns the current class, \n",
    "    the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    document = documents.iloc[i]\n",
    "    pq = document[\"tokens + bigrams\"]\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = model[vq]\n",
    "    sim = index[qtfidf]\n",
    "\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "            \n",
    "    \n",
    "    return [i, get_info_document(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.475333Z",
     "start_time": "2020-12-12T18:58:44.471334Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.sum(sim) == 0.0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/3, 1/3, 1/3])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        tfidf_scores = np.array(sim)\n",
    "        probabilities = tfidf_scores / np.sum(tfidf_scores)\n",
    "    \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.481334Z",
     "start_time": "2020-12-12T18:58:44.476334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> La sentencia de la Audiencia Nacional que absuelve al jefe de los Mossos durante el 1-O, Josep Lluís\n",
      "[politics] ==> 0.258\n",
      "[health] ==> 0.000\n",
      "[sports] ==> 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[67,\n",
       " {'current_class': 2,\n",
       "  'predicted_class': 2,\n",
       "  'probabilities': array([0., 0., 1.], dtype=float32)}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_document_tfidf(model_tfidf, dictionary, bow, index, test_data, 67, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.485333Z",
     "start_time": "2020-12-12T18:58:44.482334Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_tfidf(function, model, dictionary, bow, index, data):\n",
    "    def classify(doc_i):\n",
    "        return function(model, dictionary, bow, index, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.525335Z",
     "start_time": "2020-12-12T18:58:44.486335Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics]\n",
    "model_w2v = models.Word2Vec(sentences = glossaries, window = 5, workers = 12, min_count = 1, seed=50)\n",
    "\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.529335Z",
     "start_time": "2020-12-12T18:58:44.526334Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_min(model):\n",
    "    vocab = model.wv.vocab\n",
    "    \n",
    "    maxs = []\n",
    "    mins = []\n",
    "    \n",
    "    for key in vocab:\n",
    "        maxs.append(max(model.wv[key]))\n",
    "        mins.append(min(model.wv[key]))\n",
    "    return max(maxs), min(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.534337Z",
     "start_time": "2020-12-12T18:58:44.530333Z"
    }
   },
   "outputs": [],
   "source": [
    "MAXI, MINI = get_max_min(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.538337Z",
     "start_time": "2020-12-12T18:58:44.535333Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_from_document(model, document):\n",
    "    embeddings = []\n",
    "    \n",
    "    for word in document:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else: # no está en el vocab\n",
    "            embeddings.append(np.random.uniform(low = MINI, high = MAXI, size = 100))\n",
    "    \n",
    "    return np.mean(embeddings, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.544333Z",
     "start_time": "2020-12-12T18:58:44.539334Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries_vector = [get_embeddings_from_document(model_w2v, glossary) for glossary in glossaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.548336Z",
     "start_time": "2020-12-12T18:58:44.545334Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document_w2v(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(sim) == 0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/3, 1/3, 1/3])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        w2v_scores = np.array(sim, dtype = \"float32\")\n",
    "        probabilities = (w2v_scores - w2v_scores.min()) / (w2v_scores.max() - w2v_scores.min()) \n",
    "        probabilities /= np.sum(probabilities)\n",
    "\n",
    "        \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.554333Z",
     "start_time": "2020-12-12T18:58:44.548336Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_w2v(glossaries_vector, model, documents, i, verbose = False):\n",
    "    document = documents.iloc[i]\n",
    "    doc_vector = get_embeddings_from_document(model, document[\"tokens + bigrams\"])\n",
    "\n",
    "    ranking = [[i, cosine_similarity(np.array(doc_vector).reshape(1,-1), np.array(glossary).reshape(1,-1)).item()] \n",
    "               for i, glossary in enumerate(glossaries_vector)]\n",
    "    \n",
    "    sim = [rank[1] for rank in ranking] \n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, get_info_document_w2v(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.564333Z",
     "start_time": "2020-12-12T18:58:44.555334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> El cerebro de los mamíferos está compuesto por dos lados, o hemisferios, que conecta los tractos ner\n",
      "Scores:\n",
      "[health] ==> 0.131\n",
      "[sports] ==> 0.082\n",
      "[politics] ==> 0.073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " {'current_class': 0,\n",
       "  'predicted_class': 0,\n",
       "  'probabilities': array([0.8643284 , 0.13567162, 0.        ], dtype=float32)}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_doc_w2v(glossaries_vector, model_w2v, test_data, 16, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.568332Z",
     "start_time": "2020-12-12T18:58:44.565336Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_w2v(function, glossaries_vector, model, data):\n",
    "    def classify(doc_i):\n",
    "        return function(glossaries_vector, model, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.573333Z",
     "start_time": "2020-12-12T18:58:44.569333Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_bayes(classifier, documents, i, verbose = False):\n",
    "    doc = documents.iloc[i]\n",
    "    #doc = vectorizer.transform([document[\"preprocesado\"]])\n",
    "    \n",
    "    y_pred = classifier.predict_proba([doc[\"preprocesado\"]])[0]\n",
    "    \n",
    "    ranking = [[i,prob] for i,prob in enumerate(y_pred)]\n",
    "    \n",
    "    probabilities = y_pred\n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    current_class = inverted_keys_dic[doc[\"class\"]]\n",
    "    predicted_class = ranking[0][0]\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + doc[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.578336Z",
     "start_time": "2020-12-12T18:58:44.574332Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_bayes(function, clf, documents):\n",
    "    def classify(doc_i):\n",
    "        return function(clf, documents, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.589333Z",
     "start_time": "2020-12-12T18:58:44.579333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossaries_joined = [\" \".join(gloss) for gloss in glossaries]\n",
    "\n",
    "X_train = glossaries_joined\n",
    "y_train = [0,1,2]\n",
    "\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                     ('clf', MultinomialNB(alpha=1e-1))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.595334Z",
     "start_time": "2020-12-12T18:58:44.590334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> \n",
      "Chiellini, Bonucci, Barzagli, Zambrotta...la lista de defensas italianos que han triunfado y elevad\n",
      "Scores:\n",
      "[sports] ==> 0.749\n",
      "[health] ==> 0.126\n",
      "[politics] ==> 0.125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[103,\n",
       " {'current_class': 1,\n",
       "  'predicted_class': 1,\n",
       "  'probabilities': array([0.12635959, 0.7486078 , 0.12503262])}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_doc_bayes(clf, test_data, 103, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.600333Z",
     "start_time": "2020-12-12T18:58:44.596334Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_documents(test_data, classify):\n",
    "    \"\"\"\n",
    "    Classifies the documents given a specific classification function.\n",
    "    \n",
    "    \"\"\"\n",
    "    test_data = test_data.copy()\n",
    "    \n",
    "    infos = [classify(i) for i in range(len(test_data))]\n",
    "    data = fill_test_data(test_data, infos)\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.606334Z",
     "start_time": "2020-12-12T18:58:44.601333Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_test_data(test_data, infos):\n",
    "    \"\"\"\n",
    "    Auxiliary function to fill the dataframe with info about the classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    current_class = pd.Series([info[1][\"current_class\"] for info in infos])\n",
    "    predicted_class = pd.Series([info[1][\"predicted_class\"] for info in infos])\n",
    "    p_health = pd.Series([info[1][\"probabilities\"][0] for info in infos])\n",
    "    p_sports = pd.Series([info[1][\"probabilities\"][1] for info in infos])\n",
    "    p_politics = pd.Series([info[1][\"probabilities\"][2] for info in infos])\n",
    "\n",
    "    data[\"current_class\"] = current_class\n",
    "    data[\"predicted_class\"] = predicted_class\n",
    "    data[\"p_health\"] = p_health\n",
    "    data[\"p_sports\"] = p_sports\n",
    "    data[\"p_politics\"] = p_politics\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.611333Z",
     "start_time": "2020-12-12T18:58:44.607333Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename(df):\n",
    "    \"\"\"\n",
    "    Computes the filename for each document based on the performed classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    confidence = \"%.3f\" % df[\"confidence\"]\n",
    "    current_class = df[\"class\"]\n",
    "    predicted_class = df[\"predicted_class_name\"]\n",
    "    correct = current_class == predicted_class\n",
    "    name = df[\"doc_name\"].split(\".\")[0]\n",
    "    \n",
    "    return f\"../classification/{predicted_class}/{confidence}_{name}-{correct}-{current_class}-{predicted_class}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.616335Z",
     "start_time": "2020-12-12T18:58:44.612336Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(path, content):\n",
    "    \"\"\"\n",
    "    Writes a file given its path and content.\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.621336Z",
     "start_time": "2020-12-12T18:58:44.617337Z"
    }
   },
   "outputs": [],
   "source": [
    "def move_files(data, tables = False):\n",
    "    \"\"\"\n",
    "    Moves the files to their corresponding new directory after classification is done.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    classes = [[0, \"p_health\"], [1, \"p_sports\"], [2, \"p_politics\"]]\n",
    "\n",
    "    for cl in classes:\n",
    "        docs = data[data[\"current_class\"] == cl[0]]\n",
    "        docs = docs.sort_values(by=[cl[1]], ascending=False)\n",
    "        docs[\"predicted_class_name\"] = docs[\"predicted_class\"].apply(lambda x : keys_dic[x])\n",
    "        docs[\"confidence\"] = docs[[\"p_health\", \"p_sports\", \"p_politics\"]].max(axis=1)\n",
    "        docs[\"file\"] = docs.apply(lambda x: get_filename(x), axis=1)\n",
    "        docs.apply(lambda row: write_file(row[\"file\"], row[\"text\"]), axis = 1)\n",
    "        if tables:\n",
    "            # tabla para la memoria\n",
    "            docs = docs[[\"doc_name\", \"class\", \"p_health\", \"p_sports\", \"p_politics\", \"predicted_class_name\"]]\n",
    "            docs = docs.rename(columns = {\"predicted_class_name\": \"predicted_class\"})\n",
    "            print(docs.to_latex(bold_rows = True, float_format=\"%.2f\", column_format = \"llllll\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:44.626333Z",
     "start_time": "2020-12-12T18:58:44.622336Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute(test_data, classification_function, move = True, tables = False):\n",
    "    \"\"\"\n",
    "    General function that classifies the documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    print(\"############################################################\")\n",
    "    print(\"Starting document´s classification...\")\n",
    "    data = classify_documents(data, classification_function)\n",
    "    sleep(1)\n",
    "    print(\"Document´s classification done...\")\n",
    "    if move:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        sleep(1)\n",
    "        print(\"Moving files to the correct directories...\")\n",
    "        move_files(data, tables)\n",
    "        sleep(1)\n",
    "        print(\"Files moved.\")\n",
    "    print(\"############################################################\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:47.744019Z",
     "start_time": "2020-12-12T18:58:44.627336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Starting document´s classification...\n",
      "Document´s classification done...\n",
      "-----------------------------------------------------------\n",
      "Moving files to the correct directories...\n",
      "Files moved.\n",
      "############################################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>tokens + bigrams</th>\n",
       "      <th>current_class</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>p_health</th>\n",
       "      <th>p_sports</th>\n",
       "      <th>p_politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>health_23.txt</td>\n",
       "      <td>Hace unos días Alejandro Díez, madrileño de 24...</td>\n",
       "      <td>health</td>\n",
       "      <td>hace unos días alejandro díez madrileño de  añ...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "      <td>[se trata, este tipo, se trata, ha sido, es de...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.847396</td>\n",
       "      <td>0.152605</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>health_24.txt</td>\n",
       "      <td>Casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>health</td>\n",
       "      <td>casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "      <td>[sobre todo, se trata, frente al, segunda ola,...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895831</td>\n",
       "      <td>0.069446</td>\n",
       "      <td>0.034723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>health_25.txt</td>\n",
       "      <td>Un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>health</td>\n",
       "      <td>un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "      <td>[frente al, frente al, cada vez, new york, más...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>health_26.txt</td>\n",
       "      <td>Los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>health</td>\n",
       "      <td>los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "      <td>[muy probable, se trata, sino también, sin emb...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>health_27.txt</td>\n",
       "      <td>El estrés de la rutina diaria, las preocupacio...</td>\n",
       "      <td>health</td>\n",
       "      <td>el estrés de la rutina diaria las preocupacion...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "      <td>[sin embargo, más allá, sin embargo, muchas pe...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.534069</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>145</td>\n",
       "      <td>sports_50.txt</td>\n",
       "      <td>La cuarentena que cumplen algunas gimnastas, l...</td>\n",
       "      <td>sports</td>\n",
       "      <td>la cuarentena que cumplen algunas gimnastas la...</td>\n",
       "      <td>[cuarentena, cumplen, gimnastas, dificultades,...</td>\n",
       "      <td>[muchos casos, muchos casos, las autoridades, ...</td>\n",
       "      <td>[cuarentena, cumplen, gimnastas, dificultades,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.358104</td>\n",
       "      <td>0.641896</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>146</td>\n",
       "      <td>sports_6.txt</td>\n",
       "      <td>El Sevilla obtuvo en Krasnodar su billete para...</td>\n",
       "      <td>sports</td>\n",
       "      <td>el sevilla obtuvo en krasnodar su billete para...</td>\n",
       "      <td>[sevilla, obtuvo, krasnodar, billete, octavos,...</td>\n",
       "      <td>[todas las, todas las, todas las, todas las, t...</td>\n",
       "      <td>[sevilla, obtuvo, krasnodar, billete, octavos,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>147</td>\n",
       "      <td>sports_7.txt</td>\n",
       "      <td>Ronald Koeman decidió dar una oportunidad a Ca...</td>\n",
       "      <td>sports</td>\n",
       "      <td>ronald koeman decidió dar una oportunidad a ca...</td>\n",
       "      <td>[ronald, koeman, decidió, oportunidad, carles,...</td>\n",
       "      <td>[apostar por, apostar por, apostar por, frente...</td>\n",
       "      <td>[ronald, koeman, decidió, oportunidad, carles,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>148</td>\n",
       "      <td>sports_8.txt</td>\n",
       "      <td>\\nChiellini, Bonucci, Barzagli, Zambrotta...la...</td>\n",
       "      <td>sports</td>\n",
       "      <td>\\nchiellini bonucci barzagli zambrottala lista...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "      <td>[frente al, sin embargo, frente al, sin embarg...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>149</td>\n",
       "      <td>sports_9.txt</td>\n",
       "      <td>Darko Brasanac es la principal novedad en la c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>darko brasanac es la principal novedad en la c...</td>\n",
       "      <td>[darko, brasanac, principal, novedad, convocat...</td>\n",
       "      <td>[frente al, sin embargo, frente al, sin embarg...</td>\n",
       "      <td>[darko, brasanac, principal, novedad, convocat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075931</td>\n",
       "      <td>0.924069</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       doc_name                                               text  \\\n",
       "0       15  health_23.txt  Hace unos días Alejandro Díez, madrileño de 24...   \n",
       "1       16  health_24.txt  Casi todos los planes contra el coronavirus un...   \n",
       "2       17  health_25.txt  Un correcto descanso nocturno no sólo es impor...   \n",
       "3       18  health_26.txt  Los problemas de sueño son cada vez más frecue...   \n",
       "4       19  health_27.txt  El estrés de la rutina diaria, las preocupacio...   \n",
       "..     ...            ...                                                ...   \n",
       "100    145  sports_50.txt  La cuarentena que cumplen algunas gimnastas, l...   \n",
       "101    146   sports_6.txt  El Sevilla obtuvo en Krasnodar su billete para...   \n",
       "102    147   sports_7.txt  Ronald Koeman decidió dar una oportunidad a Ca...   \n",
       "103    148   sports_8.txt  \\nChiellini, Bonucci, Barzagli, Zambrotta...la...   \n",
       "104    149   sports_9.txt  Darko Brasanac es la principal novedad en la c...   \n",
       "\n",
       "      class                                       preprocesado  \\\n",
       "0    health  hace unos días alejandro díez madrileño de  añ...   \n",
       "1    health  casi todos los planes contra el coronavirus un...   \n",
       "2    health  un correcto descanso nocturno no sólo es impor...   \n",
       "3    health  los problemas de sueño son cada vez más frecue...   \n",
       "4    health  el estrés de la rutina diaria las preocupacion...   \n",
       "..      ...                                                ...   \n",
       "100  sports  la cuarentena que cumplen algunas gimnastas la...   \n",
       "101  sports  el sevilla obtuvo en krasnodar su billete para...   \n",
       "102  sports  ronald koeman decidió dar una oportunidad a ca...   \n",
       "103  sports  \\nchiellini bonucci barzagli zambrottala lista...   \n",
       "104  sports  darko brasanac es la principal novedad en la c...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [días, alejandro, díez, madrileño, años, levan...   \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...   \n",
       "2    [correcto, descanso, nocturno, importante, sen...   \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...   \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...   \n",
       "..                                                 ...   \n",
       "100  [cuarentena, cumplen, gimnastas, dificultades,...   \n",
       "101  [sevilla, obtuvo, krasnodar, billete, octavos,...   \n",
       "102  [ronald, koeman, decidió, oportunidad, carles,...   \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...   \n",
       "104  [darko, brasanac, principal, novedad, convocat...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    [se trata, este tipo, se trata, ha sido, es de...   \n",
       "1    [sobre todo, se trata, frente al, segunda ola,...   \n",
       "2    [frente al, frente al, cada vez, new york, más...   \n",
       "3    [muy probable, se trata, sino también, sin emb...   \n",
       "4    [sin embargo, más allá, sin embargo, muchas pe...   \n",
       "..                                                 ...   \n",
       "100  [muchos casos, muchos casos, las autoridades, ...   \n",
       "101  [todas las, todas las, todas las, todas las, t...   \n",
       "102  [apostar por, apostar por, apostar por, frente...   \n",
       "103  [frente al, sin embargo, frente al, sin embarg...   \n",
       "104  [frente al, sin embargo, frente al, sin embarg...   \n",
       "\n",
       "                                      tokens + bigrams  current_class  \\\n",
       "0    [días, alejandro, díez, madrileño, años, levan...              0   \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...              0   \n",
       "2    [correcto, descanso, nocturno, importante, sen...              0   \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...              0   \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...              0   \n",
       "..                                                 ...            ...   \n",
       "100  [cuarentena, cumplen, gimnastas, dificultades,...              1   \n",
       "101  [sevilla, obtuvo, krasnodar, billete, octavos,...              1   \n",
       "102  [ronald, koeman, decidió, oportunidad, carles,...              1   \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...              1   \n",
       "104  [darko, brasanac, principal, novedad, convocat...              1   \n",
       "\n",
       "     predicted_class  p_health  p_sports  p_politics  \n",
       "0                  0  0.847396  0.152605    0.000000  \n",
       "1                  0  0.895831  0.069446    0.034723  \n",
       "2                  0  0.777778  0.111111    0.111111  \n",
       "3                  0  0.666667  0.000000    0.333333  \n",
       "4                  1  0.465931  0.534069    0.000000  \n",
       "..               ...       ...       ...         ...  \n",
       "100                1  0.358104  0.641896    0.000000  \n",
       "101                1  0.000000  1.000000    0.000000  \n",
       "102                1  0.000000  1.000000    0.000000  \n",
       "103                1  0.000000  1.000000    0.000000  \n",
       "104                1  0.075931  0.924069    0.000000  \n",
       "\n",
       "[105 rows x 13 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = execute(test_data, classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:47.749019Z",
     "start_time": "2020-12-12T18:58:47.745018Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_model(data, model, classify_function):\n",
    "    \"\"\"\n",
    "    Function that evaluates the performance of a specific model.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    print(\"#######################################################\")\n",
    "    print(\"Evaluating \"+ model + \"...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    data = classify_documents(data, classify_function)\n",
    "    \n",
    "    y_true = data[\"current_class\"]\n",
    "    y_pred = data[\"predicted_class\"]\n",
    "    original = len(y_pred)\n",
    "    y_pred = data[data[\"predicted_class\"] != -1][\"predicted_class\"]\n",
    "    unknown = original - len(y_pred)\n",
    "    y_true = y_true.loc[y_pred.index]\n",
    "        \n",
    "    print(classification_report(y_true, y_pred, target_names=[\"health\", \"sports\", \"politics\"]))\n",
    "    print(\"Confusion matrix ==>\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    print(f\"{unknown} documents couldn´t been classified.\")\n",
    "\n",
    "    #precisions = []\n",
    "    #recalls = []\n",
    "\n",
    "    #for i in range(len(cm[0])):\n",
    "     #   name = keys_dic[i]\n",
    "      #  print(f\"Computing statistics about {name}:\")\n",
    "       # recall = cm[i,i] / np.sum(cm[i,:])\n",
    "      #  precision = cm[i,i] / np.sum(cm[:,i])\n",
    "      #  print(f\"\\tPrecision ==> {precision}\")\n",
    "      #  print(f\"\\tRecall ==> {recall}\")\n",
    "\n",
    "      #  precisions.append(precision)\n",
    "      #  recalls.append(recall)\n",
    "\n",
    "    #precisions = np.array(precisions)\n",
    "    #recalls = np.array(recalls)\n",
    "    #f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "    #accuracy = accuracy_score(y_true, y_pred)\n",
    "    #print(f\"Average precision ==> {precisions.mean()}\")\n",
    "    #print(f\"Average recall ==> {recalls.mean()}\")\n",
    "    #print(f\"F1-Score ==> {f1}\")\n",
    "    #print(f\"Overall accuracy score ==> {accuracy}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Model evaluated\")\n",
    "    print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:47.797017Z",
     "start_time": "2020-12-12T18:58:47.750017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating tf-idf...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.79      0.86      0.82        35\n",
      "      sports       0.84      0.84      0.84        32\n",
      "    politics       0.97      0.88      0.92        34\n",
      "\n",
      "    accuracy                           0.86       101\n",
      "   macro avg       0.87      0.86      0.86       101\n",
      "weighted avg       0.87      0.86      0.86       101\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[30  5  0]\n",
      " [ 4 27  1]\n",
      " [ 4  0 30]]\n",
      "4 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_model(test_data, \"tf-idf\", classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:48.065018Z",
     "start_time": "2020-12-12T18:58:47.798018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating Word2Vec...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.56      0.69      0.62        35\n",
      "      sports       0.55      0.49      0.52        35\n",
      "    politics       0.52      0.46      0.48        35\n",
      "\n",
      "    accuracy                           0.54       105\n",
      "   macro avg       0.54      0.54      0.54       105\n",
      "weighted avg       0.54      0.54      0.54       105\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[24  8  3]\n",
      " [ 6 17 12]\n",
      " [13  6 16]]\n",
      "0 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_model(test_data, \"Word2Vec\", classify_w2v(classify_doc_w2v, glossaries_vector, model_w2v, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-12T18:58:48.221017Z",
     "start_time": "2020-12-12T18:58:48.066016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating Multinomial NB (tfidf)...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.82      0.80      0.81        35\n",
      "      sports       0.85      0.80      0.82        35\n",
      "    politics       0.87      0.94      0.90        35\n",
      "\n",
      "    accuracy                           0.85       105\n",
      "   macro avg       0.85      0.85      0.85       105\n",
      "weighted avg       0.85      0.85      0.85       105\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[28  4  3]\n",
      " [ 5 28  2]\n",
      " [ 1  1 33]]\n",
      "0 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_model(test_data, \"Multinomial NB (tfidf)\", classify_bayes(classify_doc_bayes, clf, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
