{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:08:41.279511Z",
     "start_time": "2020-12-09T19:08:40.142513Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "\n",
    "# Joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy_spanish_lemmatizer import SpacyCustomLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:12:49.005893Z",
     "start_time": "2020-12-09T19:12:49.003893Z"
    }
   },
   "outputs": [],
   "source": [
    "path_health = \"../documents/health\"\n",
    "path_politics = \"../documents/politics\"\n",
    "path_sports = \"../documents/sports\"\n",
    "path_documents = \"../documents\"\n",
    "path_stopwords = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:08:41.288511Z",
     "start_time": "2020-12-09T19:08:41.283512Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    return open(path,encoding='utf-8').read(), path.split(\"\\\\\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:08:41.799511Z",
     "start_time": "2020-12-09T19:08:41.289511Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = Parallel(n_jobs = -1)(delayed(load_document)(path) for path in glob.glob(path_documents+\"/*/*.txt\"))\n",
    "documents = pd.DataFrame(documents, columns=[\"text\", \"class\"])\n",
    "documents['text'] = documents['text'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:08:41.813512Z",
     "start_time": "2020-12-09T19:08:41.800513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   class\n",
       "0  Aceptémoslo de una vez: perder peso de manera ...  health\n",
       "1  Sin tiempo para hacer recuento de daños, irrum...  health\n",
       "2  Mucha gente intenta mostrar en las redes socia...  health\n",
       "3  Una faceta clave en la frenética lucha global ...  health\n",
       "4  La curva de contagios de coronavirus se mantie...  health"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:14:39.410635Z",
     "start_time": "2020-12-09T19:14:38.527836Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "    \n",
    "nlp = spacy.load(\"es\")\n",
    "lemmatizer = SpacyCustomLemmatizer()\n",
    "\n",
    "def load_stopwords(path):\n",
    "    return [line.strip() for line in open(path_stopwords, encoding = \"utf-8\").readlines()]\n",
    "\n",
    "STOP_WORDS = set(load_stopwords(path_stopwords))\n",
    "\n",
    "def delete_stop_words(doc):\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return clean\n",
    "\n",
    "def preprocess_document(document):\n",
    "    document = REPLACE_NO_SPACE.sub(NO_SPACE, document.lower())\n",
    "    document = REPLACE_WITH_SPACE.sub(SPACE, document)\n",
    "    # tokens = wordpunct_tokenize(document)\n",
    "    # tokens = delete_proper_nouns(tokens)\n",
    "    tokens = delete_stop_words(document)\n",
    "    return tokens\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tokens = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "# TODO: REVISAR ESTO\n",
    "\n",
    "def delete_proper_nouns(tokens):\n",
    "    # Tag the tokens with their type - ie are they nouns or not\n",
    "    lTokens = pos_tag(tokens)\n",
    "    # find all the proper nouns and print them out\n",
    "    lTagDict = findtags('NNP', lTokens)\n",
    "    return [token.lower() for token in tokens if token not in lTagDict]\n",
    "    \n",
    "def findtags(tag_prefix, tagged_text):\n",
    "    \"\"\"\n",
    "    Find tokens matching the specified tag_prefix\n",
    "    \"\"\"\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    print(cfd.conditions())\n",
    "    return [list(cfd[tag].keys()) for tag in cfd.conditions()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:14:40.527575Z",
     "start_time": "2020-12-09T19:14:40.248575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "      <td>[aceptémoslo, vez, perder, peso, manera, rápid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "      <td>[tiempo, hacer, recuento, daños, irrumpe, segu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "      <td>[mucha, gente, intenta, mostrar, redes, social...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "      <td>[faceta, clave, frenética, lucha, global, pfiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "      <td>[curva, contagios, coronavirus, mantiene, espa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   class  \\\n",
       "0  Aceptémoslo de una vez: perder peso de manera ...  health   \n",
       "1  Sin tiempo para hacer recuento de daños, irrum...  health   \n",
       "2  Mucha gente intenta mostrar en las redes socia...  health   \n",
       "3  Una faceta clave en la frenética lucha global ...  health   \n",
       "4  La curva de contagios de coronavirus se mantie...  health   \n",
       "\n",
       "                                        preprocesado  \n",
       "0  [aceptémoslo, vez, perder, peso, manera, rápid...  \n",
       "1  [tiempo, hacer, recuento, daños, irrumpe, segu...  \n",
       "2  [mucha, gente, intenta, mostrar, redes, social...  \n",
       "3  [faceta, clave, frenética, lucha, global, pfiz...  \n",
       "4  [curva, contagios, coronavirus, mantiene, espa...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[\"preprocesado\"] = documents[\"text\"].apply(lambda x: preprocess_document(x))\n",
    "# documents[\"lematizado\"] = documents[\"preprocesado\"].apply(lambda x: lemmatize(x))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:14:46.775004Z",
     "start_time": "2020-12-09T19:14:46.769002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ==> 45 documents\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_health = documents[documents[\"class\"] == \"health\"].iloc[:15]\n",
    "train_politics = documents[documents[\"class\"] == \"politics\"].iloc[:15]\n",
    "train_sports = documents[documents[\"class\"] == \"sports\"].iloc[:15]\n",
    "\n",
    "train_data = pd.concat([train_health, train_politics, train_sports])\n",
    "print(f\"Training data ==> {len(train_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:14:47.315000Z",
     "start_time": "2020-12-09T19:14:47.308000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data ==> 105 documents\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_health = documents[documents[\"class\"] == \"health\"].iloc[15:]\n",
    "test_politics = documents[documents[\"class\"] == \"politics\"].iloc[15:]\n",
    "test_sports = documents[documents[\"class\"] == \"sports\"].iloc[15:]\n",
    "\n",
    "test_data = pd.concat([test_health, test_politics, test_sports])\n",
    "print(f\"Testing data ==> {len(test_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción del glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:14:47.838000Z",
     "start_time": "2020-12-09T19:14:47.836000Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_dir = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:14:48.362001Z",
     "start_time": "2020-12-09T19:14:48.358002Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_tfidf_keywords(df, k):\n",
    "    tokens = df[\"preprocesado\"].values\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    bow = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    bow_tfidf = tfidf[bow]\n",
    "    tfidf_dic = {dictionary.get(id): value for doc in bow_tfidf for id, value in doc}\n",
    "    tfidf_list = [k for k, v in sorted(tfidf_dic.items(), key=lambda item: item[1], reverse = True)]\n",
    "    return tfidf_list[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:14:48.990000Z",
     "start_time": "2020-12-09T19:14:48.883000Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health = get_k_tfidf_keywords(train_health, 30)\n",
    "keywords_tfidf_politics = get_k_tfidf_keywords(train_politics, 30)\n",
    "keywords_tfidf_sports = get_k_tfidf_keywords(train_sports, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:18:06.970771Z",
     "start_time": "2020-12-09T19:18:06.965761Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(d1, d2, d3):\n",
    "\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    \n",
    "    for key in deleted:\n",
    "        try:\n",
    "            d1.remove(key)\n",
    "        except:\n",
    "            print(f\"D1 no tiene {key}\")\n",
    "        try:\n",
    "            d2.remove(key)\n",
    "        except:\n",
    "            print(f\"D2 no tiene {key}\")\n",
    "        try:\n",
    "            d3.remove(key)\n",
    "        except:\n",
    "            print(f\"D3 no tiene {key}\")\n",
    "            \n",
    "    return d1, d2, d3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:18:07.500760Z",
     "start_time": "2020-12-09T19:18:07.498759Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports = remove_duplicates(keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:19:14.137790Z",
     "start_time": "2020-12-09T19:19:14.133789Z"
    }
   },
   "outputs": [],
   "source": [
    "import RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:30:04.886498Z",
     "start_time": "2020-12-09T19:30:04.835500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sort_tuple(tup):\n",
    "    tup.sort(key = lambda x: x[1])\n",
    "    return tup\n",
    "\n",
    "rake = RAKE.Rake(path_stopwords)\n",
    "train_health[\"joined\"] = train_health[\"preprocesado\"].apply(lambda x: \" \".join(x))\n",
    "train_health['joined'] = train_health.joined.astype(str)\n",
    "data = \" \".join(train_health[\"joined\"].values)\n",
    "print(type(data))\n",
    "sort_tuple(rake.run(data))[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAKE Nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:35:38.985302Z",
     "start_time": "2020-12-09T19:35:38.976301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake(list(STOP_WORDS), max_length=1)\n",
    "\n",
    "r.extract_keywords_from_sentences(train_health[\"joined\"].values)\n",
    "r.get_ranked_phrases()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:40:54.085418Z",
     "start_time": "2020-12-09T19:40:53.730422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('puede', 0.19594940686009282), ('puedes', 0.19594940686009282), ('dia', 0.17008555228542271), ('dias', 0.17008555228542271), ('enfermedad', 0.16558211421752667), ('enfermedades', 0.16558211421752667), ('solo', 0.152695702636688), ('solos', 0.152695702636688), ('personas', 0.14003570409239718), ('persona', 0.14003570409239718), ('caso', 0.13349165653363246), ('casos', 0.13349165653363246), ('ser', 0.12216997285059364), ('estudio', 0.11202040604382715), ('estudios', 0.11202040604382715), ('forma', 0.11181865814141997), ('formas', 0.11181865814141997), ('comidas', 0.10754844009392951), ('comida', 0.10754844009392951), ('salud', 0.09408628758123108), ('saludable', 0.09408628758123108), ('saludables', 0.09408628758123108), ('asi', 0.09401564774487135), ('hora', 0.09375266330875939), ('horas', 0.09375266330875939), ('pueden', 0.09040105628916605), ('tiempo', 0.09018828300330411), ('tiempos', 0.09018828300330411), ('menos', 0.08643507842851535), ('ano', 0.08482748300054685), ('anos', 0.08482748300054685), ('partes', 0.083206319024872), ('parte', 0.083206319024872), ('mayor', 0.08254898748363243), ('mayores', 0.08254898748363243), ('ademas', 0.08252254511830605), ('aunque', 0.08208431424118719), ('infarto', 0.0810960233014451), ('infartos', 0.0810960233014451), ('sangre', 0.0805069175750122), ('cuerpo', 0.07949119193958992), ('manera', 0.07891103168155612), ('maneras', 0.07891103168155612), ('dos', 0.07833410327535667), ('mejor', 0.07803561528569339), ('mejores', 0.07803561528569339), ('covid', 0.07757719859815503), ('virus', 0.07639218137011007), ('semana', 0.0747447569215983), ('semanas', 0.0747447569215983)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "print(keywords(data, scores=True, words=30, pos_filter=('NNP', 'JJ', \"NNPS\", \"VB\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:18:09.219076Z",
     "start_time": "2020-12-09T19:18:09.214078Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_kmeans_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"preprocesado\"].apply(lambda x: \" \".join(x))\n",
    "    k_means_data = data[\"joined\"].values\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(k_means_data)\n",
    "    \n",
    "    model = KMeans(n_clusters=3, init='k-means++', max_iter=1000, n_init=1, random_state = 5, algorithm=\"full\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    keywords_kmeans_politics = [terms[ind] for ind in order_centroids[0, :k]]\n",
    "    keywords_kmeans_health = [terms[ind] for ind in order_centroids[1, :k]]\n",
    "    keywords_kmeans_sports = [terms[ind] for ind in order_centroids[2, :k]]\n",
    "    \n",
    "    return keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:18:09.838048Z",
     "start_time": "2020-12-09T19:18:09.790045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30\n",
      "D3 no tiene sánchez\n",
      "D1 no tiene madrid\n",
      "D1 no tiene puede\n",
      "D1 no tiene ser\n",
      "D3 no tiene gobierno\n",
      "D3 no tiene españa\n",
      "27 24 27\n"
     ]
    }
   ],
   "source": [
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = get_k_kmeans_keywords(train_data, 30)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))\n",
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = remove_duplicates(keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
