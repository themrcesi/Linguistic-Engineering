{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Índice<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-documentos\" data-toc-modified-id=\"Carga-de-documentos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Carga de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocesado\" data-toc-modified-id=\"Preprocesado-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preprocesado</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bigramas\" data-toc-modified-id=\"Bigramas-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Bigramas</a></span></li></ul></li></ul></li><li><span><a href=\"#Glosario\" data-toc-modified-id=\"Glosario-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-de-keywords\" data-toc-modified-id=\"Extracción-de-keywords-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extracción de keywords</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-propia\" data-toc-modified-id=\"Extracción-propia-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Extracción propia</a></span></li><li><span><a href=\"#Gensim\" data-toc-modified-id=\"Gensim-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Gensim</a></span></li><li><span><a href=\"#Kmeans\" data-toc-modified-id=\"Kmeans-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Kmeans</a></span></li></ul></li><li><span><a href=\"#Formación-del-glosario\" data-toc-modified-id=\"Formación-del-glosario-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Formación del glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automatizado\" data-toc-modified-id=\"Automatizado-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Automatizado</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificador\" data-toc-modified-id=\"Clasificador-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clasificador</a></span><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-glosarios\" data-toc-modified-id=\"Carga-de-glosarios-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Carga de glosarios</a></span></li><li><span><a href=\"#Bigramas-de-test-data\" data-toc-modified-id=\"Bigramas-de-test-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bigramas de test data</a></span></li><li><span><a href=\"#Modelos\" data-toc-modified-id=\"Modelos-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Naive Bayes</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificación-de-documentos\" data-toc-modified-id=\"Clasificación-de-documentos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clasificación de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li><li><span><a href=\"#Clasificación\" data-toc-modified-id=\"Clasificación-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Clasificación</a></span></li></ul></li><li><span><a href=\"#Evaluación-de-modelos\" data-toc-modified-id=\"Evaluación-de-modelos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluación de modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:37.691789Z",
     "start_time": "2020-12-20T21:31:36.784789Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.summarization import keywords\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# Operatos\n",
    "from operator import itemgetter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "# statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# utils\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:37.694790Z",
     "start_time": "2020-12-20T21:31:37.692789Z"
    }
   },
   "outputs": [],
   "source": [
    "path_health = \"../documents/health\"\n",
    "path_politics = \"../documents/politics\"\n",
    "path_sports = \"../documents/sports\"\n",
    "path_documents = \"../documents\"\n",
    "path_stopwords = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:37.700789Z",
     "start_time": "2020-12-20T21:31:37.695789Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    return path.split(\"\\\\\")[-1], open(path,encoding='utf-8').read(), path.split(\"\\\\\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:38.177790Z",
     "start_time": "2020-12-20T21:31:37.701790Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = Parallel(n_jobs = -1)(delayed(load_document)(path) for path in glob.glob(path_documents+\"/*/*.txt\"))\n",
    "documents = pd.DataFrame(documents, columns=[\"doc_name\", \"text\", \"class\"])\n",
    "documents['text'] = documents['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:38.189792Z",
     "start_time": "2020-12-20T21:31:38.178790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>sports_20.txt</td>\n",
       "      <td>No paró el crono en 9.58, pero casi. Aunque en...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>health_36.txt</td>\n",
       "      <td>Gestionar en cualquier ámbito y el sanitario n...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>technology_39.txt</td>\n",
       "      <td>Cuando se creó internet, la red sólo estaba di...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>technology_9.txt</td>\n",
       "      <td>Cada vez se llevan a cabo un mayor número de c...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>technology_49.txt</td>\n",
       "      <td>FORD TRABAJA EN UNA TECNOLOGÍA CAPAZ DE PREVEN...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              doc_name                                               text  \\\n",
       "112      sports_20.txt  No paró el crono en 9.58, pero casi. Aunque en...   \n",
       "29       health_36.txt  Gestionar en cualquier ámbito y el sanitario n...   \n",
       "182  technology_39.txt  Cuando se creó internet, la red sólo estaba di...   \n",
       "199   technology_9.txt  Cada vez se llevan a cabo un mayor número de c...   \n",
       "193  technology_49.txt  FORD TRABAJA EN UNA TECNOLOGÍA CAPAZ DE PREVEN...   \n",
       "\n",
       "          class  \n",
       "112      sports  \n",
       "29       health  \n",
       "182  technology  \n",
       "199  technology  \n",
       "193  technology  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = documents.sample(frac=1, random_state = 2)\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:38.195790Z",
     "start_time": "2020-12-20T21:31:38.190792Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\&)|(\\%)|(\\$)|(\\€)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def load_stopwords(path):\n",
    "    return [line.strip() for line in open(path_stopwords, encoding = \"utf-8\").readlines()]\n",
    "\n",
    "STOP_WORDS = set(load_stopwords(path_stopwords))\n",
    "\n",
    "def delete_stop_words(doc):\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return clean\n",
    "\n",
    "def preprocess_document(document):\n",
    "    document = REPLACE_NO_SPACE.sub(NO_SPACE, document.lower())\n",
    "    document = REPLACE_WITH_SPACE.sub(SPACE, document)\n",
    "    return document\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tokens = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:38.694789Z",
     "start_time": "2020-12-20T21:31:38.196789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>sports_20.txt</td>\n",
       "      <td>No paró el crono en 9.58, pero casi. Aunque en...</td>\n",
       "      <td>sports</td>\n",
       "      <td>no paró el crono en  pero casi aunque en esta ...</td>\n",
       "      <td>[paró, crono, ocasión, minutos, segundos, usai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>health_36.txt</td>\n",
       "      <td>Gestionar en cualquier ámbito y el sanitario n...</td>\n",
       "      <td>health</td>\n",
       "      <td>gestionar en cualquier ámbito y el sanitario n...</td>\n",
       "      <td>[gestionar, ámbito, sanitario, excepción, prec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>technology_39.txt</td>\n",
       "      <td>Cuando se creó internet, la red sólo estaba di...</td>\n",
       "      <td>technology</td>\n",
       "      <td>cuando se creó internet la red sólo estaba dis...</td>\n",
       "      <td>[creó, internet, red, disponible, pequeño, gru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>technology_9.txt</td>\n",
       "      <td>Cada vez se llevan a cabo un mayor número de c...</td>\n",
       "      <td>technology</td>\n",
       "      <td>cada vez se llevan a cabo un mayor número de c...</td>\n",
       "      <td>[llevan, cabo, número, compras, internet, estu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>technology_49.txt</td>\n",
       "      <td>FORD TRABAJA EN UNA TECNOLOGÍA CAPAZ DE PREVEN...</td>\n",
       "      <td>technology</td>\n",
       "      <td>ford trabaja en una tecnología capaz de preven...</td>\n",
       "      <td>[ford, tecnología, capaz, prevenir, accidentes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              doc_name                                               text  \\\n",
       "112      sports_20.txt  No paró el crono en 9.58, pero casi. Aunque en...   \n",
       "29       health_36.txt  Gestionar en cualquier ámbito y el sanitario n...   \n",
       "182  technology_39.txt  Cuando se creó internet, la red sólo estaba di...   \n",
       "199   technology_9.txt  Cada vez se llevan a cabo un mayor número de c...   \n",
       "193  technology_49.txt  FORD TRABAJA EN UNA TECNOLOGÍA CAPAZ DE PREVEN...   \n",
       "\n",
       "          class                                       preprocesado  \\\n",
       "112      sports  no paró el crono en  pero casi aunque en esta ...   \n",
       "29       health  gestionar en cualquier ámbito y el sanitario n...   \n",
       "182  technology  cuando se creó internet la red sólo estaba dis...   \n",
       "199  technology  cada vez se llevan a cabo un mayor número de c...   \n",
       "193  technology  ford trabaja en una tecnología capaz de preven...   \n",
       "\n",
       "                                                tokens  \n",
       "112  [paró, crono, ocasión, minutos, segundos, usai...  \n",
       "29   [gestionar, ámbito, sanitario, excepción, prec...  \n",
       "182  [creó, internet, red, disponible, pequeño, gru...  \n",
       "199  [llevan, cabo, número, compras, internet, estu...  \n",
       "193  [ford, tecnología, capaz, prevenir, accidentes...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[\"preprocesado\"] = documents[\"text\"].apply(lambda x: preprocess_document(x))\n",
    "documents[\"tokens\"] = documents[\"preprocesado\"].apply(lambda x: delete_stop_words(x))\n",
    "# documents[\"lematizado\"] = documents[\"preprocesado\"].apply(lambda x: lemmatize(x))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:38.703789Z",
     "start_time": "2020-12-20T21:31:38.695788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ==> 45 documents\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_health = documents[documents[\"class\"] == \"health\"].iloc[:15]\n",
    "train_politics = documents[documents[\"class\"] == \"politics\"].iloc[:15]\n",
    "train_sports = documents[documents[\"class\"] == \"sports\"].iloc[:15]\n",
    "\n",
    "train_data = pd.concat([train_health, train_politics, train_sports])\n",
    "print(f\"Training data ==> {len(train_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:38.711789Z",
     "start_time": "2020-12-20T21:31:38.704788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data ==> 105 documents\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_health = documents[documents[\"class\"] == \"health\"].iloc[15:]\n",
    "test_politics = documents[documents[\"class\"] == \"politics\"].iloc[15:]\n",
    "test_sports = documents[documents[\"class\"] == \"sports\"].iloc[15:]\n",
    "\n",
    "test_data = pd.concat([test_health, test_politics, test_sports])\n",
    "test_data.reset_index(inplace = True)\n",
    "print(f\"Testing data ==> {len(test_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:38.716789Z",
     "start_time": "2020-12-20T21:31:38.712788Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(documents, threshold):\n",
    "    token_ = [doc.split(\" \") for doc in documents]\n",
    "    bigram = Phrases(token_, min_count=1, threshold=threshold, delimiter=b' ')\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    bigram_token = []\n",
    "    for sent in token_:\n",
    "        for bigram in bigram_phraser[sent]:\n",
    "            if len(bigram.split(\" \")) > 1: # comprobamos que realmente es un bigrama\n",
    "                bigram_token.append(bigram) \n",
    "    return list(set(bigram_token))\n",
    "           \n",
    "def check_bigram(x, bigrams):\n",
    "    return [bigram for bigram in bigrams if x.find(bigram) != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.418790Z",
     "start_time": "2020-12-20T21:31:38.717789Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_sports = get_bigrams(train_sports[\"preprocesado\"].values, 50)\n",
    "bigrams_health = get_bigrams(train_health[\"preprocesado\"].values, 50)\n",
    "bigrams_politics = get_bigrams(train_politics[\"preprocesado\"].values, 50)\n",
    "bigrams = get_bigrams(train_data[\"preprocesado\"].values, 50)\n",
    "\n",
    "\n",
    "train_sports[\"bigrams\"] = train_sports[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_sports))\n",
    "train_health[\"bigrams\"] = train_health[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_health))\n",
    "train_politics[\"bigrams\"] = train_politics[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_politics))\n",
    "\n",
    "train_sports[\"tokens + bigrams\"] = train_sports[\"tokens\"] + train_sports[\"bigrams\"]\n",
    "train_health[\"tokens + bigrams\"] = train_health[\"tokens\"] + train_health[\"bigrams\"]\n",
    "train_politics[\"tokens + bigrams\"] = train_politics[\"tokens\"] + train_politics[\"bigrams\"]\n",
    "\n",
    "train_data[\"bigrams\"] = train_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "train_data[\"tokens + bigrams\"] = train_data[\"tokens\"] + train_data[\"bigrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.421790Z",
     "start_time": "2020-12-20T21:31:39.419790Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_dir = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.431789Z",
     "start_time": "2020-12-20T21:31:39.422791Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_tfidf_keywords(df, k):\n",
    "    tokens = df[\"tokens + bigrams\"].values\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    bow = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    bow_tfidf = tfidf[bow]\n",
    "    tfidf_dic = {dictionary.get(id): value for doc in bow_tfidf for id, value in doc}\n",
    "    tfidf_list = [k for k, v in sorted(tfidf_dic.items(), key=lambda item: item[1], reverse = True)]\n",
    "    return tfidf_list[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.534789Z",
     "start_time": "2020-12-20T21:31:39.432790Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health = get_k_tfidf_keywords(train_health, 100)\n",
    "keywords_tfidf_politics = get_k_tfidf_keywords(train_politics, 100)\n",
    "keywords_tfidf_sports = get_k_tfidf_keywords(train_sports, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.538789Z",
     "start_time": "2020-12-20T21:31:39.535790Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(d1, d2, d3):\n",
    "\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    \n",
    "    for key in deleted:\n",
    "        try:\n",
    "            d1.remove(key)\n",
    "        except:\n",
    "            print(f\"D1 no tiene {key}\")\n",
    "        try:\n",
    "            d2.remove(key)\n",
    "        except:\n",
    "            print(f\"D2 no tiene {key}\")\n",
    "        try:\n",
    "            d3.remove(key)\n",
    "        except:\n",
    "            print(f\"D3 no tiene {key}\")\n",
    "            \n",
    "    return d1, d2, d3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.545788Z",
     "start_time": "2020-12-20T21:31:39.538789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D3 no tiene gestión\n",
      "D1 no tiene grupos\n"
     ]
    }
   ],
   "source": [
    "keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports = remove_duplicates(keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.549788Z",
     "start_time": "2020-12-20T21:31:39.546789Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_table_keywords(health, sports, politics):\n",
    "    aa = pd.DataFrame(health, columns=[\"health\"])\n",
    "    aa[\"sports\"] = pd.Series(sports)\n",
    "    aa[\"politics\"] = pd.Series(politics)\n",
    "    print(aa.to_latex(bold_rows = True, column_format = \"lll\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.553789Z",
     "start_time": "2020-12-20T21:31:39.550789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print_table_keywords(keywords_tfidf_health, keywords_tfidf_sports, keywords_tfidf_politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:39.558788Z",
     "start_time": "2020-12-20T21:31:39.553789Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_gensim_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens + bigrams\"].apply(lambda x: \" \".join(x))\n",
    "    data['joined'] = data.joined.astype(str)\n",
    "    data = \" \".join(data[\"joined\"].values)\n",
    "    return [key[0] for key in keywords(data, scores=True, words=k, pos_filter=('NNP', 'JJ', \"NNPS\", \"VB\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.642788Z",
     "start_time": "2020-12-20T21:31:39.558788Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D3 no tiene decadas\n",
      "D3 no tiene tipo\n",
      "D3 no tiene investigacion\n",
      "D2 no tiene llegar\n",
      "D3 no tiene medidas\n",
      "D3 no tiene proceso\n",
      "D2 no tiene herramientas\n",
      "D3 no tiene efecto\n",
      "D1 no tiene numeros\n",
      "D2 no tiene maximo\n",
      "D3 no tiene objetivos\n",
      "D1 no tiene nacional\n",
      "D1 no tiene dudas\n",
      "D1 no tiene duda\n",
      "D1 no tiene espanol\n",
      "D2 no tiene record\n",
      "D1 no tiene entrevista\n",
      "D3 no tiene frente\n",
      "D3 no tiene pais\n",
      "D3 no tiene sistema\n",
      "D3 no tiene familias\n",
      "D1 no tiene linea\n",
      "D3 no tiene hablar\n",
      "D3 no tiene control\n",
      "D1 no tiene proyecto\n",
      "D3 no tiene palabras\n",
      "D3 no tiene papel\n",
      "D3 no tiene mundo\n",
      "D3 no tiene cambio\n",
      "D2 no tiene horas\n",
      "D3 no tiene decada\n",
      "D3 no tiene edad\n",
      "D1 no tiene visita\n",
      "D3 no tiene caso\n",
      "D3 no tiene general\n",
      "D1 no tiene punto\n",
      "D1 no tiene condiciones\n",
      "D2 no tiene opcion\n",
      "D1 no tiene espanoles\n",
      "D3 no tiene autonomicos\n",
      "D3 no tiene palabra\n",
      "D1 no tiene servicios\n",
      "D3 no tiene casos\n",
      "D1 no tiene real\n",
      "D2 no tiene pruebas\n",
      "D1 no tiene espacio\n",
      "D2 no tiene importantes\n",
      "D3 no tiene medida\n",
      "D3 no tiene ley\n",
      "D2 no tiene importante\n",
      "D3 no tiene presidentes\n",
      "D3 no tiene comunidad\n",
      "D2 no tiene ideas\n",
      "D3 no tiene media\n",
      "D3 no tiene millon\n",
      "D3 no tiene coronavirus\n",
      "D3 no tiene comunidades\n",
      "D3 no tiene han\n",
      "D2 no tiene dificil\n",
      "D3 no tiene ejecutivo\n",
      "D2 no tiene resultado\n",
      "D3 no tiene jueves\n",
      "D1 no tiene catalan\n",
      "D3 no tiene objetivo\n",
      "D3 no tiene altas\n",
      "D3 no tiene vidas\n",
      "D3 no tiene social\n",
      "D3 no tiene cabo\n",
      "D2 no tiene mundial\n",
      "D2 no tiene jovenes\n",
      "D3 no tiene conjunto\n",
      "D3 no tiene sociales\n",
      "D1 no tiene posibles\n",
      "D3 no tiene publica\n",
      "D3 no tiene puntos\n",
      "D3 no tiene decision\n",
      "D3 no tiene esta\n",
      "D2 no tiene tecnica\n",
      "D2 no tiene problemas\n",
      "D2 no tiene anos\n",
      "D3 no tiene este\n",
      "D3 no tiene contagio\n"
     ]
    }
   ],
   "source": [
    "keywords_gensim_health = get_k_gensim_keywords(train_health, 300)\n",
    "keywords_gensim_politics = get_k_gensim_keywords(train_politics, 300)\n",
    "keywords_gensim_sports = get_k_gensim_keywords(train_sports, 300)\n",
    "\n",
    "keywords_gensim_health, keywords_k_gensim_politics, keywords_k_gensim_sports = remove_duplicates(keywords_gensim_health, keywords_gensim_politics, keywords_gensim_sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.645788Z",
     "start_time": "2020-12-20T21:31:40.643788Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print_table_keywords(keywords_gensim_health, keywords_k_gensim_sports, keywords_k_gensim_politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.658789Z",
     "start_time": "2020-12-20T21:31:40.646787Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_kmeans_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    k_means_data = data[\"joined\"].values\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(k_means_data)\n",
    "    \n",
    "    model = KMeans(n_clusters=3, init='k-means++', max_iter=1000, n_init=1, random_state = 5, algorithm=\"full\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    keywords_kmeans_politics = [terms[ind] for ind in order_centroids[0, :k]]\n",
    "    keywords_kmeans_health = [terms[ind] for ind in order_centroids[1, :k]]\n",
    "    keywords_kmeans_sports = [terms[ind] for ind in order_centroids[2, :k]]\n",
    "    \n",
    "    return keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.791787Z",
     "start_time": "2020-12-20T21:31:40.661789Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2 no tiene semana\n",
      "D2 no tiene sistema\n",
      "D2 no tiene sánchez\n",
      "D1 no tiene salud\n",
      "D2 no tiene comisión\n",
      "D3 no tiene juego\n",
      "D3 no tiene español\n",
      "D2 no tiene horas\n",
      "D2 no tiene enfermedad\n",
      "D2 no tiene ley\n",
      "D2 no tiene fiscal\n",
      "D2 no tiene forma\n",
      "D2 no tiene año\n",
      "D1 no tiene media\n",
      "D2 no tiene goles\n",
      "D2 no tiene día\n",
      "D2 no tiene presupuestos\n",
      "D2 no tiene años\n",
      "D1 no tiene andalucía\n",
      "D2 no tiene gobierno\n"
     ]
    }
   ],
   "source": [
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = get_k_kmeans_keywords(train_data, 100)\n",
    "# print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))\n",
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = remove_duplicates(keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports)\n",
    "# print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.795790Z",
     "start_time": "2020-12-20T21:31:40.793787Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print_table_keywords(keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formación del glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.801788Z",
     "start_time": "2020-12-20T21:31:40.796789Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_relevant_keywords(d1, d2, d3):\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    return deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.807789Z",
     "start_time": "2020-12-20T21:31:40.802789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics ==>  ['enmiendas', 'catalanes', 'fusión', 'congreso', 'sociedad', 'estatal', 'portavoz', 'catalunya', 'obligaciones', 'hija', 'texto', 'exterior', 'expresidente', 'carrillo', 'partido', 'rodríguez', 'pnl', 'ministro', 'montero', 'lastra', 'informe', 'interior', 'diputadas', 'penal', 'pnv', 'migrantes', 'instalaciones', 'cajas', 'eutanasia', 'ministra', 'delito', 'callar', 'investigación', 'libertad', 'vuelos', 'pleno', 'defensa', 'canarias', 'precio', 'populares', 'militar', 'elena', 'gonzález', 'socialista', 'decisiones']\n",
      "Sports ==>  ['champions', 'femenino', 'sevilla', 'alfredo', 'dechambeau', 'inglaterra', 'leyenda', 'wilkinson', 'coe', 'liga', 'decidir', 'hoyo', 'bogey', 'equipos', 'ademarista', 'candidatura', 'campeonato', 'pelotas', 'bola', 'coi', 'debut', 'ademar', 'competir', 'necesitamos', 'virus', 'azulgrana', 'salamanca', 'rafa', 'putt', 'puestos', 'haaland', 'regresa', 'documental', 'par', 'industrias', 'goles', 'campeón', 'nadal', 'bolt', 'hamilton', 'koeman', 'talento', 'generalitat', 'millones', 'grupo', 'habla']\n",
      "Health ==>  ['viernes', 'microbios', 'grasas', 'teléfonos', 'estudios', 'lesiones', 'masa', 'sentían', 'valores', 'patógenos', 'magra', 'bacterias', 'encierro', 'alimentaria', 'mascotas', 'animales', 'vitamina', 'dueños', 'confinamiento', 'compañía', 'placebo', 'soledad', 'gestor', 'prematura', 'probable', 'dosis', 'animal', 'vph', 'vacuna', 'hospital', 'verrugas', 'personalidad', 'urinaria', 'profesionales', 'tuberculosis', 'móviles', 'mascota', 'casa', 'enfermera', 'facebook', 'factores', 'robot', 'metabolismo', 'cuerpo', 'sexual', 'metabólica', 'familia', 'ensayos', 'prematuras', 'mental', 'fruta']\n"
     ]
    }
   ],
   "source": [
    "relevant_keywords_politics = list(check_relevant_keywords(keywords_kmeans_politics, keywords_gensim_politics, keywords_tfidf_politics))\n",
    "relevant_keywords_health = list(check_relevant_keywords(keywords_kmeans_health, keywords_gensim_health, keywords_tfidf_health))\n",
    "relevant_keywords_sports = list(check_relevant_keywords(keywords_kmeans_sports, keywords_gensim_sports, keywords_tfidf_sports))\n",
    "\n",
    "print(\"Politics ==> \", relevant_keywords_politics)\n",
    "print(\"Sports ==> \", relevant_keywords_sports)\n",
    "print(\"Health ==> \", relevant_keywords_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.811789Z",
     "start_time": "2020-12-20T21:31:40.808789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print_table_keywords(relevant_keywords_health, relevant_keywords_sports, relevant_keywords_politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.815788Z",
     "start_time": "2020-12-20T21:31:40.812789Z"
    }
   },
   "outputs": [],
   "source": [
    "path_keys_health = \"../keywords/keys_health.txt\"\n",
    "path_keys_sports = \"../keywords/keys_sports.txt\"\n",
    "path_keys_politics = \"../keywords/keys_politics.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:40.820790Z",
     "start_time": "2020-12-20T21:31:40.816789Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_health = [key.strip() for key in open(path_keys_health, encoding=\"utf-8\").readlines()]\n",
    "keys_sports = [key.strip() for key in open(path_keys_sports, encoding=\"utf-8\").readlines()]\n",
    "keys_politics = [key.strip() for key in open(path_keys_politics, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "keys_dic = {-1: \"unknown\", 0: \"health\", 1: \"sports\", 2: \"politics\"}\n",
    "inverted_keys_dic = {\"unknown\": -1, \"health\": 0, \"sports\": 1, \"politics\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramas de test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.060791Z",
     "start_time": "2020-12-20T21:31:40.821790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>tokens + bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>health_47.txt</td>\n",
       "      <td>La Organización Mundial de la Salud (OMS) ha a...</td>\n",
       "      <td>health</td>\n",
       "      <td>la organización mundial de la salud oms ha act...</td>\n",
       "      <td>[organización, mundial, salud, oms, actualizad...</td>\n",
       "      <td>[las mujeres, este jueves, hasta ahora, sino t...</td>\n",
       "      <td>[organización, mundial, salud, oms, actualizad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>health_14.txt</td>\n",
       "      <td>Viven entre nosotros, puede ser ese hombre que...</td>\n",
       "      <td>health</td>\n",
       "      <td>viven entre nosotros puede ser ese hombre que ...</td>\n",
       "      <td>[viven, hombre, camina, prisa, lluvia, aquella...</td>\n",
       "      <td>[muchos casos, se trata, e ir, él mismo, media...</td>\n",
       "      <td>[viven, hombre, camina, prisa, lluvia, aquella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>health_2.txt</td>\n",
       "      <td>Las autoridades de Senegal han comenzado una i...</td>\n",
       "      <td>health</td>\n",
       "      <td>las autoridades de senegal han comenzado una i...</td>\n",
       "      <td>[autoridades, senegal, comenzado, investigació...</td>\n",
       "      <td>[se trata, presenta una, sin embargo, una seri...</td>\n",
       "      <td>[autoridades, senegal, comenzado, investigació...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>health_31.txt</td>\n",
       "      <td>Tres de los proyectos de la vacuna contra el c...</td>\n",
       "      <td>health</td>\n",
       "      <td>tres de los proyectos de la vacuna contra el c...</td>\n",
       "      <td>[proyectos, vacuna, covid, investigan, españa,...</td>\n",
       "      <td>[no obstante, este martes, son suficientes]</td>\n",
       "      <td>[proyectos, vacuna, covid, investigan, españa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>health_17.txt</td>\n",
       "      <td>En lo que se refiere a nuestro aparato cardioc...</td>\n",
       "      <td>health</td>\n",
       "      <td>en lo que se refiere a nuestro aparato cardioc...</td>\n",
       "      <td>[refiere, aparato, cardiocirculatorio, corazón...</td>\n",
       "      <td>[se trata, e ir, cada uno, puede llegar, las c...</td>\n",
       "      <td>[refiere, aparato, cardiocirculatorio, corazón...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>102</td>\n",
       "      <td>sports_11.txt</td>\n",
       "      <td>Para muchos el nombre de Sonny Vaccaro apenas ...</td>\n",
       "      <td>sports</td>\n",
       "      <td>para muchos el nombre de sonny vaccaro apenas ...</td>\n",
       "      <td>[nombre, sonny, vaccaro, relevante, aficionado...</td>\n",
       "      <td>[por culpa]</td>\n",
       "      <td>[nombre, sonny, vaccaro, relevante, aficionado...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>116</td>\n",
       "      <td>sports_24.txt</td>\n",
       "      <td>Rafael Nadal se clasificó este jueves por sext...</td>\n",
       "      <td>sports</td>\n",
       "      <td>rafael nadal se clasificó este jueves por sext...</td>\n",
       "      <td>[rafael, nadal, clasificó, jueves, sexta, semi...</td>\n",
       "      <td>[ha demostrado, este jueves, sigue siendo, las...</td>\n",
       "      <td>[rafael, nadal, clasificó, jueves, sexta, semi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>124</td>\n",
       "      <td>sports_31.txt</td>\n",
       "      <td>La selección de Australia se rehizo ante Nueva...</td>\n",
       "      <td>sports</td>\n",
       "      <td>la selección de australia se rehizo ante nueva...</td>\n",
       "      <td>[selección, australia, rehizo, zelanda, termin...</td>\n",
       "      <td>[sin embargo]</td>\n",
       "      <td>[selección, australia, rehizo, zelanda, termin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>148</td>\n",
       "      <td>sports_8.txt</td>\n",
       "      <td>\\nChiellini, Bonucci, Barzagli, Zambrotta...la...</td>\n",
       "      <td>sports</td>\n",
       "      <td>\\nchiellini bonucci barzagli zambrottala lista...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "      <td>[pero sí, sin embargo, mayor parte, primera ve...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>104</td>\n",
       "      <td>sports_13.txt</td>\n",
       "      <td>ARicky Rubio todavía le dura el descontento po...</td>\n",
       "      <td>sports</td>\n",
       "      <td>aricky rubio todavía le dura el descontento po...</td>\n",
       "      <td>[aricky, rubio, dura, descontento, forma, sali...</td>\n",
       "      <td>[una estrella]</td>\n",
       "      <td>[aricky, rubio, dura, descontento, forma, sali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       doc_name                                               text  \\\n",
       "0       41  health_47.txt  La Organización Mundial de la Salud (OMS) ha a...   \n",
       "1        5  health_14.txt  Viven entre nosotros, puede ser ese hombre que...   \n",
       "2       11   health_2.txt  Las autoridades de Senegal han comenzado una i...   \n",
       "3       24  health_31.txt  Tres de los proyectos de la vacuna contra el c...   \n",
       "4        8  health_17.txt  En lo que se refiere a nuestro aparato cardioc...   \n",
       "..     ...            ...                                                ...   \n",
       "100    102  sports_11.txt  Para muchos el nombre de Sonny Vaccaro apenas ...   \n",
       "101    116  sports_24.txt  Rafael Nadal se clasificó este jueves por sext...   \n",
       "102    124  sports_31.txt  La selección de Australia se rehizo ante Nueva...   \n",
       "103    148   sports_8.txt  \\nChiellini, Bonucci, Barzagli, Zambrotta...la...   \n",
       "104    104  sports_13.txt  ARicky Rubio todavía le dura el descontento po...   \n",
       "\n",
       "      class                                       preprocesado  \\\n",
       "0    health  la organización mundial de la salud oms ha act...   \n",
       "1    health  viven entre nosotros puede ser ese hombre que ...   \n",
       "2    health  las autoridades de senegal han comenzado una i...   \n",
       "3    health  tres de los proyectos de la vacuna contra el c...   \n",
       "4    health  en lo que se refiere a nuestro aparato cardioc...   \n",
       "..      ...                                                ...   \n",
       "100  sports  para muchos el nombre de sonny vaccaro apenas ...   \n",
       "101  sports  rafael nadal se clasificó este jueves por sext...   \n",
       "102  sports  la selección de australia se rehizo ante nueva...   \n",
       "103  sports  \\nchiellini bonucci barzagli zambrottala lista...   \n",
       "104  sports  aricky rubio todavía le dura el descontento po...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [organización, mundial, salud, oms, actualizad...   \n",
       "1    [viven, hombre, camina, prisa, lluvia, aquella...   \n",
       "2    [autoridades, senegal, comenzado, investigació...   \n",
       "3    [proyectos, vacuna, covid, investigan, españa,...   \n",
       "4    [refiere, aparato, cardiocirculatorio, corazón...   \n",
       "..                                                 ...   \n",
       "100  [nombre, sonny, vaccaro, relevante, aficionado...   \n",
       "101  [rafael, nadal, clasificó, jueves, sexta, semi...   \n",
       "102  [selección, australia, rehizo, zelanda, termin...   \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...   \n",
       "104  [aricky, rubio, dura, descontento, forma, sali...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    [las mujeres, este jueves, hasta ahora, sino t...   \n",
       "1    [muchos casos, se trata, e ir, él mismo, media...   \n",
       "2    [se trata, presenta una, sin embargo, una seri...   \n",
       "3          [no obstante, este martes, son suficientes]   \n",
       "4    [se trata, e ir, cada uno, puede llegar, las c...   \n",
       "..                                                 ...   \n",
       "100                                        [por culpa]   \n",
       "101  [ha demostrado, este jueves, sigue siendo, las...   \n",
       "102                                      [sin embargo]   \n",
       "103  [pero sí, sin embargo, mayor parte, primera ve...   \n",
       "104                                     [una estrella]   \n",
       "\n",
       "                                      tokens + bigrams  \n",
       "0    [organización, mundial, salud, oms, actualizad...  \n",
       "1    [viven, hombre, camina, prisa, lluvia, aquella...  \n",
       "2    [autoridades, senegal, comenzado, investigació...  \n",
       "3    [proyectos, vacuna, covid, investigan, españa,...  \n",
       "4    [refiere, aparato, cardiocirculatorio, corazón...  \n",
       "..                                                 ...  \n",
       "100  [nombre, sonny, vaccaro, relevante, aficionado...  \n",
       "101  [rafael, nadal, clasificó, jueves, sexta, semi...  \n",
       "102  [selección, australia, rehizo, zelanda, termin...  \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...  \n",
       "104  [aricky, rubio, dura, descontento, forma, sali...  \n",
       "\n",
       "[105 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"bigrams\"] = test_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "test_data[\"tokens + bigrams\"] = test_data[\"tokens\"] + test_data[\"bigrams\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.063792Z",
     "start_time": "2020-12-20T21:31:41.061791Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.070790Z",
     "start_time": "2020-12-20T21:31:41.063792Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(glossary for glossary in glossaries)\n",
    "dictionary.save('keys.dict')  # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.074788Z",
     "start_time": "2020-12-20T21:31:41.070790Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, docs, dictionary):\n",
    "        self.docs = docs\n",
    "        self.dict = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dict.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.078791Z",
     "start_time": "2020-12-20T21:31:41.075791Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = MyCorpus(glossaries, dictionary)\n",
    "corpora.MmCorpus.serialize(\"keys.mm\", bow, metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.085790Z",
     "start_time": "2020-12-20T21:31:41.079792Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "index = Similarity(index_temp, bow, num_features=len(dictionary))  # create index\n",
    "index.save(\"keys.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.089790Z",
     "start_time": "2020-12-20T21:31:41.086790Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tfidf = models.TfidfModel(bow,smartirs=\"lpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.094789Z",
     "start_time": "2020-12-20T21:31:41.089790Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_document_tfidf(model, dictionary, bow, index, documents, i, verbose = False):\n",
    "    \"\"\"\n",
    "    Given a specific document, computes the ranking of the classes and returns the current class, \n",
    "    the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    document = documents.iloc[i]\n",
    "    pq = document[\"tokens + bigrams\"]\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = model[vq]\n",
    "    sim = index[qtfidf]\n",
    "\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "            \n",
    "    \n",
    "    return [i, get_info_document(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.099791Z",
     "start_time": "2020-12-20T21:31:41.095791Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.sum(sim) == 0.0:# or len(np.unique(sim)) != 3:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/3, 1/3, 1/3])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        tfidf_scores = np.array(sim)\n",
    "        probabilities = tfidf_scores / np.sum(tfidf_scores)\n",
    "    \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.105791Z",
     "start_time": "2020-12-20T21:31:41.100790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> En lo que se refiere a nuestro aparato cardiocirculatorio (corazón, venas y arterias), es importante\n",
      "[health] ==> 0.438\n",
      "[sports] ==> 0.000\n",
      "[politics] ==> 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " {'current_class': 0,\n",
       "  'predicted_class': 0,\n",
       "  'probabilities': array([1., 0., 0.], dtype=float32)}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_document_tfidf(model_tfidf, dictionary, bow, index, test_data, 4, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.110790Z",
     "start_time": "2020-12-20T21:31:41.106789Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_tfidf(function, model, dictionary, bow, index, data):\n",
    "    def classify(doc_i):\n",
    "        return function(model, dictionary, bow, index, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.155789Z",
     "start_time": "2020-12-20T21:31:41.111792Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics]\n",
    "model_w2v = models.Word2Vec(sentences = glossaries, window = 5, workers = 12, min_count = 1, seed=50)\n",
    "\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.159790Z",
     "start_time": "2020-12-20T21:31:41.156789Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_min(model):\n",
    "    vocab = model.wv.vocab\n",
    "    \n",
    "    maxs = []\n",
    "    mins = []\n",
    "    \n",
    "    for key in vocab:\n",
    "        maxs.append(max(model.wv[key]))\n",
    "        mins.append(min(model.wv[key]))\n",
    "    return max(maxs), min(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.166788Z",
     "start_time": "2020-12-20T21:31:41.160789Z"
    }
   },
   "outputs": [],
   "source": [
    "MAXI, MINI = get_max_min(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.173790Z",
     "start_time": "2020-12-20T21:31:41.167788Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_from_document(model, document):\n",
    "    embeddings = []\n",
    "    \n",
    "    for word in document:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else: # no está en el vocab\n",
    "            embeddings.append(np.random.uniform(low = MINI, high = MAXI, size = 100))\n",
    "    \n",
    "    return np.mean(embeddings, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.178788Z",
     "start_time": "2020-12-20T21:31:41.174789Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries_vector = [get_embeddings_from_document(model_w2v, glossary) for glossary in glossaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.183789Z",
     "start_time": "2020-12-20T21:31:41.179789Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document_w2v(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(sim) == 0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/3, 1/3, 1/3])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        w2v_scores = np.array(sim, dtype = \"float32\")\n",
    "        probabilities = (w2v_scores - w2v_scores.min()) / (w2v_scores.max() - w2v_scores.min()) \n",
    "        probabilities /= np.sum(probabilities)\n",
    "\n",
    "        \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.188789Z",
     "start_time": "2020-12-20T21:31:41.184789Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_w2v(glossaries_vector, model, documents, i, verbose = False):\n",
    "    document = documents.iloc[i]\n",
    "    doc_vector = get_embeddings_from_document(model, document[\"tokens + bigrams\"])\n",
    "\n",
    "    ranking = [[i, cosine_similarity(np.array(doc_vector).reshape(1,-1), np.array(glossary).reshape(1,-1)).item()] \n",
    "               for i, glossary in enumerate(glossaries_vector)]\n",
    "    \n",
    "    sim = [rank[1] for rank in ranking] \n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, get_info_document_w2v(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.197790Z",
     "start_time": "2020-12-20T21:31:41.189789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> En lo que se refiere a nuestro aparato cardiocirculatorio (corazón, venas y arterias), es importante\n",
      "Scores:\n",
      "[health] ==> 0.223\n",
      "[sports] ==> -0.013\n",
      "[politics] ==> -0.065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " {'current_class': 0,\n",
       "  'predicted_class': 0,\n",
       "  'probabilities': array([0.8469854 , 0.15301454, 0.        ], dtype=float32)}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_doc_w2v(glossaries_vector, model_w2v, test_data, 4, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.200791Z",
     "start_time": "2020-12-20T21:31:41.197790Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_w2v(function, glossaries_vector, model, data):\n",
    "    def classify(doc_i):\n",
    "        return function(glossaries_vector, model, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.208788Z",
     "start_time": "2020-12-20T21:31:41.201789Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_bayes(classifier, documents, i, verbose = False):\n",
    "    doc = documents.iloc[i]\n",
    "    #doc = vectorizer.transform([document[\"preprocesado\"]])\n",
    "    \n",
    "    y_pred = classifier.predict_proba([doc[\"preprocesado\"]])[0]\n",
    "    \n",
    "    ranking = [[i,prob] for i,prob in enumerate(y_pred)]\n",
    "    \n",
    "    probabilities = y_pred\n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    current_class = inverted_keys_dic[doc[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(y_pred > 1/3) > 1 and len(np.unique(y_pred)) < 3:\n",
    "        predicted_class = -1\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + doc[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.212791Z",
     "start_time": "2020-12-20T21:31:41.209791Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_bayes(function, clf, documents):\n",
    "    def classify(doc_i):\n",
    "        return function(clf, documents, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.227789Z",
     "start_time": "2020-12-20T21:31:41.213789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None,\n",
       "                             fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossaries_joined = [\" \".join(gloss) for gloss in glossaries]\n",
    "\n",
    "X_train = glossaries_joined\n",
    "y_train = [0,1,2]\n",
    "\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', BernoulliNB(alpha=0.1))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.233789Z",
     "start_time": "2020-12-20T21:31:41.228788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> Eso fue lo que afirmaron los expertos de la Escuela de Medicina de Harvard en su informe 'Core exerc\n",
      "Scores:\n",
      "[health] ==> 1.000\n",
      "[politics] ==> 0.000\n",
      "[sports] ==> 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " {'current_class': 0,\n",
       "  'predicted_class': 0,\n",
       "  'probabilities': array([9.99993740e-01, 5.13154906e-08, 6.20917436e-06])}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_doc_bayes(clf, test_data, 6, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.240789Z",
     "start_time": "2020-12-20T21:31:41.234789Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_documents(test_data, classify):\n",
    "    \"\"\"\n",
    "    Classifies the documents given a specific classification function.\n",
    "    \n",
    "    \"\"\"\n",
    "    test_data = test_data.copy()\n",
    "    \n",
    "    infos = [classify(i) for i in range(len(test_data))]\n",
    "    data = fill_test_data(test_data, infos)\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.251790Z",
     "start_time": "2020-12-20T21:31:41.241790Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_test_data(test_data, infos):\n",
    "    \"\"\"\n",
    "    Auxiliary function to fill the dataframe with info about the classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    current_class = pd.Series([info[1][\"current_class\"] for info in infos])\n",
    "    predicted_class = pd.Series([info[1][\"predicted_class\"] for info in infos])\n",
    "    p_health = pd.Series([info[1][\"probabilities\"][0] for info in infos])\n",
    "    p_sports = pd.Series([info[1][\"probabilities\"][1] for info in infos])\n",
    "    p_politics = pd.Series([info[1][\"probabilities\"][2] for info in infos])\n",
    "\n",
    "    data[\"current_class\"] = current_class\n",
    "    data[\"predicted_class\"] = predicted_class\n",
    "    data[\"p_health\"] = p_health\n",
    "    data[\"p_sports\"] = p_sports\n",
    "    data[\"p_politics\"] = p_politics\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.260790Z",
     "start_time": "2020-12-20T21:31:41.252790Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename(df):\n",
    "    \"\"\"\n",
    "    Computes the filename for each document based on the performed classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    confidence = \"%.3f\" % df[\"confidence\"]\n",
    "    current_class = df[\"class\"]\n",
    "    predicted_class = df[\"predicted_class_name\"]\n",
    "    correct = current_class == predicted_class\n",
    "    name = df[\"doc_name\"].split(\".\")[0]\n",
    "    \n",
    "    return f\"../classification/{predicted_class}/{confidence}_{name}-{correct}-{current_class}-{predicted_class}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.267789Z",
     "start_time": "2020-12-20T21:31:41.261790Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(path, content):\n",
    "    \"\"\"\n",
    "    Writes a file given its path and content.\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.279789Z",
     "start_time": "2020-12-20T21:31:41.268789Z"
    }
   },
   "outputs": [],
   "source": [
    "def move_files(data, tables=True):\n",
    "    \"\"\"\n",
    "    Moves the files to their corresponding new directory after classification is done.\n",
    "\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    classes = [[0, \"p_health\"], [1, \"p_sports\"], [2, \"p_politics\"]]\n",
    "\n",
    "    for cl in classes:\n",
    "        docs = data[data[\"current_class\"] == cl[0]]\n",
    "        docs = docs.sort_values(by=[cl[1]], ascending=False)\n",
    "        docs[\"predicted_class_name\"] = docs[\"predicted_class\"].apply(\n",
    "            lambda x: keys_dic[x])\n",
    "        docs[\"confidence\"] = docs[[\"p_health\",\n",
    "                                   \"p_sports\", \"p_politics\"]].max(axis=1)\n",
    "        docs[\"file\"] = docs.apply(lambda x: get_filename(x), axis=1)\n",
    "        docs.apply(lambda row: write_file(row[\"file\"], row[\"text\"]), axis=1)\n",
    "        if tables:\n",
    "            # tabla para la memoria\n",
    "            docs = docs[[\"doc_name\", \"class\", \"p_health\",\n",
    "                         \"p_sports\", \"p_politics\", \"predicted_class_name\"]]\n",
    "            docs = docs.rename(\n",
    "                columns={\"predicted_class_name\": \"predicted_class\"})\n",
    "            print(docs.to_latex(bold_rows=True, float_format=\"%.2f\",\n",
    "                                column_format=\"llllll\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:41.284789Z",
     "start_time": "2020-12-20T21:31:41.280788Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute(test_data, classification_function, move = True, tables = False):\n",
    "    \"\"\"\n",
    "    General function that classifies the documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    print(\"############################################################\")\n",
    "    print(\"Starting document´s classification...\")\n",
    "    data = classify_documents(data, classification_function)\n",
    "    sleep(1)\n",
    "    print(\"Document´s classification done...\")\n",
    "    if move:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        sleep(1)\n",
    "        print(\"Moving files to the correct directories...\")\n",
    "        move_files(data)\n",
    "        sleep(1)\n",
    "        print(\"Files moved.\")\n",
    "    print(\"############################################################\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación\n",
    "\n",
    "Descomente solo la línea del modelo que quiera probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:44.459039Z",
     "start_time": "2020-12-20T21:31:41.285790Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Starting document´s classification...\n",
      "Document´s classification done...\n",
      "-----------------------------------------------------------\n",
      "Moving files to the correct directories...\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "      doc\\_name &   class &  p\\_health &  p\\_sports &  p\\_politics & predicted\\_class \\\\\n",
      "\\midrule\n",
      " health\\_14.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_17.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_24.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_23.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_28.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  health\\_2.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_45.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_10.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_16.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_49.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  health\\_1.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  health\\_3.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  health\\_7.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_43.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_13.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_35.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  health\\_6.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_25.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  health\\_9.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_47.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_29.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_48.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_31.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_46.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_34.txt &  health &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " health\\_42.txt &  health &      0.92 &      0.08 &        0.00 &          health \\\\\n",
      " health\\_27.txt &  health &      0.92 &      0.08 &        0.00 &          health \\\\\n",
      " health\\_44.txt &  health &      0.92 &      0.08 &        0.00 &          health \\\\\n",
      " health\\_26.txt &  health &      0.92 &      0.00 &        0.08 &          health \\\\\n",
      " health\\_39.txt &  health &      0.08 &      0.91 &        0.01 &          sports \\\\\n",
      " health\\_38.txt &  health &      0.08 &      0.91 &        0.01 &          sports \\\\\n",
      " health\\_40.txt &  health &      0.08 &      0.01 &        0.91 &        politics \\\\\n",
      " health\\_33.txt &  health &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "  health\\_4.txt &  health &      0.00 &      0.99 &        0.01 &          sports \\\\\n",
      " health\\_37.txt &  health &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "      doc\\_name &   class &  p\\_health &  p\\_sports &  p\\_politics & predicted\\_class \\\\\n",
      "\\midrule\n",
      "  sports\\_1.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "  sports\\_9.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_16.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_12.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_42.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_14.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "  sports\\_5.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_13.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_15.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "  sports\\_6.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_34.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_41.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "  sports\\_8.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_24.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_11.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_10.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_49.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "  sports\\_7.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_19.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_29.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_22.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_44.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_45.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_48.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_26.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_28.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      "  sports\\_4.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_31.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_17.txt &  sports &      0.00 &      1.00 &        0.00 &          sports \\\\\n",
      " sports\\_50.txt &  sports &      0.08 &      0.92 &        0.00 &          sports \\\\\n",
      " sports\\_36.txt &  sports &      0.08 &      0.92 &        0.00 &          sports \\\\\n",
      " sports\\_27.txt &  sports &      0.08 &      0.91 &        0.01 &          sports \\\\\n",
      " sports\\_39.txt &  sports &      0.08 &      0.91 &        0.01 &          sports \\\\\n",
      " sports\\_32.txt &  sports &      0.08 &      0.91 &        0.01 &          sports \\\\\n",
      " sports\\_25.txt &  sports &      0.08 &      0.01 &        0.91 &        politics \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "        doc\\_name &     class &  p\\_health &  p\\_sports &  p\\_politics & predicted\\_class \\\\\n",
      "\\midrule\n",
      " politics\\_28.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_15.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_18.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_32.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_30.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      "  politics\\_4.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_43.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_33.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_19.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_14.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_27.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_40.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_46.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_34.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_26.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      "  politics\\_8.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      "  politics\\_9.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_17.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_47.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_44.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_38.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      "  politics\\_1.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_10.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_25.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      " politics\\_50.txt &  politics &      0.00 &      0.00 &        1.00 &        politics \\\\\n",
      "  politics\\_7.txt &  politics &      0.00 &      0.01 &        0.99 &        politics \\\\\n",
      " politics\\_11.txt &  politics &      0.08 &      0.00 &        0.92 &        politics \\\\\n",
      " politics\\_37.txt &  politics &      0.00 &      0.50 &        0.50 &        politics \\\\\n",
      " politics\\_42.txt &  politics &      0.00 &      0.50 &        0.50 &        politics \\\\\n",
      " politics\\_49.txt &  politics &      0.04 &      0.48 &        0.48 &         unknown \\\\\n",
      " politics\\_31.txt &  politics &      0.92 &      0.00 &        0.08 &          health \\\\\n",
      "  politics\\_3.txt &  politics &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  politics\\_2.txt &  politics &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "  politics\\_6.txt &  politics &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      " politics\\_21.txt &  politics &      1.00 &      0.00 &        0.00 &          health \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "#data = execute(test_data, classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))#\n",
    "#data = execute(test_data, classify_w2v(classify_doc_w2v, glossaries_vector, model_w2v, test_data))\n",
    "data = execute(test_data, classify_bayes(classify_doc_bayes, clf, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:44.464043Z",
     "start_time": "2020-12-20T21:31:44.460040Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_model(data, model, classify_function):\n",
    "    \"\"\"\n",
    "    Function that evaluates the performance of a specific model.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    print(\"#######################################################\")\n",
    "    print(\"Evaluating \"+ model + \"...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    data = classify_documents(data, classify_function)\n",
    "    \n",
    "    y_true = data[\"current_class\"]\n",
    "    y_pred = data[\"predicted_class\"]\n",
    "    original = len(y_pred)\n",
    "    y_pred = data[data[\"predicted_class\"] != -1][\"predicted_class\"]\n",
    "    unknown = original - len(y_pred)\n",
    "    y_true = y_true.loc[y_pred.index]\n",
    "        \n",
    "    print(classification_report(y_true, y_pred, target_names=[\"health\", \"sports\", \"politics\"]))\n",
    "    print(\"Confusion matrix ==>\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    print(f\"{unknown} documents couldn´t been classified.\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Model evaluated\")\n",
    "    print(\"#######################################################\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:44.510040Z",
     "start_time": "2020-12-20T21:31:44.465043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating tf-idf...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.75      0.86      0.80        35\n",
      "      sports       0.85      0.88      0.86        32\n",
      "    politics       1.00      0.82      0.90        34\n",
      "\n",
      "    accuracy                           0.85       101\n",
      "   macro avg       0.87      0.85      0.85       101\n",
      "weighted avg       0.87      0.85      0.85       101\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[30  5  0]\n",
      " [ 4 28  0]\n",
      " [ 6  0 28]]\n",
      "4 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "data_tfidf = evaluate_single_model(test_data, \"tf-idf\", classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:44.748040Z",
     "start_time": "2020-12-20T21:31:44.511040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating Word2Vec...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.66      0.66      0.66        35\n",
      "      sports       0.66      0.71      0.68        35\n",
      "    politics       0.66      0.60      0.63        35\n",
      "\n",
      "    accuracy                           0.66       105\n",
      "   macro avg       0.66      0.66      0.66       105\n",
      "weighted avg       0.66      0.66      0.66       105\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[23  5  7]\n",
      " [ 6 25  4]\n",
      " [ 6  8 21]]\n",
      "0 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "data_w2v = evaluate_single_model(test_data, \"Word2Vec\", classify_w2v(classify_doc_w2v, glossaries_vector, model_w2v, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:31:44.875040Z",
     "start_time": "2020-12-20T21:31:44.749041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating Bernoulli NB (tfidf)...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.85      0.83      0.84        35\n",
      "      sports       0.87      0.97      0.92        35\n",
      "    politics       0.94      0.85      0.89        34\n",
      "\n",
      "    accuracy                           0.88       104\n",
      "   macro avg       0.89      0.88      0.88       104\n",
      "weighted avg       0.89      0.88      0.88       104\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[29  5  1]\n",
      " [ 0 34  1]\n",
      " [ 5  0 29]]\n",
      "1 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "data_bayes = evaluate_single_model(test_data, \"Bernoulli NB (tfidf)\", classify_bayes(classify_doc_bayes, clf, test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
