{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:00.649272Z",
     "start_time": "2020-12-11T21:01:59.609275Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.summarization import keywords\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# Operatos\n",
    "from operator import itemgetter\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy_spanish_lemmatizer import SpacyCustomLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# statistics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# utils\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:00.655274Z",
     "start_time": "2020-12-11T21:02:00.652276Z"
    }
   },
   "outputs": [],
   "source": [
    "path_health = \"../documents/health\"\n",
    "path_politics = \"../documents/politics\"\n",
    "path_sports = \"../documents/sports\"\n",
    "path_documents = \"../documents\"\n",
    "path_stopwords = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:00.659273Z",
     "start_time": "2020-12-11T21:02:00.656275Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    return path.split(\"\\\\\")[-1], open(path,encoding='utf-8').read(), path.split(\"\\\\\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:01.249274Z",
     "start_time": "2020-12-11T21:02:00.660273Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = Parallel(n_jobs = -1)(delayed(load_document)(path) for path in glob.glob(path_documents+\"/*/*.txt\"))\n",
    "documents = pd.DataFrame(documents, columns=[\"doc_name\", \"text\", \"class\"])\n",
    "documents['text'] = documents['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:01.264273Z",
     "start_time": "2020-12-11T21:02:01.250273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health\n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health\n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health\n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health\n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:02.216272Z",
     "start_time": "2020-12-11T21:02:01.265272Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\&)|(\\%)|(\\$)|(\\€)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "    \n",
    "nlp = spacy.load(\"es\")\n",
    "lemmatizer = SpacyCustomLemmatizer()\n",
    "\n",
    "def load_stopwords(path):\n",
    "    return [line.strip() for line in open(path_stopwords, encoding = \"utf-8\").readlines()]\n",
    "\n",
    "STOP_WORDS = set(load_stopwords(path_stopwords))\n",
    "\n",
    "def delete_stop_words(doc):\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return clean\n",
    "\n",
    "def preprocess_document(document):\n",
    "    document = REPLACE_NO_SPACE.sub(NO_SPACE, document.lower())\n",
    "    document = REPLACE_WITH_SPACE.sub(SPACE, document)\n",
    "    # tokens = wordpunct_tokenize(document)\n",
    "    # tokens = delete_proper_nouns(tokens)\n",
    "    return document\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tokens = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "# TODO: REVISAR ESTO\n",
    "\n",
    "def delete_proper_nouns(tokens):\n",
    "    # Tag the tokens with their type - ie are they nouns or not\n",
    "    lTokens = pos_tag(tokens)\n",
    "    # find all the proper nouns and print them out\n",
    "    lTagDict = findtags('NNP', lTokens)\n",
    "    return [token.lower() for token in tokens if token not in lTagDict]\n",
    "    \n",
    "def findtags(tag_prefix, tagged_text):\n",
    "    \"\"\"\n",
    "    Find tokens matching the specified tag_prefix\n",
    "    \"\"\"\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    print(cfd.conditions())\n",
    "    return [list(cfd[tag].keys()) for tag in cfd.conditions()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:02.544273Z",
     "start_time": "2020-12-11T21:02:02.217272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "      <td>aceptémoslo de una vez perder peso de manera r...</td>\n",
       "      <td>[aceptémoslo, perder, peso, rápida, indolora, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "      <td>sin tiempo para hacer recuento de daños irrump...</td>\n",
       "      <td>[recuento, daños, irrumpe, ola, virus, golpear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "      <td>mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>[gente, mostrar, redes, sociales, versión, fot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "      <td>una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>[faceta, clave, frenética, lucha, global, pfiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "      <td>la curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>[curva, contagios, coronavirus, mantiene, espa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class  \\\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health   \n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health   \n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health   \n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health   \n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health   \n",
       "\n",
       "                                        preprocesado  \\\n",
       "0  aceptémoslo de una vez perder peso de manera r...   \n",
       "1  sin tiempo para hacer recuento de daños irrump...   \n",
       "2  mucha gente intenta mostrar en las redes socia...   \n",
       "3  una faceta clave en la frenética lucha global ...   \n",
       "4  la curva de contagios de coronavirus se mantie...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [aceptémoslo, perder, peso, rápida, indolora, ...  \n",
       "1  [recuento, daños, irrumpe, ola, virus, golpear...  \n",
       "2  [gente, mostrar, redes, sociales, versión, fot...  \n",
       "3  [faceta, clave, frenética, lucha, global, pfiz...  \n",
       "4  [curva, contagios, coronavirus, mantiene, espa...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[\"preprocesado\"] = documents[\"text\"].apply(lambda x: preprocess_document(x))\n",
    "documents[\"tokens\"] = documents[\"preprocesado\"].apply(lambda x: delete_stop_words(x))\n",
    "# documents[\"lematizado\"] = documents[\"preprocesado\"].apply(lambda x: lemmatize(x))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:02.553272Z",
     "start_time": "2020-12-11T21:02:02.545272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ==> 45 documents\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_health = documents[documents[\"class\"] == \"health\"].iloc[:15]\n",
    "train_politics = documents[documents[\"class\"] == \"politics\"].iloc[:15]\n",
    "train_sports = documents[documents[\"class\"] == \"sports\"].iloc[:15]\n",
    "\n",
    "train_data = pd.concat([train_health, train_politics, train_sports])\n",
    "print(f\"Training data ==> {len(train_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:02.560275Z",
     "start_time": "2020-12-11T21:02:02.554274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data ==> 105 documents\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_health = documents[documents[\"class\"] == \"health\"].iloc[15:]\n",
    "test_politics = documents[documents[\"class\"] == \"politics\"].iloc[15:]\n",
    "test_sports = documents[documents[\"class\"] == \"sports\"].iloc[15:]\n",
    "\n",
    "test_data = pd.concat([test_health, test_politics, test_sports])\n",
    "test_data.reset_index(inplace = True)\n",
    "print(f\"Testing data ==> {len(test_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:02.567273Z",
     "start_time": "2020-12-11T21:02:02.561274Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(documents, threshold):\n",
    "    token_ = [doc.split(\" \") for doc in documents]\n",
    "    bigram = Phrases(token_, min_count=1, threshold=threshold, delimiter=b' ')\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    bigram_token = []\n",
    "    for sent in token_:\n",
    "        for bigram in bigram_phraser[sent]:\n",
    "            if len(bigram.split(\" \")) > 1: # comprobamos que realmente es un bigrama\n",
    "                bigram_token.append(bigram) \n",
    "    return bigram_token\n",
    "           \n",
    "def check_bigram(x, bigrams):\n",
    "    if x.find(\"jamón serrano\") != -1 or x.find(\"jamón\") != -1:\n",
    "        print(x)\n",
    "    return [bigram for bigram in bigrams if x.find(bigram) != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:03.441272Z",
     "start_time": "2020-12-11T21:02:02.568273Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_sports = get_bigrams(train_sports[\"preprocesado\"].values, 50)\n",
    "bigrams_health = get_bigrams(train_health[\"preprocesado\"].values, 50)\n",
    "bigrams_politics = get_bigrams(train_politics[\"preprocesado\"].values, 50)\n",
    "bigrams = get_bigrams(train_data[\"preprocesado\"].values, 50)\n",
    "\n",
    "\n",
    "train_sports[\"bigrams\"] = train_sports[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_sports))\n",
    "train_health[\"bigrams\"] = train_health[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_health))\n",
    "train_politics[\"bigrams\"] = train_politics[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_politics))\n",
    "\n",
    "train_sports[\"tokens + bigrams\"] = train_sports[\"tokens\"] + train_sports[\"bigrams\"]\n",
    "train_health[\"tokens + bigrams\"] = train_health[\"tokens\"] + train_health[\"bigrams\"]\n",
    "train_politics[\"tokens + bigrams\"] = train_politics[\"tokens\"] + train_politics[\"bigrams\"]\n",
    "\n",
    "train_data[\"bigrams\"] = train_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "train_data[\"tokens + bigrams\"] = train_data[\"tokens\"] + train_data[\"bigrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:03.444274Z",
     "start_time": "2020-12-11T21:02:03.442272Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_dir = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:03.449275Z",
     "start_time": "2020-12-11T21:02:03.445271Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_tfidf_keywords(df, k):\n",
    "    tokens = df[\"tokens + bigrams\"].values\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    bow = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    bow_tfidf = tfidf[bow]\n",
    "    tfidf_dic = {dictionary.get(id): value for doc in bow_tfidf for id, value in doc}\n",
    "    tfidf_list = [k for k, v in sorted(tfidf_dic.items(), key=lambda item: item[1], reverse = True)]\n",
    "    return tfidf_list[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:03.555274Z",
     "start_time": "2020-12-11T21:02:03.450273Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health = get_k_tfidf_keywords(train_health, 100)\n",
    "keywords_tfidf_politics = get_k_tfidf_keywords(train_politics, 100)\n",
    "keywords_tfidf_sports = get_k_tfidf_keywords(train_sports, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:03.561272Z",
     "start_time": "2020-12-11T21:02:03.556273Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(d1, d2, d3):\n",
    "\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    \n",
    "    for key in deleted:\n",
    "        try:\n",
    "            d1.remove(key)\n",
    "        except:\n",
    "            print(f\"D1 no tiene {key}\")\n",
    "        try:\n",
    "            d2.remove(key)\n",
    "        except:\n",
    "            print(f\"D2 no tiene {key}\")\n",
    "        try:\n",
    "            d3.remove(key)\n",
    "        except:\n",
    "            print(f\"D3 no tiene {key}\")\n",
    "            \n",
    "    return d1, d2, d3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:03.564273Z",
     "start_time": "2020-12-11T21:02:03.562273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 no tiene defensa\n",
      "D1 no tiene récord\n"
     ]
    }
   ],
   "source": [
    "keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports = remove_duplicates(keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:03.568273Z",
     "start_time": "2020-12-11T21:02:03.565271Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_gensim_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens + bigrams\"].apply(lambda x: \" \".join(x))\n",
    "    data['joined'] = data.joined.astype(str)\n",
    "    data = \" \".join(data[\"joined\"].values)\n",
    "    return [key[0] for key in keywords(data, scores=True, words=k, pos_filter=('NNP', 'JJ', \"NNPS\", \"VB\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:04.794271Z",
     "start_time": "2020-12-11T21:02:03.569272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 no tiene presidente\n",
      "D1 no tiene real\n",
      "D1 no tiene partido\n",
      "D3 no tiene dia\n",
      "D3 no tiene trata\n",
      "D2 no tiene puntos\n",
      "D2 no tiene punto\n",
      "D3 no tiene persona\n",
      "D1 no tiene partidos\n",
      "D2 no tiene grupo\n",
      "D2 no tiene horas\n",
      "D3 no tiene personas\n",
      "D3 no tiene sin\n",
      "D3 no tiene pandemia\n",
      "D3 no tiene casos\n",
      "D1 no tiene situacion\n",
      "D1 no tiene espana\n"
     ]
    }
   ],
   "source": [
    "keywords_gensim_health = get_k_gensim_keywords(train_health, 100)\n",
    "keywords_gensim_politics = get_k_gensim_keywords(train_politics, 100)\n",
    "keywords_gensim_sports = get_k_gensim_keywords(train_sports, 100)\n",
    "\n",
    "keywords_gensim_health, keywords_k_gensim_politics, keywords_k_gensim_sports = remove_duplicates(keywords_gensim_health, keywords_gensim_politics, keywords_gensim_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:04.800273Z",
     "start_time": "2020-12-11T21:02:04.795273Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_kmeans_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    k_means_data = data[\"joined\"].values\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(k_means_data)\n",
    "    \n",
    "    model = KMeans(n_clusters=3, init='k-means++', max_iter=1000, n_init=1, random_state = 5, algorithm=\"full\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    keywords_kmeans_politics = [terms[ind] for ind in order_centroids[0, :k]]\n",
    "    keywords_kmeans_health = [terms[ind] for ind in order_centroids[1, :k]]\n",
    "    keywords_kmeans_sports = [terms[ind] for ind in order_centroids[2, :k]]\n",
    "    \n",
    "    return keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:04.934272Z",
     "start_time": "2020-12-11T21:02:04.801272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n",
      "D1 no tiene forma\n",
      "D3 no tiene presidente\n",
      "D3 no tiene real\n",
      "D3 no tiene partido\n",
      "D3 no tiene pedro sánchez\n",
      "D3 no tiene rey\n",
      "D1 no tiene madrid\n",
      "D3 no tiene fiscal\n",
      "D3 no tiene pedro\n",
      "D3 no tiene gobierno\n",
      "D3 no tiene sánchez\n",
      "D1 no tiene infarto\n",
      "D3 no tiene españa\n",
      "D1 no tiene estudio\n",
      "D1 no tiene mundo\n",
      "D3 no tiene psoe\n",
      "88 83 94\n"
     ]
    }
   ],
   "source": [
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = get_k_kmeans_keywords(train_data, 100)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))\n",
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports = remove_duplicates(keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health), len(keywords_kmeans_sports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formación del glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:04.938275Z",
     "start_time": "2020-12-11T21:02:04.935271Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_relevant_keywords(d1, d2, d3):\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    return deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:04.948271Z",
     "start_time": "2020-12-11T21:02:04.939274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics ==>  {'proyecto', 'callar', 'bildu', 'pnv', 'instituciones', 'código', 'desahucio', 'texto', 'ley', 'gonzález', 'acuerdos', 'alberto', 'generales', 'jueves', 'marín', 'eta', 'informes', 'notas', 'becerril', 'independentistas', 'armonización fiscal', 'erc', 'mal', 'presupuestos', 'penal', 'navidades', 'dsn', 'regulación', 'azurmendi', 'pablo iglesias', 'ciudadanos', 'iceta', 'felipe gonzález', 'vox', 'casa real', 'ayuntamiento', 'armonización', 'eutanasia', 'vivienda', 'injurias', 'atención', 'sociedad', 'ascen', 'presupuestos generales', 'claro', 'diputados', 'ministro', 'militares', 'comunidades', 'código penal', 'jiménez becerril', 'congreso'}\n",
      "Sports ==>  {'curry', 'franquicia', 'jugador', 'gol', 'haaland', 'thompson', 'barça', 'lakers', 'dortmund', 'michael jordan', 'hombre', 'grupos', 'jordan', 'bolt', 'temporadas', 'temporada', 'campazzo', 'realmente', 'hablando', 'atlético', 'goles', 'warriors', 'campeones', 'base', 'alfredo', 'marc', 'volver', 'equipo', 'competir', 'situaciones', 'equipos', 'ricky', 'vaccaro', 'historia', 'balonmano', 'nike', 'juego', 'duro', 'rehabilitación', 'gasol', 'mercado', 'lesión', 'pasa', 'entrenador', 'escolta', 'estrella', 'liga', 'pista', 'jornet', 'firmar', 'jugadores', 'bla'}\n",
      "Health ==>  {'ensayo', 'móviles', 'importante', 'moderna', 'facebook', 'anticuerpos', 'miocardio', 'suicidios', 'vacuna', 'plasma', 'riesgo', 'caso', 'fruta', 'covid', 'expertos', 'comida', 'pecho', 'muertes', 'teléfonos móviles', 'suicidio', 'tuberculosis', 'frente', 'centro', 'sanidad', 'sangre', 'tipo', 'tasa', 'tratamiento', 'plato', 'metabolismo', 'estudios', 'alimentos', 'contagios', 'alergia', 'sistema', 'virus', 'salud', 'enfermedad', 'infarto', 'teléfonos', 'alergias', 'fallecidos', 'coronavirus'}\n"
     ]
    }
   ],
   "source": [
    "relevant_keywords_politics = check_relevant_keywords(keywords_kmeans_politics, keywords_gensim_politics, keywords_tfidf_politics)\n",
    "relevant_keywords_health = check_relevant_keywords(keywords_kmeans_health, keywords_gensim_health, keywords_tfidf_health)\n",
    "relevant_keywords_sports = check_relevant_keywords(keywords_kmeans_sports, keywords_gensim_sports, keywords_tfidf_sports)\n",
    "\n",
    "print(\"Politics ==> \", relevant_keywords_politics)\n",
    "print(\"Sports ==> \", relevant_keywords_sports)\n",
    "print(\"Health ==> \", relevant_keywords_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:04.958272Z",
     "start_time": "2020-12-11T21:02:04.949271Z"
    }
   },
   "outputs": [],
   "source": [
    "path_keys_health = \"../keywords/keys_health.txt\"\n",
    "path_keys_sports = \"../keywords/keys_sports.txt\"\n",
    "path_keys_politics = \"../keywords/keys_politics.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:04.977273Z",
     "start_time": "2020-12-11T21:02:04.959274Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_health = [key.strip() for key in open(path_keys_health, encoding=\"utf-8\").readlines()]\n",
    "keys_sports = [key.strip() for key in open(path_keys_sports, encoding=\"utf-8\").readlines()]\n",
    "keys_politics = [key.strip() for key in open(path_keys_politics, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "keys_dic = {0: \"health\", 1: \"sports\", 2: \"politics\"}\n",
    "inverted_keys_dic = {\"health\": 0, \"sports\": 1, \"politics\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramas de test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.532272Z",
     "start_time": "2020-12-11T21:02:04.978272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>tokens + bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>health_23.txt</td>\n",
       "      <td>Hace unos días Alejandro Díez, madrileño de 24...</td>\n",
       "      <td>health</td>\n",
       "      <td>hace unos días alejandro díez madrileño de  añ...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "      <td>[se trata, este tipo, se trata, ha sido, es de...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>health_24.txt</td>\n",
       "      <td>Casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>health</td>\n",
       "      <td>casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "      <td>[sobre todo, se trata, frente al, segunda ola,...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>health_25.txt</td>\n",
       "      <td>Un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>health</td>\n",
       "      <td>un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "      <td>[frente al, frente al, cada vez, new york, más...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>health_26.txt</td>\n",
       "      <td>Los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>health</td>\n",
       "      <td>los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "      <td>[muy probable, se trata, sino también, sin emb...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>health_27.txt</td>\n",
       "      <td>El estrés de la rutina diaria, las preocupacio...</td>\n",
       "      <td>health</td>\n",
       "      <td>el estrés de la rutina diaria las preocupacion...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "      <td>[sin embargo, más allá, sin embargo, muchas pe...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>145</td>\n",
       "      <td>sports_50.txt</td>\n",
       "      <td>La cuarentena que cumplen algunas gimnastas, l...</td>\n",
       "      <td>sports</td>\n",
       "      <td>la cuarentena que cumplen algunas gimnastas la...</td>\n",
       "      <td>[cuarentena, cumplen, gimnastas, dificultades,...</td>\n",
       "      <td>[muchos casos, muchos casos, las autoridades, ...</td>\n",
       "      <td>[cuarentena, cumplen, gimnastas, dificultades,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>146</td>\n",
       "      <td>sports_6.txt</td>\n",
       "      <td>El Sevilla obtuvo en Krasnodar su billete para...</td>\n",
       "      <td>sports</td>\n",
       "      <td>el sevilla obtuvo en krasnodar su billete para...</td>\n",
       "      <td>[sevilla, obtuvo, krasnodar, billete, octavos,...</td>\n",
       "      <td>[todas las, todas las, todas las, todas las, t...</td>\n",
       "      <td>[sevilla, obtuvo, krasnodar, billete, octavos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>147</td>\n",
       "      <td>sports_7.txt</td>\n",
       "      <td>Ronald Koeman decidió dar una oportunidad a Ca...</td>\n",
       "      <td>sports</td>\n",
       "      <td>ronald koeman decidió dar una oportunidad a ca...</td>\n",
       "      <td>[ronald, koeman, decidió, oportunidad, carles,...</td>\n",
       "      <td>[apostar por, apostar por, apostar por, frente...</td>\n",
       "      <td>[ronald, koeman, decidió, oportunidad, carles,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>148</td>\n",
       "      <td>sports_8.txt</td>\n",
       "      <td>\\nChiellini, Bonucci, Barzagli, Zambrotta...la...</td>\n",
       "      <td>sports</td>\n",
       "      <td>\\nchiellini bonucci barzagli zambrottala lista...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "      <td>[frente al, sin embargo, frente al, sin embarg...</td>\n",
       "      <td>[chiellini, bonucci, barzagli, zambrottala, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>149</td>\n",
       "      <td>sports_9.txt</td>\n",
       "      <td>Darko Brasanac es la principal novedad en la c...</td>\n",
       "      <td>sports</td>\n",
       "      <td>darko brasanac es la principal novedad en la c...</td>\n",
       "      <td>[darko, brasanac, principal, novedad, convocat...</td>\n",
       "      <td>[frente al, sin embargo, frente al, sin embarg...</td>\n",
       "      <td>[darko, brasanac, principal, novedad, convocat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       doc_name                                               text  \\\n",
       "0       15  health_23.txt  Hace unos días Alejandro Díez, madrileño de 24...   \n",
       "1       16  health_24.txt  Casi todos los planes contra el coronavirus un...   \n",
       "2       17  health_25.txt  Un correcto descanso nocturno no sólo es impor...   \n",
       "3       18  health_26.txt  Los problemas de sueño son cada vez más frecue...   \n",
       "4       19  health_27.txt  El estrés de la rutina diaria, las preocupacio...   \n",
       "..     ...            ...                                                ...   \n",
       "100    145  sports_50.txt  La cuarentena que cumplen algunas gimnastas, l...   \n",
       "101    146   sports_6.txt  El Sevilla obtuvo en Krasnodar su billete para...   \n",
       "102    147   sports_7.txt  Ronald Koeman decidió dar una oportunidad a Ca...   \n",
       "103    148   sports_8.txt  \\nChiellini, Bonucci, Barzagli, Zambrotta...la...   \n",
       "104    149   sports_9.txt  Darko Brasanac es la principal novedad en la c...   \n",
       "\n",
       "      class                                       preprocesado  \\\n",
       "0    health  hace unos días alejandro díez madrileño de  añ...   \n",
       "1    health  casi todos los planes contra el coronavirus un...   \n",
       "2    health  un correcto descanso nocturno no sólo es impor...   \n",
       "3    health  los problemas de sueño son cada vez más frecue...   \n",
       "4    health  el estrés de la rutina diaria las preocupacion...   \n",
       "..      ...                                                ...   \n",
       "100  sports  la cuarentena que cumplen algunas gimnastas la...   \n",
       "101  sports  el sevilla obtuvo en krasnodar su billete para...   \n",
       "102  sports  ronald koeman decidió dar una oportunidad a ca...   \n",
       "103  sports  \\nchiellini bonucci barzagli zambrottala lista...   \n",
       "104  sports  darko brasanac es la principal novedad en la c...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [días, alejandro, díez, madrileño, años, levan...   \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...   \n",
       "2    [correcto, descanso, nocturno, importante, sen...   \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...   \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...   \n",
       "..                                                 ...   \n",
       "100  [cuarentena, cumplen, gimnastas, dificultades,...   \n",
       "101  [sevilla, obtuvo, krasnodar, billete, octavos,...   \n",
       "102  [ronald, koeman, decidió, oportunidad, carles,...   \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...   \n",
       "104  [darko, brasanac, principal, novedad, convocat...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    [se trata, este tipo, se trata, ha sido, es de...   \n",
       "1    [sobre todo, se trata, frente al, segunda ola,...   \n",
       "2    [frente al, frente al, cada vez, new york, más...   \n",
       "3    [muy probable, se trata, sino también, sin emb...   \n",
       "4    [sin embargo, más allá, sin embargo, muchas pe...   \n",
       "..                                                 ...   \n",
       "100  [muchos casos, muchos casos, las autoridades, ...   \n",
       "101  [todas las, todas las, todas las, todas las, t...   \n",
       "102  [apostar por, apostar por, apostar por, frente...   \n",
       "103  [frente al, sin embargo, frente al, sin embarg...   \n",
       "104  [frente al, sin embargo, frente al, sin embarg...   \n",
       "\n",
       "                                      tokens + bigrams  \n",
       "0    [días, alejandro, díez, madrileño, años, levan...  \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...  \n",
       "2    [correcto, descanso, nocturno, importante, sen...  \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...  \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...  \n",
       "..                                                 ...  \n",
       "100  [cuarentena, cumplen, gimnastas, dificultades,...  \n",
       "101  [sevilla, obtuvo, krasnodar, billete, octavos,...  \n",
       "102  [ronald, koeman, decidió, oportunidad, carles,...  \n",
       "103  [chiellini, bonucci, barzagli, zambrottala, li...  \n",
       "104  [darko, brasanac, principal, novedad, convocat...  \n",
       "\n",
       "[105 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"bigrams\"] = test_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "test_data[\"tokens + bigrams\"] = test_data[\"tokens\"] + test_data[\"bigrams\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.537273Z",
     "start_time": "2020-12-11T21:02:05.535271Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.649331Z",
     "start_time": "2020-12-11T21:02:05.538272Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(glossary for glossary in glossaries)\n",
    "dictionary.save('keys.dict')  # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.653331Z",
     "start_time": "2020-12-11T21:02:05.650333Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, docs, dictionary):\n",
    "        self.docs = docs\n",
    "        self.dict = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dict.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.657332Z",
     "start_time": "2020-12-11T21:02:05.654333Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = MyCorpus(glossaries, dictionary)\n",
    "corpora.MmCorpus.serialize(\"keys.mm\", bow, metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.669331Z",
     "start_time": "2020-12-11T21:02:05.658333Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "index = Similarity(index_temp, bow, num_features=len(dictionary))  # create index\n",
    "index.save(\"keys.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.673332Z",
     "start_time": "2020-12-11T21:02:05.670331Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tfidf = models.TfidfModel(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.680331Z",
     "start_time": "2020-12-11T21:02:05.674332Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_document_tfidf(model, dictionary, bow, index, documents, i, verbose = False):\n",
    "    \"\"\"\n",
    "    Given a specific document, computes the ranking of the classes and returns the current class, \n",
    "    the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    document = documents.iloc[i]\n",
    "    pq = document[\"tokens + bigrams\"]\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = model[vq]\n",
    "    sim = index[qtfidf]\n",
    "\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "            \n",
    "    \n",
    "    return [i, get_info_document(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.684332Z",
     "start_time": "2020-12-11T21:02:05.681332Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.sum(sim) == 0.0:\n",
    "        predicted_class = np.random.randint(3)\n",
    "        probabilities = np.array([1/3, 1/3, 1/3])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        tfidf_scores = np.array(sim)\n",
    "        probabilities = tfidf_scores / np.sum(tfidf_scores)\n",
    "    \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.690332Z",
     "start_time": "2020-12-11T21:02:05.685331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> Los problemas de sueño son cada vez más frecuentes en nuestra sociedad, y llegan a afectar a uno de \n",
      "[health] ==> 0.257\n",
      "[politics] ==> 0.093\n",
      "[sports] ==> 0.000\n"
     ]
    }
   ],
   "source": [
    "_ = classify_document_tfidf(model_tfidf, dictionary, bow, index, test_data, 3, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.694332Z",
     "start_time": "2020-12-11T21:02:05.691331Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_tfidf(function, model, dictionary, bow, index, data):\n",
    "    def classify(doc_i):\n",
    "        return function(model, dictionary, bow, index, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.701330Z",
     "start_time": "2020-12-11T21:02:05.695332Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_test_data(test_data, infos):\n",
    "    \"\"\"\n",
    "    Auxiliary function to fill the dataframe with info about the classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    current_class = pd.Series([info[1][\"current_class\"] for info in infos])\n",
    "    predicted_class = pd.Series([info[1][\"predicted_class\"] for info in infos])\n",
    "    p_health = pd.Series([info[1][\"probabilities\"][0] for info in infos])\n",
    "    p_sports = pd.Series([info[1][\"probabilities\"][1] for info in infos])\n",
    "    p_politics = pd.Series([info[1][\"probabilities\"][2] for info in infos])\n",
    "\n",
    "    data[\"current_class\"] = current_class\n",
    "    data[\"predicted_class\"] = predicted_class\n",
    "    data[\"p_health\"] = p_health\n",
    "    data[\"p_sports\"] = p_sports\n",
    "    data[\"p_politics\"] = p_politics\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.706332Z",
     "start_time": "2020-12-11T21:02:05.702332Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_documents(test_data, classify):\n",
    "    \"\"\"\n",
    "    Classifies the documents given a specific classification function.\n",
    "    \n",
    "    \"\"\"\n",
    "    test_data = test_data.copy()\n",
    "    \n",
    "    infos = [classify(i) for i in range(len(test_data))]\n",
    "    data = fill_test_data(test_data, infos)\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.711331Z",
     "start_time": "2020-12-11T21:02:05.707331Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename(df):\n",
    "    \"\"\"\n",
    "    Computes the filename for each document based on the performed classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    confidence = \"%.3f\" % df[\"confidence\"]\n",
    "    current_class = df[\"class\"]\n",
    "    predicted_class = df[\"predicted_class_name\"]\n",
    "    correct = current_class == predicted_class\n",
    "    name = df[\"doc_name\"].split(\".\")[0]\n",
    "    \n",
    "    return f\"../classification/{predicted_class}/{confidence}_{name}-{correct}-{current_class}-{predicted_class}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.715331Z",
     "start_time": "2020-12-11T21:02:05.712332Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(path, content):\n",
    "    \"\"\"\n",
    "    Writes a file given its path and content.\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.723332Z",
     "start_time": "2020-12-11T21:02:05.716332Z"
    }
   },
   "outputs": [],
   "source": [
    "def move_files(data, tables = False):\n",
    "    \"\"\"\n",
    "    Moves the files to their corresponding new directory after classification is done.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    classes = [[0, \"p_health\"], [1, \"p_sports\"], [2, \"p_politics\"]]\n",
    "\n",
    "    for cl in classes:\n",
    "        docs = data[data[\"current_class\"] == cl[0]]\n",
    "        docs = docs.sort_values(by=[cl[1]], ascending=False)\n",
    "        docs[\"predicted_class_name\"] = docs[\"predicted_class\"].apply(lambda x : keys_dic[x])\n",
    "        docs[\"confidence\"] = docs[[\"p_health\", \"p_sports\", \"p_politics\"]].max(axis=1)\n",
    "        docs[\"file\"] = docs.apply(lambda x: get_filename(x), axis=1)\n",
    "        docs.apply(lambda row: write_file(row[\"file\"], row[\"text\"]), axis = 1)\n",
    "        if tables:\n",
    "            # tabla para la memoria\n",
    "            docs = docs[[\"doc_name\", \"class\", \"p_health\", \"p_sports\", \"p_politics\", \"predicted_class_name\"]]\n",
    "            docs = docs.rename(columns = {\"predicted_class_name\": \"predicted_class\"})\n",
    "            print(docs.to_latex(bold_rows = True, float_format=\"%.2f\", column_format = \"llllll\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:05.727332Z",
     "start_time": "2020-12-11T21:02:05.724332Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute(test_data, classification_function, move = True, tables = False):\n",
    "    \"\"\"\n",
    "    General function that classifies the documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    print(\"############################################################\")\n",
    "    print(\"Starting document´s classification...\")\n",
    "    data = classify_documents(data, classification_function)\n",
    "    sleep(1)\n",
    "    print(\"Document´s classification done...\")\n",
    "    if move:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        sleep(1)\n",
    "        print(\"Moving files to the correct directories...\")\n",
    "        move_files(data, tables)\n",
    "        sleep(1)\n",
    "        print(\"Files moved.\")\n",
    "    print(\"############################################################\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:02:08.812050Z",
     "start_time": "2020-12-11T21:02:05.728331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Starting document´s classification...\n",
      "Document´s classification done...\n",
      "-----------------------------------------------------------\n",
      "Moving files to the correct directories...\n",
      "Files moved.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "data = execute(test_data, classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:10:28.441294Z",
     "start_time": "2020-12-11T21:10:28.436297Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_model(data, model, classify_function):\n",
    "    \"\"\"\n",
    "    Function that evaluates the performance of a specific model.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    print(\"#######################################################\")\n",
    "    print(\"Evaluating \"+ model + \"...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    data = classify_documents(data, classify_function)\n",
    "    \n",
    "    y_true = data[\"current_class\"]\n",
    "    y_pred = data[\"predicted_class\"]\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"health\", \"sports\", \"politics\"]))\n",
    "    print(\"Confusion matrix ==>\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    #precisions = []\n",
    "    #recalls = []\n",
    "\n",
    "    #for i in range(len(cm[0])):\n",
    "     #   name = keys_dic[i]\n",
    "      #  print(f\"Computing statistics about {name}:\")\n",
    "       # recall = cm[i,i] / np.sum(cm[i,:])\n",
    "      #  precision = cm[i,i] / np.sum(cm[:,i])\n",
    "      #  print(f\"\\tPrecision ==> {precision}\")\n",
    "      #  print(f\"\\tRecall ==> {recall}\")\n",
    "\n",
    "      #  precisions.append(precision)\n",
    "      #  recalls.append(recall)\n",
    "\n",
    "    #precisions = np.array(precisions)\n",
    "    #recalls = np.array(recalls)\n",
    "    #f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "    #accuracy = accuracy_score(y_true, y_pred)\n",
    "    #print(f\"Average precision ==> {precisions.mean()}\")\n",
    "    #print(f\"Average recall ==> {recalls.mean()}\")\n",
    "    #print(f\"F1-Score ==> {f1}\")\n",
    "    #print(f\"Overall accuracy score ==> {accuracy}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Model evaluated\")\n",
    "    print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-11T21:10:29.465294Z",
     "start_time": "2020-12-11T21:10:29.423294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating tf-idf...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.80      0.91      0.85        35\n",
      "      sports       0.91      0.83      0.87        35\n",
      "    politics       0.91      0.86      0.88        35\n",
      "\n",
      "    accuracy                           0.87       105\n",
      "   macro avg       0.87      0.87      0.87       105\n",
      "weighted avg       0.87      0.87      0.87       105\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[32  2  1]\n",
      " [ 4 29  2]\n",
      " [ 4  1 30]]\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_model(test_data, \"tf-idf\", classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
