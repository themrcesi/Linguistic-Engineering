{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Índice<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-documentos\" data-toc-modified-id=\"Carga-de-documentos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Carga de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocesado\" data-toc-modified-id=\"Preprocesado-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preprocesado</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bigramas\" data-toc-modified-id=\"Bigramas-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Bigramas</a></span></li></ul></li></ul></li><li><span><a href=\"#Glosario\" data-toc-modified-id=\"Glosario-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-de-keywords\" data-toc-modified-id=\"Extracción-de-keywords-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extracción de keywords</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-propia\" data-toc-modified-id=\"Extracción-propia-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Extracción propia</a></span></li><li><span><a href=\"#Gensim\" data-toc-modified-id=\"Gensim-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Gensim</a></span></li><li><span><a href=\"#Kmeans\" data-toc-modified-id=\"Kmeans-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Kmeans</a></span></li></ul></li><li><span><a href=\"#Formación-del-glosario\" data-toc-modified-id=\"Formación-del-glosario-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Formación del glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automatizado\" data-toc-modified-id=\"Automatizado-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Automatizado</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificador\" data-toc-modified-id=\"Clasificador-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clasificador</a></span><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-glosarios\" data-toc-modified-id=\"Carga-de-glosarios-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Carga de glosarios</a></span></li><li><span><a href=\"#Bigramas-de-test-data\" data-toc-modified-id=\"Bigramas-de-test-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bigramas de test data</a></span></li><li><span><a href=\"#Modelos\" data-toc-modified-id=\"Modelos-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Naive Bayes</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificación-de-documentos\" data-toc-modified-id=\"Clasificación-de-documentos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clasificación de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li><li><span><a href=\"#Clasificación\" data-toc-modified-id=\"Clasificación-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Clasificación</a></span></li></ul></li><li><span><a href=\"#Evaluación-de-modelos\" data-toc-modified-id=\"Evaluación-de-modelos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluación de modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:10.195130Z",
     "start_time": "2020-12-15T21:28:09.088130Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.summarization import keywords\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# Operatos\n",
    "from operator import itemgetter\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy_spanish_lemmatizer import SpacyCustomLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# utils\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:10.198129Z",
     "start_time": "2020-12-15T21:28:10.196130Z"
    }
   },
   "outputs": [],
   "source": [
    "path_health = \"../documents/health\"\n",
    "path_politics = \"../documents/politics\"\n",
    "path_sports = \"../documents/sports\"\n",
    "path_technology = \"../documents/technology\"\n",
    "path_documents = \"../documents\"\n",
    "path_stopwords = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:10.204129Z",
     "start_time": "2020-12-15T21:28:10.199129Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    return path.split(\"\\\\\")[-1], open(path,encoding='utf-8').read(), path.split(\"\\\\\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:10.673129Z",
     "start_time": "2020-12-15T21:28:10.205130Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = Parallel(n_jobs = -1)(delayed(load_document)(path) for path in glob.glob(path_documents+\"/*/*.txt\"))\n",
    "documents = pd.DataFrame(documents, columns=[\"doc_name\", \"text\", \"class\"])\n",
    "documents['text'] = documents['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:10.686129Z",
     "start_time": "2020-12-15T21:28:10.674130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health\n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health\n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health\n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health\n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:10.695129Z",
     "start_time": "2020-12-15T21:28:10.687129Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\&)|(\\%)|(\\$)|(\\€)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "    \n",
    "#nlp = spacy.load(\"es\")\n",
    "lemmatizer = SpacyCustomLemmatizer()\n",
    "\n",
    "def load_stopwords(path):\n",
    "    return [line.strip() for line in open(path_stopwords, encoding = \"utf-8\").readlines()]\n",
    "\n",
    "STOP_WORDS = set(load_stopwords(path_stopwords))\n",
    "\n",
    "def delete_stop_words(doc):\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return clean\n",
    "\n",
    "def preprocess_document(document):\n",
    "    document = REPLACE_NO_SPACE.sub(NO_SPACE, document.lower())\n",
    "    document = REPLACE_WITH_SPACE.sub(SPACE, document)\n",
    "    # tokens = wordpunct_tokenize(document)\n",
    "    # tokens = delete_proper_nouns(tokens)\n",
    "    return document\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tokens = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "# TODO: REVISAR ESTO\n",
    "\n",
    "def delete_proper_nouns(tokens):\n",
    "    # Tag the tokens with their type - ie are they nouns or not\n",
    "    lTokens = pos_tag(tokens)\n",
    "    # find all the proper nouns and print them out\n",
    "    lTagDict = findtags('NNP', lTokens)\n",
    "    return [token.lower() for token in tokens if token not in lTagDict]\n",
    "    \n",
    "def findtags(tag_prefix, tagged_text):\n",
    "    \"\"\"\n",
    "    Find tokens matching the specified tag_prefix\n",
    "    \"\"\"\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    print(cfd.conditions())\n",
    "    return [list(cfd[tag].keys()) for tag in cfd.conditions()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:11.199130Z",
     "start_time": "2020-12-15T21:28:10.696129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "      <td>aceptémoslo de una vez perder peso de manera r...</td>\n",
       "      <td>[aceptémoslo, perder, peso, rápida, indolora, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "      <td>sin tiempo para hacer recuento de daños irrump...</td>\n",
       "      <td>[recuento, daños, irrumpe, ola, virus, golpear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "      <td>mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>[gente, mostrar, redes, sociales, versión, fot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "      <td>una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>[faceta, clave, frenética, lucha, global, pfiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "      <td>la curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>[curva, contagios, coronavirus, mantiene, espa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class  \\\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health   \n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health   \n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health   \n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health   \n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health   \n",
       "\n",
       "                                        preprocesado  \\\n",
       "0  aceptémoslo de una vez perder peso de manera r...   \n",
       "1  sin tiempo para hacer recuento de daños irrump...   \n",
       "2  mucha gente intenta mostrar en las redes socia...   \n",
       "3  una faceta clave en la frenética lucha global ...   \n",
       "4  la curva de contagios de coronavirus se mantie...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [aceptémoslo, perder, peso, rápida, indolora, ...  \n",
       "1  [recuento, daños, irrumpe, ola, virus, golpear...  \n",
       "2  [gente, mostrar, redes, sociales, versión, fot...  \n",
       "3  [faceta, clave, frenética, lucha, global, pfiz...  \n",
       "4  [curva, contagios, coronavirus, mantiene, espa...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[\"preprocesado\"] = documents[\"text\"].apply(lambda x: preprocess_document(x))\n",
    "documents[\"tokens\"] = documents[\"preprocesado\"].apply(lambda x: delete_stop_words(x))\n",
    "# documents[\"lematizado\"] = documents[\"preprocesado\"].apply(lambda x: lemmatize(x))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:11.208129Z",
     "start_time": "2020-12-15T21:28:11.200129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ==> 60 documents\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_health = documents[documents[\"class\"] == \"health\"].iloc[:15]\n",
    "train_politics = documents[documents[\"class\"] == \"politics\"].iloc[:15]\n",
    "train_sports = documents[documents[\"class\"] == \"sports\"].iloc[:15]\n",
    "train_technology = documents[documents[\"class\"] == \"technology\"].iloc[:15]\n",
    "\n",
    "train_data = pd.concat([train_health, train_politics, train_sports, train_technology])\n",
    "print(f\"Training data ==> {len(train_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:11.215129Z",
     "start_time": "2020-12-15T21:28:11.209129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data ==> 140 documents\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_health = documents[documents[\"class\"] == \"health\"].iloc[15:]\n",
    "test_politics = documents[documents[\"class\"] == \"politics\"].iloc[15:]\n",
    "test_sports = documents[documents[\"class\"] == \"sports\"].iloc[15:]\n",
    "test_technology = documents[documents[\"class\"] == \"technology\"].iloc[15:]\n",
    "\n",
    "test_data = pd.concat([test_health, test_politics, test_sports, test_technology])\n",
    "test_data.reset_index(inplace = True)\n",
    "print(f\"Testing data ==> {len(test_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:11.221129Z",
     "start_time": "2020-12-15T21:28:11.216129Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(documents, threshold):\n",
    "    token_ = [doc.split(\" \") for doc in documents]\n",
    "    bigram = Phrases(token_, min_count=1, threshold=threshold, delimiter=b' ')\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    bigram_token = []\n",
    "    for sent in token_:\n",
    "        for bigram in bigram_phraser[sent]:\n",
    "            if len(bigram.split(\" \")) > 1: # comprobamos que realmente es un bigrama\n",
    "                bigram_token.append(bigram) \n",
    "    return bigram_token\n",
    "           \n",
    "def check_bigram(x, bigrams):\n",
    "    if x.find(\"jamón serrano\") != -1 or x.find(\"jamón\") != -1:\n",
    "        print(x)\n",
    "    return [bigram for bigram in bigrams if x.find(bigram) != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:12.729324Z",
     "start_time": "2020-12-15T21:28:11.222129Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_sports = get_bigrams(train_sports[\"preprocesado\"].values, 50)\n",
    "bigrams_health = get_bigrams(train_health[\"preprocesado\"].values, 50)\n",
    "bigrams_politics = get_bigrams(train_politics[\"preprocesado\"].values, 50)\n",
    "bigrams_technology = get_bigrams(train_technology[\"preprocesado\"].values, 50)\n",
    "bigrams = get_bigrams(train_data[\"preprocesado\"].values, 50)\n",
    "\n",
    "\n",
    "train_sports[\"bigrams\"] = train_sports[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_sports))\n",
    "train_health[\"bigrams\"] = train_health[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_health))\n",
    "train_politics[\"bigrams\"] = train_politics[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_politics))\n",
    "train_technology[\"bigrams\"] = train_technology[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_technology))\n",
    "\n",
    "train_sports[\"tokens + bigrams\"] = train_sports[\"tokens\"] + train_sports[\"bigrams\"]\n",
    "train_health[\"tokens + bigrams\"] = train_health[\"tokens\"] + train_health[\"bigrams\"]\n",
    "train_politics[\"tokens + bigrams\"] = train_politics[\"tokens\"] + train_politics[\"bigrams\"]\n",
    "train_technology[\"tokens + bigrams\"] = train_technology[\"tokens\"] + train_technology[\"bigrams\"]\n",
    "\n",
    "train_data[\"bigrams\"] = train_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "train_data[\"tokens + bigrams\"] = train_data[\"tokens\"] + train_data[\"bigrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:12.733324Z",
     "start_time": "2020-12-15T21:28:12.730326Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_dir = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:12.741324Z",
     "start_time": "2020-12-15T21:28:12.734326Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_tfidf_keywords(df, k):\n",
    "    tokens = df[\"tokens + bigrams\"].values\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    bow = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    bow_tfidf = tfidf[bow]\n",
    "    tfidf_dic = {dictionary.get(id): value for doc in bow_tfidf for id, value in doc}\n",
    "    tfidf_list = [k for k, v in sorted(tfidf_dic.items(), key=lambda item: item[1], reverse = True)]\n",
    "    return tfidf_list[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:12.925324Z",
     "start_time": "2020-12-15T21:28:12.742324Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health = get_k_tfidf_keywords(train_health, 250)\n",
    "keywords_tfidf_politics = get_k_tfidf_keywords(train_politics, 250)\n",
    "keywords_tfidf_sports = get_k_tfidf_keywords(train_sports, 250)\n",
    "keywords_tfidf_technology = get_k_tfidf_keywords(train_technology, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:12.930324Z",
     "start_time": "2020-12-15T21:28:12.926324Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(d1, d2, d3, d4):\n",
    "\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d1) & set(d4)\n",
    "    i4 = set(d2) & set(d3)\n",
    "    i5 = set(d3) & set(d4)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2,i3,i4,i5)))\n",
    "    for key in deleted:\n",
    "        try:\n",
    "            d1.remove(key)\n",
    "        except:\n",
    "            print(f\"D1 no tiene {key}\")\n",
    "        try:\n",
    "            d2.remove(key)\n",
    "        except:\n",
    "            print(f\"D2 no tiene {key}\")\n",
    "        try:\n",
    "            d3.remove(key)\n",
    "        except:\n",
    "            print(f\"D3 no tiene {key}\")\n",
    "        try:\n",
    "            d4.remove(key)\n",
    "        except:\n",
    "            print(f\"D4 no tiene {key}\")\n",
    "            \n",
    "    return d1, d2, d3, d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:12.936325Z",
     "start_time": "2020-12-15T21:28:12.931324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D3 no tiene sin embargo\n",
      "D1 no tiene sobre todo\n",
      "D4 no tiene sobre todo\n",
      "D2 no tiene pantalla\n",
      "D3 no tiene pantalla\n",
      "D1 no tiene defensa\n",
      "D4 no tiene defensa\n",
      "D3 no tiene sanidad\n",
      "D4 no tiene sanidad\n",
      "D2 no tiene facebook\n",
      "D3 no tiene facebook\n",
      "D2 no tiene por ejemplo\n",
      "D3 no tiene por ejemplo\n",
      "D1 no tiene récord\n",
      "D1 no tiene gente\n",
      "D3 no tiene andalucía\n",
      "D4 no tiene andalucía\n",
      "D2 no tiene mar\n",
      "D3 no tiene mar\n",
      "D2 no tiene actividad\n",
      "D3 no tiene actividad\n",
      "D2 no tiene copa\n",
      "D4 no tiene copa\n",
      "D3 no tiene padres\n",
      "D4 no tiene padres\n",
      "D1 no tiene ha sido\n",
      "D2 no tiene ha sido\n"
     ]
    }
   ],
   "source": [
    "keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports, keywords_tfidf_technology = remove_duplicates(\n",
    "    keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports, keywords_tfidf_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:12.943324Z",
     "start_time": "2020-12-15T21:28:12.938324Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_gensim_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens + bigrams\"].apply(lambda x: \" \".join(x))\n",
    "    data['joined'] = data.joined.astype(str)\n",
    "    data = \" \".join(data[\"joined\"].values)\n",
    "    return [key[0] for key in keywords(data, scores=True, words=k, pos_filter=('NNP', 'JJ', \"NNPS\", \"VB\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:14.827395Z",
     "start_time": "2020-12-15T21:28:12.944325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 no tiene base\n",
      "D2 no tiene base\n",
      "D2 no tiene productos\n",
      "D3 no tiene productos\n",
      "D4 no tiene han\n",
      "D2 no tiene puntos\n",
      "D3 no tiene gente\n",
      "D1 no tiene partido\n",
      "D4 no tiene partido\n",
      "D1 no tiene baja\n",
      "D2 no tiene baja\n",
      "D2 no tiene joven\n",
      "D4 no tiene joven\n",
      "D2 no tiene niveles\n",
      "D3 no tiene niveles\n",
      "D2 no tiene parte\n",
      "D4 no tiene parte\n",
      "D2 no tiene moviles\n",
      "D3 no tiene moviles\n",
      "D1 no tiene jornada\n",
      "D2 no tiene jornada\n",
      "D3 no tiene casos\n",
      "D3 no tiene dia\n",
      "D1 no tiene marcas\n",
      "D2 no tiene marcas\n",
      "D3 no tiene pandemia\n",
      "D1 no tiene acciones\n",
      "D4 no tiene acciones\n",
      "D2 no tiene unidos\n",
      "D3 no tiene unidos\n",
      "D1 no tiene mercado\n",
      "D2 no tiene mercado\n",
      "D1 no tiene kilometros\n",
      "D2 no tiene kilometros\n",
      "D1 no tiene resultados\n",
      "D2 no tiene resultados\n",
      "D2 no tiene paises\n",
      "D3 no tiene paises\n",
      "D2 no tiene unido\n",
      "D3 no tiene unido\n",
      "D2 no tiene manos\n",
      "D4 no tiene manos\n",
      "D4 no tiene dificil\n",
      "D2 no tiene todos\n",
      "D3 no tiene todos\n",
      "D2 no tiene paso\n",
      "D3 no tiene paso\n",
      "D1 no tiene acuerdo\n",
      "D2 no tiene del\n",
      "D4 no tiene del\n",
      "D2 no tiene prueba\n",
      "D4 no tiene prueba\n",
      "D2 no tiene jovenes\n",
      "D4 no tiene jovenes\n",
      "D3 no tiene las\n",
      "D4 no tiene las\n",
      "D2 no tiene opciones\n",
      "D2 no tiene ejemplos\n",
      "D3 no tiene ejemplos\n",
      "D2 no tiene movil\n",
      "D3 no tiene movil\n",
      "D1 no tiene espana\n",
      "D3 no tiene mujeres\n",
      "D4 no tiene mujeres\n",
      "D2 no tiene tipos\n",
      "D3 no tiene tipos\n",
      "D2 no tiene opcion\n",
      "D4 no tiene opcion\n",
      "D2 no tiene problemas\n",
      "D3 no tiene problemas\n",
      "D1 no tiene nacional\n",
      "D4 no tiene nacional\n",
      "D3 no tiene mujer\n",
      "D4 no tiene mujer\n",
      "D2 no tiene horas\n",
      "D1 no tiene final\n",
      "D2 no tiene mejores\n",
      "D4 no tiene mejores\n",
      "D4 no tiene numero\n",
      "D1 no tiene cambio\n",
      "D2 no tiene cambio\n",
      "D2 no tiene facebook\n",
      "D3 no tiene facebook\n",
      "D1 no tiene situaciones\n",
      "D2 no tiene situaciones\n",
      "D2 no tiene nuestro\n",
      "D3 no tiene nuestro\n",
      "D2 no tiene seguro\n",
      "D4 no tiene seguro\n",
      "D2 no tiene forma\n",
      "D4 no tiene forma\n",
      "D2 no tiene puede\n",
      "D3 no tiene puede\n",
      "D2 no tiene noviembre\n",
      "D4 no tiene noviembre\n",
      "D2 no tiene pasos\n",
      "D3 no tiene pasos\n",
      "D2 no tiene pais\n",
      "D3 no tiene pais\n",
      "D3 no tiene falta\n",
      "D4 no tiene falta\n",
      "D2 no tiene mar\n",
      "D3 no tiene mar\n",
      "D3 no tiene centro\n",
      "D4 no tiene centro\n",
      "D3 no tiene medidas\n",
      "D3 no tiene padres\n",
      "D4 no tiene padres\n",
      "D2 no tiene sin embargo\n",
      "D3 no tiene sin embargo\n",
      "D1 no tiene principio\n",
      "D4 no tiene principio\n",
      "D4 no tiene grupo\n",
      "D3 no tiene control\n",
      "D4 no tiene control\n",
      "D2 no tiene cifra\n",
      "D3 no tiene cifra\n",
      "D1 no tiene relacion\n",
      "D2 no tiene relacion\n",
      "D1 no tiene interesante\n",
      "D2 no tiene interesante\n",
      "D2 no tiene resultado\n",
      "D3 no tiene resultado\n",
      "D2 no tiene formas\n",
      "D3 no tiene formas\n",
      "D2 no tiene especialmente\n",
      "D3 no tiene dato\n",
      "D3 no tiene sociales\n",
      "D4 no tiene sociales\n",
      "D2 no tiene caso\n",
      "D3 no tiene caso\n",
      "D2 no tiene estados\n",
      "D3 no tiene estados\n",
      "D3 no tiene muy\n",
      "D2 no tiene pruebas\n",
      "D4 no tiene pruebas\n",
      "D1 no tiene segunda\n",
      "D4 no tiene segunda\n",
      "D1 no tiene equipos\n",
      "D2 no tiene equipos\n",
      "D1 no tiene principios\n",
      "D4 no tiene principios\n",
      "D2 no tiene efectos\n",
      "D3 no tiene efectos\n",
      "D1 no tiene cabeza\n",
      "D4 no tiene cabeza\n",
      "D3 no tiene persona\n",
      "D3 no tiene este\n",
      "D2 no tiene agua\n",
      "D3 no tiene agua\n",
      "D3 no tiene padre\n",
      "D4 no tiene padre\n",
      "D1 no tiene marca\n",
      "D2 no tiene marca\n",
      "D1 no tiene equipo\n",
      "D2 no tiene equipo\n",
      "D4 no tiene ano\n",
      "D3 no tiene semanas\n",
      "D1 no tiene publico\n",
      "D1 no tiene cambios\n",
      "D4 no tiene ideas\n",
      "D2 no tiene ejemplo\n",
      "D3 no tiene ejemplo\n",
      "D1 no tiene pensar\n",
      "D2 no tiene pensar\n",
      "D2 no tiene experto\n",
      "D3 no tiene experto\n",
      "D1 no tiene plantilla\n",
      "D2 no tiene plantilla\n",
      "D2 no tiene hasta\n",
      "D3 no tiene hasta\n",
      "D2 no tiene nuestros\n",
      "D3 no tiene nuestros\n",
      "D1 no tiene por\n",
      "D2 no tiene por\n",
      "D3 no tiene trata\n",
      "D3 no tiene tipo\n",
      "D2 no tiene punto\n",
      "D2 no tiene general\n",
      "D4 no tiene general\n",
      "D1 no tiene situacion\n",
      "D3 no tiene muerte\n",
      "D4 no tiene muerte\n",
      "D1 no tiene dejar\n",
      "D4 no tiene dejar\n",
      "D3 no tiene personas\n",
      "D3 no tiene medio\n",
      "D2 no tiene efecto\n",
      "D3 no tiene efecto\n",
      "D2 no tiene estudio\n",
      "D3 no tiene estudio\n",
      "D1 no tiene metros\n",
      "D2 no tiene metros\n",
      "D3 no tiene mas\n"
     ]
    }
   ],
   "source": [
    "keywords_gensim_health = get_k_gensim_keywords(train_health, 250)\n",
    "keywords_gensim_politics = get_k_gensim_keywords(train_politics, 250)\n",
    "keywords_gensim_sports = get_k_gensim_keywords(train_sports, 250)\n",
    "keywords_gensim_technology = get_k_gensim_keywords(train_technology, 250)\n",
    "\n",
    "keywords_gensim_health, keywords_k_gensim_politics, keywords_k_gensim_sports, keywords_k_gensim_technology = remove_duplicates(\n",
    "    keywords_gensim_health, keywords_gensim_politics, keywords_gensim_sports, keywords_gensim_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:14.836396Z",
     "start_time": "2020-12-15T21:28:14.831395Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_kmeans_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    k_means_data = data[\"joined\"].values\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(k_means_data)\n",
    "    \n",
    "    model = KMeans(n_clusters=4, init='k-means++', max_iter=1000, n_init=1, random_state = 5, algorithm=\"full\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    keywords_kmeans_politics = [terms[ind] for ind in order_centroids[0, :k]]\n",
    "    keywords_kmeans_technology = [terms[ind] for ind in order_centroids[1, :k]]\n",
    "    keywords_kmeans_sports = [terms[ind] for ind in order_centroids[2, :k]]\n",
    "    keywords_kmeans_health = [terms[ind] for ind in order_centroids[3, :k]]\n",
    "    \n",
    "    return keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:15.027397Z",
     "start_time": "2020-12-15T21:28:14.837396Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200 200 200\n",
      "D3 no tiene social\n",
      "D4 no tiene social\n",
      "D2 no tiene millones\n",
      "D4 no tiene millones\n",
      "D1 no tiene volver\n",
      "D2 no tiene volver\n",
      "D3 no tiene situación\n",
      "D4 no tiene situación\n",
      "D3 no tiene vida\n",
      "D4 no tiene vida\n",
      "D3 no tiene falta\n",
      "D4 no tiene falta\n",
      "D1 no tiene base\n",
      "D2 no tiene base\n",
      "D3 no tiene covid\n",
      "D4 no tiene covid\n",
      "D3 no tiene virus\n",
      "D4 no tiene virus\n",
      "D2 no tiene riesgo\n",
      "D3 no tiene riesgo\n",
      "D3 no tiene grupo\n",
      "D4 no tiene grupo\n",
      "D2 no tiene comida\n",
      "D3 no tiene comida\n",
      "D3 no tiene importante\n",
      "D4 no tiene importante\n",
      "D3 no tiene día\n",
      "D4 no tiene día\n",
      "D2 no tiene hielo\n",
      "D3 no tiene hielo\n",
      "D3 no tiene récord\n",
      "D4 no tiene récord\n",
      "D1 no tiene partido\n",
      "D4 no tiene partido\n",
      "D3 no tiene coronavirus\n",
      "D4 no tiene coronavirus\n",
      "D3 no tiene media\n",
      "D4 no tiene media\n",
      "D3 no tiene casos\n",
      "D4 no tiene casos\n",
      "D2 no tiene compañía\n",
      "D3 no tiene compañía\n",
      "D3 no tiene caso\n",
      "D4 no tiene caso\n",
      "D2 no tiene datos\n",
      "D3 no tiene datos\n",
      "D3 no tiene pandemia\n",
      "D4 no tiene pandemia\n",
      "D2 no tiene año\n",
      "D4 no tiene año\n",
      "D3 no tiene horas\n",
      "D4 no tiene horas\n",
      "D2 no tiene enfermedad\n",
      "D3 no tiene enfermedad\n",
      "D3 no tiene días\n",
      "D4 no tiene días\n",
      "D2 no tiene unidos\n",
      "D3 no tiene unidos\n",
      "D3 no tiene país\n",
      "D4 no tiene país\n",
      "D3 no tiene semana\n",
      "D4 no tiene semana\n",
      "D2 no tiene mercado\n",
      "D4 no tiene mercado\n",
      "D4 no tiene años\n",
      "D3 no tiene personas\n",
      "D4 no tiene personas\n",
      "D2 no tiene agua\n",
      "D3 no tiene agua\n",
      "D1 no tiene pablo\n",
      "D4 no tiene pablo\n",
      "D1 no tiene marca\n",
      "D4 no tiene marca\n",
      "D3 no tiene claro\n",
      "D4 no tiene claro\n",
      "D2 no tiene estudio\n",
      "D3 no tiene estudio\n",
      "D3 no tiene seguridad\n",
      "D4 no tiene seguridad\n",
      "D1 no tiene real\n",
      "D4 no tiene real\n",
      "D1 no tiene madrid\n",
      "D4 no tiene madrid\n",
      "D2 no tiene equipo\n",
      "D4 no tiene equipo\n",
      "164 172 188 189\n"
     ]
    }
   ],
   "source": [
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology = get_k_kmeans_keywords(\n",
    "    train_data, 200)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health),\n",
    "      len(keywords_kmeans_sports), len(keywords_kmeans_technology))\n",
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywÇords_kmeans_technology = remove_duplicates(\n",
    "    keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health),\n",
    "      len(keywords_kmeans_sports), len(keywords_kmeans_technology))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formación del glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:15.037397Z",
     "start_time": "2020-12-15T21:28:15.033396Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_relevant_keywords(d1, d2, d3):\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    return deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:15.045396Z",
     "start_time": "2020-12-15T21:28:15.038398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics ==>  {'desahucio', 'texto', 'penal', 'militar', 'dsn', 'casa', 'militares', 'fuerzas', 'demasiado', 'decisiones', 'utrera', 'vivienda', 'tribunal', 'obligaciones', 'notas', 'navidades', 'sociedad', 'supremo', 'ayuntamiento', 'contenido', 'hora', 'violencias', 'clave', 'azurmendi', 'becerril', 'fiscal', 'manda', 'indicado', 'territorio', 'grupos', 'pnv', 'mal', 'familia', 'comunidades', 'ferial', 'hija', 'ministro', 'municipal', 'izquierda'}\n",
      "Sports ==>  {'hablar', 'temporadas', 'curry', 'goleador', 'demostrar', 'campazzo', 'laprovittola', 'sonny', 'bla', 'candidato', 'alguien', 'saunders', 'pau', 'juego', 'detroit', 'michael jordan', 'esperamos', 'duro', 'grizzlies', 'davis', 'bolt', 'actuales campeones', 'tema', 'all star', 'gasol', 'marc', 'carrera', 'adrian wojnarowski', 'marc gasol', 'estrella', 'warriors', 'distancia', 'partidos', 'favorito', 'reto', 'rehabilitación', 'goles', 'campeones', 'hombre', 'franquicia', 'zapatillas', 'espn', 'leyenda', 'barcelona', 'dellavedova', 'alfredo', 'wolves', 'regaló', 'pelicans', 'decidido', 'informado', 'sonny vaccaro', 'las cosas', 'presidente', 'andrew', 'laso', 'wojnarowski', 'nike', 'lakers', 'real madrid', 'entrenador', 'adrian', 'serbio', 'situaciones', 'sonrisa', 'discurso', 'noruega', 'mcgee', 'dortmund', 'atlético', 'causeur', 'nombre', 'par', 'meta', 'marcador', 'pelearla', 'abdi', 'pasar', 'ala pívot', 'ganar', 'balonmano', 'pistons', 'haaland', 'klay', 'last', 'ademar', 'registros', 'heat', 'entrenador madridista', 'lesión', 'thompson', 'gustaría', 'suns', 'madridista', 'queda', 'vaccaro', 'tiro puerta', 'escolta', 'obligación', 'facu', 'pasa', 'ricky', 'the last', 'goleadores', 'pista', 'carro', 'herning', 'agente libre', 'zapatilla', 'campeonato', 'galos', 'club', 'thunder', 'subastar', 'fichajes', 'raptors', 'jode', 'competir', 'barça', 'jornet', 'dance', 'pongan', 'orleans', 'sergio', 'orgulloso', 'jugador'}\n",
      "Health ==>  {'contagios', 'sal', 'país vasco', 'tasa', 'masa', 'higiene', 'incidencia', 'españa', 'medicamentos', 'lunes', 'alergia', 'fallecidos', 'ataque', 'expertos', 'fruta', 'asturias', 'elemento', 'notificados', 'sanidad', 'dosis', 'investigadores estadounidenses', 'castilla', 'ensayo', 'realiza', 'positivos', 'suicidios', 'placebo', 'voluntarios', 'donante', 'hiperinmune', 'miocardio', 'mes', 'plato', 'plasma', 'salud mental', 'león', 'alimentaria', 'placa', 'personalidad', 'escuela', 'pecho', 'microbios', 'belleza', 'vasco', 'tuberculosis', 'enfermera', 'suicidio', 'redes', 'malo'}\n",
      "Technology ==>  {'texas', 'co₂', 'modernización', 'zataca', 'sonido', 'lunar', 'eventos', 'normas', 'marketing', 'popular', 'modelo conocido', 'helicópteros', 'objetos lanzados', 'programa', 'ccus', 'aplicaciones', 'versión', 'agencia espacial', 'snapchat', 'foxtrot', 'musk', 'ribas', 'digital', 'señalado', 'lunares', 'frías', 'destruido', 'tirar', 'tuit', 'prueba', 'idealmente', 'claves', 'camino', 'sofia', 'carretera', 'accesos', 'ejército', 'ayudas', 'compartir', 'pretende', 'artemis', 'registrados', 'mejoras', 'hora', 'lanzados', 'navarra', 'combustible', 'web', 'bloque', 'auriculares', 'prohíbe', 'eei', 'acuerdos', 'herramienta', 'espacio', 'versiones', 'cohete', 'basura', 'identificados', 'chinook', 'ejemplares', 'rotores', 'webs', 'combustibles', 'explotó', 'bridenstine', 'tuits', 'spacex', 'productividad', 'exploración', 'agua molecular', 'superficie lunar', 'transportará', 'superficie', 'aterrizaje', 'cara', 'twitter', 'depósitos', 'respuestas', 'cada vez', 'rotor', 'helicóptero', 'carreteras', 'capacidades', 'trampas', 'das', 'online', 'espacial', 'secreto', 'molecular', 'cree', 'marte', 'nasa', 'cohete destruido', 'hielo', 'app', 'starship', 'emisiones', 'redes sociales', 'trampas frías', 'nave', 'evento', 'luna', 'veridas', 'aeroespacial', 'operaciones', 'gracias', 'tierra', 'nano', 'objetos', 'seguridad', 'apps', 'astronautas', 'boeing', 'longitud', 'modelo'}\n"
     ]
    }
   ],
   "source": [
    "relevant_keywords_politics = check_relevant_keywords(keywords_kmeans_politics, keywords_gensim_politics, keywords_tfidf_politics)\n",
    "relevant_keywords_health = check_relevant_keywords(keywords_kmeans_health, keywords_gensim_health, keywords_tfidf_health)\n",
    "relevant_keywords_sports = check_relevant_keywords(keywords_kmeans_sports, keywords_gensim_sports, keywords_tfidf_sports)\n",
    "relevant_keywords_technology = check_relevant_keywords(keywords_kmeans_technology, keywords_gensim_technology, keywords_tfidf_technology)\n",
    "\n",
    "print(\"Politics ==> \", relevant_keywords_politics)\n",
    "print(\"Sports ==> \", relevant_keywords_sports)\n",
    "print(\"Health ==> \", relevant_keywords_health)\n",
    "print(\"Technology ==> \", relevant_keywords_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:15.052396Z",
     "start_time": "2020-12-15T21:28:15.048396Z"
    }
   },
   "outputs": [],
   "source": [
    "path_keys_health = \"../keywords/keys_health.txt\"\n",
    "path_keys_sports = \"../keywords/keys_sports.txt\"\n",
    "path_keys_politics = \"../keywords/keys_politics.txt\"\n",
    "path_keys_technology = \"../keywords/keys_technology.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T21:28:15.063397Z",
     "start_time": "2020-12-15T21:28:15.053395Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_health = [key.strip() for key in open(path_keys_health, encoding=\"utf-8\").readlines()]\n",
    "keys_sports = [key.strip() for key in open(path_keys_sports, encoding=\"utf-8\").readlines()]\n",
    "keys_politics = [key.strip() for key in open(path_keys_politics, encoding=\"utf-8\").readlines()]\n",
    "keys_technology = [key.strip() for key in open(path_keys_technology, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "keys_dic = {-1: \"unknown\", 0: \"health\", 1: \"sports\", 2: \"politics\", 3: \"technology\"}\n",
    "inverted_keys_dic = {\"unknown\": -1, \"health\": 0, \"sports\": 1, \"politics\": 2, \"technology\": 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramas de test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:11.681Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data[\"bigrams\"] = test_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "test_data[\"tokens + bigrams\"] = test_data[\"tokens\"] + test_data[\"bigrams\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.015Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics, keys_technology]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.017Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(glossary for glossary in glossaries)\n",
    "dictionary.save('keys.dict')  # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.018Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, docs, dictionary):\n",
    "        self.docs = docs\n",
    "        self.dict = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dict.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.019Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = MyCorpus(glossaries, dictionary)\n",
    "corpora.MmCorpus.serialize(\"keys.mm\", bow, metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.020Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "index = Similarity(index_temp, bow, num_features=len(dictionary))  # create index\n",
    "index.save(\"keys.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.022Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tfidf = models.TfidfModel(bow,smartirs=\"lpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.023Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_document_tfidf(model, dictionary, bow, index, documents, i, verbose = False):\n",
    "    \"\"\"\n",
    "    Given a specific document, computes the ranking of the classes and returns the current class, \n",
    "    the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    document = documents.iloc[i]\n",
    "    pq = document[\"tokens + bigrams\"]\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = model[vq]\n",
    "    sim = index[qtfidf]\n",
    "\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "            \n",
    "    \n",
    "    return [i, get_info_document(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.024Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.sum(sim) == 0.0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        tfidf_scores = np.array(sim)\n",
    "        probabilities = tfidf_scores / np.sum(tfidf_scores)\n",
    "    \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.025Z"
    }
   },
   "outputs": [],
   "source": [
    "classify_document_tfidf(model_tfidf, dictionary, bow, index, test_data, 110, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.027Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_tfidf(function, model, dictionary, bow, index, data):\n",
    "    def classify(doc_i):\n",
    "        return function(model, dictionary, bow, index, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.200Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics, keys_technology]\n",
    "model_w2v = models.Word2Vec(sentences = glossaries, window = 5, workers = 12, min_count = 1, seed=50)\n",
    "\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.201Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_min(model):\n",
    "    vocab = model.wv.vocab\n",
    "    \n",
    "    maxs = []\n",
    "    mins = []\n",
    "    \n",
    "    for key in vocab:\n",
    "        maxs.append(max(model.wv[key]))\n",
    "        mins.append(min(model.wv[key]))\n",
    "    return max(maxs), min(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.202Z"
    }
   },
   "outputs": [],
   "source": [
    "MAXI, MINI = get_max_min(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.204Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_from_document(model, document):\n",
    "    embeddings = []\n",
    "    \n",
    "    for word in document:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else: # no está en el vocab\n",
    "            embeddings.append(np.random.uniform(low = MINI, high = MAXI, size = 100))\n",
    "    \n",
    "    return np.mean(embeddings, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.205Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries_vector = [get_embeddings_from_document(model_w2v, glossary) for glossary in glossaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.206Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document_w2v(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(sim) == 0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        w2v_scores = np.array(sim, dtype = \"float32\")\n",
    "        probabilities = (w2v_scores - w2v_scores.min()) / (w2v_scores.max() - w2v_scores.min()) \n",
    "        probabilities /= np.sum(probabilities)\n",
    "\n",
    "        \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.207Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_w2v(glossaries_vector, model, documents, i, verbose = False):\n",
    "    document = documents.iloc[i]\n",
    "    doc_vector = get_embeddings_from_document(model, document[\"tokens + bigrams\"])\n",
    "\n",
    "    ranking = [[i, cosine_similarity(np.array(doc_vector).reshape(1,-1), np.array(glossary).reshape(1,-1)).item()] \n",
    "               for i, glossary in enumerate(glossaries_vector)]\n",
    "    \n",
    "    sim = [rank[1] for rank in ranking] \n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, get_info_document_w2v(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.209Z"
    }
   },
   "outputs": [],
   "source": [
    "classify_doc_w2v(glossaries_vector, model_w2v, test_data, 118, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.211Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_w2v(function, glossaries_vector, model, data):\n",
    "    def classify(doc_i):\n",
    "        return function(glossaries_vector, model, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.390Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_bayes(classifier, documents, i, verbose = False):\n",
    "    doc = documents.iloc[i]\n",
    "    #doc = vectorizer.transform([document[\"preprocesado\"]])\n",
    "    \n",
    "    y_pred = classifier.predict_proba([doc[\"preprocesado\"]])[0]\n",
    "    \n",
    "    ranking = [[i,prob] for i,prob in enumerate(y_pred)]\n",
    "    \n",
    "    probabilities = y_pred\n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    current_class = inverted_keys_dic[doc[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(y_pred == 1/4) > 2 or np.count_nonzero(y_pred == 1/2) > 1:\n",
    "        predicted_class = -1\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + doc[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.392Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_bayes(function, clf, documents):\n",
    "    def classify(doc_i):\n",
    "        return function(clf, documents, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.393Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries_joined = [\" \".join(gloss) for gloss in glossaries]\n",
    "\n",
    "X_train = glossaries_joined\n",
    "y_train = [0,1,2, 3]\n",
    "\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                     ('clf', MultinomialNB(alpha=1e-1))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.394Z"
    }
   },
   "outputs": [],
   "source": [
    "classify_doc_bayes(clf, test_data, 110, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.777Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_documents(test_data, classify):\n",
    "    \"\"\"\n",
    "    Classifies the documents given a specific classification function.\n",
    "    \n",
    "    \"\"\"\n",
    "    test_data = test_data.copy()\n",
    "    \n",
    "    infos = [classify(i) for i in range(len(test_data))]\n",
    "    data = fill_test_data(test_data, infos)\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.779Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_test_data(test_data, infos):\n",
    "    \"\"\"\n",
    "    Auxiliary function to fill the dataframe with info about the classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    current_class = pd.Series([info[1][\"current_class\"] for info in infos])\n",
    "    predicted_class = pd.Series([info[1][\"predicted_class\"] for info in infos])\n",
    "    p_health = pd.Series([info[1][\"probabilities\"][0] for info in infos])\n",
    "    p_sports = pd.Series([info[1][\"probabilities\"][1] for info in infos])\n",
    "    p_politics = pd.Series([info[1][\"probabilities\"][2] for info in infos])\n",
    "    p_technology = pd.Series([info[1][\"probabilities\"][3] for info in infos])\n",
    "\n",
    "\n",
    "    data[\"current_class\"] = current_class\n",
    "    data[\"predicted_class\"] = predicted_class\n",
    "    data[\"p_health\"] = p_health\n",
    "    data[\"p_sports\"] = p_sports\n",
    "    data[\"p_politics\"] = p_politics\n",
    "    data[\"p_technology\"] = p_technology\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.780Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename(df):\n",
    "    \"\"\"\n",
    "    Computes the filename for each document based on the performed classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    confidence = \"%.3f\" % df[\"confidence\"]\n",
    "    current_class = df[\"class\"]\n",
    "    predicted_class = df[\"predicted_class_name\"]\n",
    "    correct = current_class == predicted_class\n",
    "    name = df[\"doc_name\"].split(\".\")[0]\n",
    "    \n",
    "    return f\"../classification/{predicted_class}/{confidence}_{name}-{correct}-{current_class}-{predicted_class}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.782Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(path, content):\n",
    "    \"\"\"\n",
    "    Writes a file given its path and content.\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.784Z"
    }
   },
   "outputs": [],
   "source": [
    "def move_files(data, tables = False):\n",
    "    \"\"\"\n",
    "    Moves the files to their corresponding new directory after classification is done.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    classes = [[0, \"p_health\"], [1, \"p_sports\"], [2, \"p_politics\"], [3, \"p_technology\"]]\n",
    "\n",
    "    for cl in classes:\n",
    "        docs = data[data[\"current_class\"] == cl[0]]\n",
    "        docs = docs.sort_values(by=[cl[1]], ascending=False)\n",
    "        docs[\"predicted_class_name\"] = docs[\"predicted_class\"].apply(lambda x : keys_dic[x])\n",
    "        docs[\"confidence\"] = docs[[\"p_health\", \"p_sports\", \"p_politics\"]].max(axis=1)\n",
    "        docs[\"file\"] = docs.apply(lambda x: get_filename(x), axis=1)\n",
    "        docs.apply(lambda row: write_file(row[\"file\"], row[\"text\"]), axis = 1)\n",
    "        if tables:\n",
    "            # tabla para la memoria\n",
    "            docs = docs[[\"doc_name\", \"class\", \"p_health\", \"p_sports\", \"p_politics\", \"p_technology\", \"predicted_class_name\"]]\n",
    "            docs = docs.rename(columns = {\"predicted_class_name\": \"predicted_class\"})\n",
    "            print(docs.to_latex(bold_rows = True, float_format=\"%.2f\", column_format = \"lllllll\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.785Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute(test_data, classification_function, move = True, tables = False):\n",
    "    \"\"\"\n",
    "    General function that classifies the documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    print(\"############################################################\")\n",
    "    print(\"Starting document´s classification...\")\n",
    "    data = classify_documents(data, classification_function)\n",
    "    sleep(1)\n",
    "    print(\"Document´s classification done...\")\n",
    "    if move:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        sleep(1)\n",
    "        print(\"Moving files to the correct directories...\")\n",
    "        move_files(data, tables)\n",
    "        sleep(1)\n",
    "        print(\"Files moved.\")\n",
    "    print(\"############################################################\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:12.998Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = execute(test_data, classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:13.486Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_model(data, model, classify_function):\n",
    "    \"\"\"\n",
    "    Function that evaluates the performance of a specific model.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    print(\"#######################################################\")\n",
    "    print(\"Evaluating \"+ model + \"...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    data = classify_documents(data, classify_function)\n",
    "    \n",
    "    y_true = data[\"current_class\"]\n",
    "    y_pred = data[\"predicted_class\"]\n",
    "    original = len(y_pred)\n",
    "    y_pred = data[data[\"predicted_class\"] != -1][\"predicted_class\"]\n",
    "    unknown = original - len(y_pred)\n",
    "    y_true = y_true.loc[y_pred.index]\n",
    "        \n",
    "    print(classification_report(y_true, y_pred, target_names=[\"health\", \"sports\", \"politics\", \"technology\"]))\n",
    "    print(\"Confusion matrix ==>\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    print(f\"{unknown} documents couldn´t been classified.\")\n",
    "\n",
    "    #precisions = []\n",
    "    #recalls = []\n",
    "\n",
    "    #for i in range(len(cm[0])):\n",
    "     #   name = keys_dic[i]\n",
    "      #  print(f\"Computing statistics about {name}:\")\n",
    "       # recall = cm[i,i] / np.sum(cm[i,:])\n",
    "      #  precision = cm[i,i] / np.sum(cm[:,i])\n",
    "      #  print(f\"\\tPrecision ==> {precision}\")\n",
    "      #  print(f\"\\tRecall ==> {recall}\")\n",
    "\n",
    "      #  precisions.append(precision)\n",
    "      #  recalls.append(recall)\n",
    "\n",
    "    #precisions = np.array(precisions)\n",
    "    #recalls = np.array(recalls)\n",
    "    #f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "    #accuracy = accuracy_score(y_true, y_pred)\n",
    "    #print(f\"Average precision ==> {precisions.mean()}\")\n",
    "    #print(f\"Average recall ==> {recalls.mean()}\")\n",
    "    #print(f\"F1-Score ==> {f1}\")\n",
    "    #print(f\"Overall accuracy score ==> {accuracy}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Model evaluated\")\n",
    "    print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:13.487Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_single_model(test_data, \"tf-idf\", classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:13.489Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_single_model(test_data, \"Word2Vec\", classify_w2v(classify_doc_w2v, glossaries_vector, model_w2v, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T21:28:13.490Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_single_model(test_data, \"Multinomial NB (tfidf)\", classify_bayes(classify_doc_bayes, clf, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
