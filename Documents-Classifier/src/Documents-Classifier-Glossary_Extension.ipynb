{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Índice<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-documentos\" data-toc-modified-id=\"Carga-de-documentos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Carga de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocesado\" data-toc-modified-id=\"Preprocesado-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preprocesado</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bigramas\" data-toc-modified-id=\"Bigramas-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Bigramas</a></span></li></ul></li></ul></li><li><span><a href=\"#Glosario\" data-toc-modified-id=\"Glosario-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-de-keywords\" data-toc-modified-id=\"Extracción-de-keywords-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extracción de keywords</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-propia\" data-toc-modified-id=\"Extracción-propia-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Extracción propia</a></span></li><li><span><a href=\"#Gensim\" data-toc-modified-id=\"Gensim-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Gensim</a></span></li><li><span><a href=\"#Kmeans\" data-toc-modified-id=\"Kmeans-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Kmeans</a></span></li></ul></li><li><span><a href=\"#Formación-del-glosario\" data-toc-modified-id=\"Formación-del-glosario-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Formación del glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automatizado\" data-toc-modified-id=\"Automatizado-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Automatizado</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificador\" data-toc-modified-id=\"Clasificador-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clasificador</a></span><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-glosarios\" data-toc-modified-id=\"Carga-de-glosarios-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Carga de glosarios</a></span></li><li><span><a href=\"#Bigramas-de-test-data\" data-toc-modified-id=\"Bigramas-de-test-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bigramas de test data</a></span></li><li><span><a href=\"#Modelos\" data-toc-modified-id=\"Modelos-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Naive Bayes</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3.3.3.1\"><span class=\"toc-item-num\">3.3.3.1&nbsp;&nbsp;</span>TFIDF</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Clasificación-de-documentos\" data-toc-modified-id=\"Clasificación-de-documentos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clasificación de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li><li><span><a href=\"#Clasificación\" data-toc-modified-id=\"Clasificación-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Clasificación</a></span></li></ul></li><li><span><a href=\"#Evaluación-de-modelos\" data-toc-modified-id=\"Evaluación-de-modelos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluación de modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T14:29:41.228477Z",
     "start_time": "2020-12-13T14:29:37.435800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.summarization import keywords\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# Operatos\n",
    "from operator import itemgetter\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "from spacy_spanish_lemmatizer import SpacyCustomLemmatizer\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# utils\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T14:29:41.233477Z",
     "start_time": "2020-12-13T14:29:41.229477Z"
    }
   },
   "outputs": [],
   "source": [
    "path_health = \"../documents/health\"\n",
    "path_politics = \"../documents/politics\"\n",
    "path_sports = \"../documents/sports\"\n",
    "path_technology = \"../documents/technology\"\n",
    "path_documents = \"../documents\"\n",
    "path_stopwords = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T14:29:41.238476Z",
     "start_time": "2020-12-13T14:29:41.234476Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    return path.split(\"\\\\\")[-1], open(path,encoding='utf-8').read(), path.split(\"\\\\\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T14:29:41.774760Z",
     "start_time": "2020-12-13T14:29:41.239476Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = Parallel(n_jobs = -1)(delayed(load_document)(path) for path in glob.glob(path_documents+\"/*/*.txt\"))\n",
    "documents = pd.DataFrame(documents, columns=[\"doc_name\", \"text\", \"class\"])\n",
    "documents['text'] = documents['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T14:29:41.787760Z",
     "start_time": "2020-12-13T14:29:41.775760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health\n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health\n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health\n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health\n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T14:29:41.803761Z",
     "start_time": "2020-12-13T14:29:41.788760Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\&)|(\\%)|(\\$)|(\\€)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "    \n",
    "#nlp = spacy.load(\"es\")\n",
    "lemmatizer = SpacyCustomLemmatizer()\n",
    "\n",
    "def load_stopwords(path):\n",
    "    return [line.strip() for line in open(path_stopwords, encoding = \"utf-8\").readlines()]\n",
    "\n",
    "STOP_WORDS = set(load_stopwords(path_stopwords))\n",
    "\n",
    "def delete_stop_words(doc):\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return clean\n",
    "\n",
    "def preprocess_document(document):\n",
    "    document = REPLACE_NO_SPACE.sub(NO_SPACE, document.lower())\n",
    "    document = REPLACE_WITH_SPACE.sub(SPACE, document)\n",
    "    # tokens = wordpunct_tokenize(document)\n",
    "    # tokens = delete_proper_nouns(tokens)\n",
    "    return document\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tokens = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in tokens]\n",
    "\n",
    "\n",
    "# TODO: REVISAR ESTO\n",
    "\n",
    "def delete_proper_nouns(tokens):\n",
    "    # Tag the tokens with their type - ie are they nouns or not\n",
    "    lTokens = pos_tag(tokens)\n",
    "    # find all the proper nouns and print them out\n",
    "    lTagDict = findtags('NNP', lTokens)\n",
    "    return [token.lower() for token in tokens if token not in lTagDict]\n",
    "    \n",
    "def findtags(tag_prefix, tagged_text):\n",
    "    \"\"\"\n",
    "    Find tokens matching the specified tag_prefix\n",
    "    \"\"\"\n",
    "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
    "                                  if tag.startswith(tag_prefix))\n",
    "    print(cfd.conditions())\n",
    "    return [list(cfd[tag].keys()) for tag in cfd.conditions()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:37.823Z"
    }
   },
   "outputs": [],
   "source": [
    "documents[\"preprocesado\"] = documents[\"text\"].apply(lambda x: preprocess_document(x))\n",
    "documents[\"tokens\"] = documents[\"preprocesado\"].apply(lambda x: delete_stop_words(x))\n",
    "# documents[\"lematizado\"] = documents[\"preprocesado\"].apply(lambda x: lemmatize(x))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:37.825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "train_health = documents[documents[\"class\"] == \"health\"].iloc[:15]\n",
    "train_politics = documents[documents[\"class\"] == \"politics\"].iloc[:15]\n",
    "train_sports = documents[documents[\"class\"] == \"sports\"].iloc[:15]\n",
    "train_technology = documents[documents[\"class\"] == \"technology\"].iloc[:15]\n",
    "\n",
    "train_data = pd.concat([train_health, train_politics, train_sports, train_technology])\n",
    "print(f\"Training data ==> {len(train_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:37.827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_health = documents[documents[\"class\"] == \"health\"].iloc[15:]\n",
    "test_politics = documents[documents[\"class\"] == \"politics\"].iloc[15:]\n",
    "test_sports = documents[documents[\"class\"] == \"sports\"].iloc[15:]\n",
    "test_technology = documents[documents[\"class\"] == \"technology\"].iloc[15:]\n",
    "\n",
    "test_data = pd.concat([test_health, test_politics, test_sports, test_technology])\n",
    "test_data.reset_index(inplace = True)\n",
    "print(f\"Testing data ==> {len(test_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:37.998Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(documents, threshold):\n",
    "    token_ = [doc.split(\" \") for doc in documents]\n",
    "    bigram = Phrases(token_, min_count=1, threshold=threshold, delimiter=b' ')\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    bigram_token = []\n",
    "    for sent in token_:\n",
    "        for bigram in bigram_phraser[sent]:\n",
    "            if len(bigram.split(\" \")) > 1: # comprobamos que realmente es un bigrama\n",
    "                bigram_token.append(bigram) \n",
    "    return bigram_token\n",
    "           \n",
    "def check_bigram(x, bigrams):\n",
    "    if x.find(\"jamón serrano\") != -1 or x.find(\"jamón\") != -1:\n",
    "        print(x)\n",
    "    return [bigram for bigram in bigrams if x.find(bigram) != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.000Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_sports = get_bigrams(train_sports[\"preprocesado\"].values, 50)\n",
    "bigrams_health = get_bigrams(train_health[\"preprocesado\"].values, 50)\n",
    "bigrams_politics = get_bigrams(train_politics[\"preprocesado\"].values, 50)\n",
    "bigrams_technology = get_bigrams(train_technology[\"preprocesado\"].values, 50)\n",
    "bigrams = get_bigrams(train_data[\"preprocesado\"].values, 50)\n",
    "\n",
    "\n",
    "train_sports[\"bigrams\"] = train_sports[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_sports))\n",
    "train_health[\"bigrams\"] = train_health[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_health))\n",
    "train_politics[\"bigrams\"] = train_politics[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_politics))\n",
    "train_technology[\"bigrams\"] = train_technology[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_technology))\n",
    "\n",
    "train_sports[\"tokens + bigrams\"] = train_sports[\"tokens\"] + train_sports[\"bigrams\"]\n",
    "train_health[\"tokens + bigrams\"] = train_health[\"tokens\"] + train_health[\"bigrams\"]\n",
    "train_politics[\"tokens + bigrams\"] = train_politics[\"tokens\"] + train_politics[\"bigrams\"]\n",
    "train_technology[\"tokens + bigrams\"] = train_technology[\"tokens\"] + train_technology[\"bigrams\"]\n",
    "\n",
    "train_data[\"bigrams\"] = train_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "train_data[\"tokens + bigrams\"] = train_data[\"tokens\"] + train_data[\"bigrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.546Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_dir = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.548Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_tfidf_keywords(df, k):\n",
    "    tokens = df[\"tokens + bigrams\"].values\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    bow = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    bow_tfidf = tfidf[bow]\n",
    "    tfidf_dic = {dictionary.get(id): value for doc in bow_tfidf for id, value in doc}\n",
    "    tfidf_list = [k for k, v in sorted(tfidf_dic.items(), key=lambda item: item[1], reverse = True)]\n",
    "    return tfidf_list[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.549Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health = get_k_tfidf_keywords(train_health, 250)\n",
    "keywords_tfidf_politics = get_k_tfidf_keywords(train_politics, 250)\n",
    "keywords_tfidf_sports = get_k_tfidf_keywords(train_sports, 250)\n",
    "keywords_tfidf_technology = get_k_tfidf_keywords(train_technology, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.551Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(d1, d2, d3, d4):\n",
    "\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d1) & set(d4)\n",
    "    i4 = set(d2) & set(d3)\n",
    "    i5 = set(d3) & set(d4)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2,i3,i4,i5)))\n",
    "    for key in deleted:\n",
    "        try:\n",
    "            d1.remove(key)\n",
    "        except:\n",
    "            print(f\"D1 no tiene {key}\")\n",
    "        try:\n",
    "            d2.remove(key)\n",
    "        except:\n",
    "            print(f\"D2 no tiene {key}\")\n",
    "        try:\n",
    "            d3.remove(key)\n",
    "        except:\n",
    "            print(f\"D3 no tiene {key}\")\n",
    "        try:\n",
    "            d4.remove(key)\n",
    "        except:\n",
    "            print(f\"D4 no tiene {key}\")\n",
    "            \n",
    "    return d1, d2, d3, d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.553Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports, keywords_tfidf_technology = remove_duplicates(\n",
    "    keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports, keywords_tfidf_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.727Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_gensim_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens + bigrams\"].apply(lambda x: \" \".join(x))\n",
    "    data['joined'] = data.joined.astype(str)\n",
    "    data = \" \".join(data[\"joined\"].values)\n",
    "    return [key[0] for key in keywords(data, scores=True, words=k, pos_filter=('NNP', 'JJ', \"NNPS\", \"VB\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.728Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_gensim_health = get_k_gensim_keywords(train_health, 250)\n",
    "keywords_gensim_politics = get_k_gensim_keywords(train_politics, 250)\n",
    "keywords_gensim_sports = get_k_gensim_keywords(train_sports, 250)\n",
    "keywords_gensim_technology = get_k_gensim_keywords(train_technology, 250)\n",
    "\n",
    "keywords_gensim_health, keywords_k_gensim_politics, keywords_k_gensim_sports, keywords_k_gensim_technology = remove_duplicates(\n",
    "    keywords_gensim_health, keywords_gensim_politics, keywords_gensim_sports, keywords_gensim_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.906Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_kmeans_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    k_means_data = data[\"joined\"].values\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(k_means_data)\n",
    "    \n",
    "    model = KMeans(n_clusters=4, init='k-means++', max_iter=1000, n_init=1, random_state = 5, algorithm=\"full\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    keywords_kmeans_politics = [terms[ind] for ind in order_centroids[0, :k]]\n",
    "    keywords_kmeans_technology = [terms[ind] for ind in order_centroids[1, :k]]\n",
    "    keywords_kmeans_sports = [terms[ind] for ind in order_centroids[2, :k]]\n",
    "    keywords_kmeans_health = [terms[ind] for ind in order_centroids[3, :k]]\n",
    "    \n",
    "    return keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:38.908Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology = get_k_kmeans_keywords(\n",
    "    train_data, 200)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health),\n",
    "      len(keywords_kmeans_sports), len(keywords_kmeans_technology))\n",
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywÇords_kmeans_technology = remove_duplicates(\n",
    "    keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology)\n",
    "print(len(keywords_kmeans_politics), len(keywords_kmeans_health),\n",
    "      len(keywords_kmeans_sports), len(keywords_kmeans_technology))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formación del glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:39.260Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_relevant_keywords(d1, d2, d3):\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    return deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:39.261Z"
    }
   },
   "outputs": [],
   "source": [
    "relevant_keywords_politics = check_relevant_keywords(keywords_kmeans_politics, keywords_gensim_politics, keywords_tfidf_politics)\n",
    "relevant_keywords_health = check_relevant_keywords(keywords_kmeans_health, keywords_gensim_health, keywords_tfidf_health)\n",
    "relevant_keywords_sports = check_relevant_keywords(keywords_kmeans_sports, keywords_gensim_sports, keywords_tfidf_sports)\n",
    "relevant_keywords_technology = check_relevant_keywords(keywords_kmeans_technology, keywords_gensim_technology, keywords_tfidf_technology)\n",
    "\n",
    "print(\"Politics ==> \", relevant_keywords_politics)\n",
    "print(\"Sports ==> \", relevant_keywords_sports)\n",
    "print(\"Health ==> \", relevant_keywords_health)\n",
    "print(\"Technology ==> \", relevant_keywords_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:39.648Z"
    }
   },
   "outputs": [],
   "source": [
    "path_keys_health = \"../keywords/keys_health.txt\"\n",
    "path_keys_sports = \"../keywords/keys_sports.txt\"\n",
    "path_keys_politics = \"../keywords/keys_politics.txt\"\n",
    "path_keys_technology = \"../keywords/keys_technology.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:39.650Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_health = [key.strip() for key in open(path_keys_health, encoding=\"utf-8\").readlines()]\n",
    "keys_sports = [key.strip() for key in open(path_keys_sports, encoding=\"utf-8\").readlines()]\n",
    "keys_politics = [key.strip() for key in open(path_keys_politics, encoding=\"utf-8\").readlines()]\n",
    "keys_technology = [key.strip() for key in open(path_keys_technology, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "keys_dic = {-1: \"unknown\", 0: \"health\", 1: \"sports\", 2: \"politics\", 3: \"technology\"}\n",
    "inverted_keys_dic = {\"unknown\": -1, \"health\": 0, \"sports\": 1, \"politics\": 2, \"technology\": 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramas de test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:39.826Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data[\"bigrams\"] = test_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "test_data[\"tokens + bigrams\"] = test_data[\"tokens\"] + test_data[\"bigrams\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.174Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics, keys_technology]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.175Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(glossary for glossary in glossaries)\n",
    "dictionary.save('keys.dict')  # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.176Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, docs, dictionary):\n",
    "        self.docs = docs\n",
    "        self.dict = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dict.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.178Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = MyCorpus(glossaries, dictionary)\n",
    "corpora.MmCorpus.serialize(\"keys.mm\", bow, metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.179Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "index = Similarity(index_temp, bow, num_features=len(dictionary))  # create index\n",
    "index.save(\"keys.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.180Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tfidf = models.TfidfModel(bow,smartirs=\"lpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.182Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_document_tfidf(model, dictionary, bow, index, documents, i, verbose = False):\n",
    "    \"\"\"\n",
    "    Given a specific document, computes the ranking of the classes and returns the current class, \n",
    "    the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    document = documents.iloc[i]\n",
    "    pq = document[\"tokens + bigrams\"]\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = model[vq]\n",
    "    sim = index[qtfidf]\n",
    "\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "            \n",
    "    \n",
    "    return [i, get_info_document(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.183Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.sum(sim) == 0.0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/3, 1/3, 1/3])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        tfidf_scores = np.array(sim)\n",
    "        probabilities = tfidf_scores / np.sum(tfidf_scores)\n",
    "    \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.184Z"
    }
   },
   "outputs": [],
   "source": [
    "classify_document_tfidf(model_tfidf, dictionary, bow, index, test_data, 110, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.186Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_tfidf(function, model, dictionary, bow, index, data):\n",
    "    def classify(doc_i):\n",
    "        return function(model, dictionary, bow, index, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.363Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics, keys_technology]\n",
    "model_w2v = models.Word2Vec(sentences = glossaries, window = 5, workers = 12, min_count = 1, seed=50)\n",
    "\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.364Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_min(model):\n",
    "    vocab = model.wv.vocab\n",
    "    \n",
    "    maxs = []\n",
    "    mins = []\n",
    "    \n",
    "    for key in vocab:\n",
    "        maxs.append(max(model.wv[key]))\n",
    "        mins.append(min(model.wv[key]))\n",
    "    return max(maxs), min(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.366Z"
    }
   },
   "outputs": [],
   "source": [
    "MAXI, MINI = get_max_min(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.367Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_from_document(model, document):\n",
    "    embeddings = []\n",
    "    \n",
    "    for word in document:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else: # no está en el vocab\n",
    "            embeddings.append(np.random.uniform(low = MINI, high = MAXI, size = 100))\n",
    "    \n",
    "    return np.mean(embeddings, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.368Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries_vector = [get_embeddings_from_document(model_w2v, glossary) for glossary in glossaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.369Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document_w2v(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(sim) == 0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/4, 1/4, 1/4])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        w2v_scores = np.array(sim, dtype = \"float32\")\n",
    "        probabilities = (w2v_scores - w2v_scores.min()) / (w2v_scores.max() - w2v_scores.min()) \n",
    "        probabilities /= np.sum(probabilities)\n",
    "\n",
    "        \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.371Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_w2v(glossaries_vector, model, documents, i, verbose = False):\n",
    "    document = documents.iloc[i]\n",
    "    doc_vector = get_embeddings_from_document(model, document[\"tokens + bigrams\"])\n",
    "\n",
    "    ranking = [[i, cosine_similarity(np.array(doc_vector).reshape(1,-1), np.array(glossary).reshape(1,-1)).item()] \n",
    "               for i, glossary in enumerate(glossaries_vector)]\n",
    "    \n",
    "    sim = [rank[1] for rank in ranking] \n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, get_info_document_w2v(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.372Z"
    }
   },
   "outputs": [],
   "source": [
    "classify_doc_w2v(glossaries_vector, model_w2v, test_data, 118, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.374Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_w2v(function, glossaries_vector, model, data):\n",
    "    def classify(doc_i):\n",
    "        return function(glossaries_vector, model, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.559Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_bayes(classifier, documents, i, verbose = False):\n",
    "    doc = documents.iloc[i]\n",
    "    #doc = vectorizer.transform([document[\"preprocesado\"]])\n",
    "    \n",
    "    y_pred = classifier.predict_proba([doc[\"preprocesado\"]])[0]\n",
    "    \n",
    "    ranking = [[i,prob] for i,prob in enumerate(y_pred)]\n",
    "    \n",
    "    probabilities = y_pred\n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    current_class = inverted_keys_dic[doc[\"class\"]]\n",
    "    predicted_class = ranking[0][0]\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + doc[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.560Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_bayes(function, clf, documents):\n",
    "    def classify(doc_i):\n",
    "        return function(clf, documents, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.769Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries_joined = [\" \".join(gloss) for gloss in glossaries]\n",
    "\n",
    "X_train = glossaries_joined\n",
    "y_train = [0,1,2, 3]\n",
    "\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                     ('clf', MultinomialNB(alpha=1e-1))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:40.771Z"
    }
   },
   "outputs": [],
   "source": [
    "classify_doc_bayes(clf, test_data, 110, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.174Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_documents(test_data, classify):\n",
    "    \"\"\"\n",
    "    Classifies the documents given a specific classification function.\n",
    "    \n",
    "    \"\"\"\n",
    "    test_data = test_data.copy()\n",
    "    \n",
    "    infos = [classify(i) for i in range(len(test_data))]\n",
    "    data = fill_test_data(test_data, infos)\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.176Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_test_data(test_data, infos):\n",
    "    \"\"\"\n",
    "    Auxiliary function to fill the dataframe with info about the classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    current_class = pd.Series([info[1][\"current_class\"] for info in infos])\n",
    "    predicted_class = pd.Series([info[1][\"predicted_class\"] for info in infos])\n",
    "    p_health = pd.Series([info[1][\"probabilities\"][0] for info in infos])\n",
    "    p_sports = pd.Series([info[1][\"probabilities\"][1] for info in infos])\n",
    "    p_politics = pd.Series([info[1][\"probabilities\"][2] for info in infos])\n",
    "\n",
    "    data[\"current_class\"] = current_class\n",
    "    data[\"predicted_class\"] = predicted_class\n",
    "    data[\"p_health\"] = p_health\n",
    "    data[\"p_sports\"] = p_sports\n",
    "    data[\"p_politics\"] = p_politics\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.177Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename(df):\n",
    "    \"\"\"\n",
    "    Computes the filename for each document based on the performed classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    confidence = \"%.3f\" % df[\"confidence\"]\n",
    "    current_class = df[\"class\"]\n",
    "    predicted_class = df[\"predicted_class_name\"]\n",
    "    correct = current_class == predicted_class\n",
    "    name = df[\"doc_name\"].split(\".\")[0]\n",
    "    \n",
    "    return f\"../classification/{predicted_class}/{confidence}_{name}-{correct}-{current_class}-{predicted_class}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.178Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(path, content):\n",
    "    \"\"\"\n",
    "    Writes a file given its path and content.\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.180Z"
    }
   },
   "outputs": [],
   "source": [
    "def move_files(data, tables = False):\n",
    "    \"\"\"\n",
    "    Moves the files to their corresponding new directory after classification is done.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    classes = [[0, \"p_health\"], [1, \"p_sports\"], [2, \"p_politics\"]]\n",
    "\n",
    "    for cl in classes:\n",
    "        docs = data[data[\"current_class\"] == cl[0]]\n",
    "        docs = docs.sort_values(by=[cl[1]], ascending=False)\n",
    "        docs[\"predicted_class_name\"] = docs[\"predicted_class\"].apply(lambda x : keys_dic[x])\n",
    "        docs[\"confidence\"] = docs[[\"p_health\", \"p_sports\", \"p_politics\"]].max(axis=1)\n",
    "        docs[\"file\"] = docs.apply(lambda x: get_filename(x), axis=1)\n",
    "        docs.apply(lambda row: write_file(row[\"file\"], row[\"text\"]), axis = 1)\n",
    "        if tables:\n",
    "            # tabla para la memoria\n",
    "            docs = docs[[\"doc_name\", \"class\", \"p_health\", \"p_sports\", \"p_politics\", \"predicted_class_name\"]]\n",
    "            docs = docs.rename(columns = {\"predicted_class_name\": \"predicted_class\"})\n",
    "            print(docs.to_latex(bold_rows = True, float_format=\"%.2f\", column_format = \"llllll\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.181Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute(test_data, classification_function, move = True, tables = False):\n",
    "    \"\"\"\n",
    "    General function that classifies the documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    print(\"############################################################\")\n",
    "    print(\"Starting document´s classification...\")\n",
    "    data = classify_documents(data, classification_function)\n",
    "    sleep(1)\n",
    "    print(\"Document´s classification done...\")\n",
    "    if move:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        sleep(1)\n",
    "        print(\"Moving files to the correct directories...\")\n",
    "        move_files(data, tables)\n",
    "        sleep(1)\n",
    "        print(\"Files moved.\")\n",
    "    print(\"############################################################\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.515Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = execute(test_data, classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.996Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_model(data, model, classify_function):\n",
    "    \"\"\"\n",
    "    Function that evaluates the performance of a specific model.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    print(\"#######################################################\")\n",
    "    print(\"Evaluating \"+ model + \"...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    data = classify_documents(data, classify_function)\n",
    "    \n",
    "    y_true = data[\"current_class\"]\n",
    "    y_pred = data[\"predicted_class\"]\n",
    "    original = len(y_pred)\n",
    "    y_pred = data[data[\"predicted_class\"] != -1][\"predicted_class\"]\n",
    "    unknown = original - len(y_pred)\n",
    "    y_true = y_true.loc[y_pred.index]\n",
    "        \n",
    "    print(classification_report(y_true, y_pred, target_names=[\"health\", \"sports\", \"politics\", \"technology\"]))\n",
    "    print(\"Confusion matrix ==>\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    print(f\"{unknown} documents couldn´t been classified.\")\n",
    "\n",
    "    #precisions = []\n",
    "    #recalls = []\n",
    "\n",
    "    #for i in range(len(cm[0])):\n",
    "     #   name = keys_dic[i]\n",
    "      #  print(f\"Computing statistics about {name}:\")\n",
    "       # recall = cm[i,i] / np.sum(cm[i,:])\n",
    "      #  precision = cm[i,i] / np.sum(cm[:,i])\n",
    "      #  print(f\"\\tPrecision ==> {precision}\")\n",
    "      #  print(f\"\\tRecall ==> {recall}\")\n",
    "\n",
    "      #  precisions.append(precision)\n",
    "      #  recalls.append(recall)\n",
    "\n",
    "    #precisions = np.array(precisions)\n",
    "    #recalls = np.array(recalls)\n",
    "    #f1 = f1_score(y_true, y_pred, average = \"macro\")\n",
    "    #accuracy = accuracy_score(y_true, y_pred)\n",
    "    #print(f\"Average precision ==> {precisions.mean()}\")\n",
    "    #print(f\"Average recall ==> {recalls.mean()}\")\n",
    "    #print(f\"F1-Score ==> {f1}\")\n",
    "    #print(f\"Overall accuracy score ==> {accuracy}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Model evaluated\")\n",
    "    print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.997Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_single_model(test_data, \"tf-idf\", classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:41.999Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_single_model(test_data, \"Word2Vec\", classify_w2v(classify_doc_w2v, glossaries_vector, model_w2v, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-13T14:29:42.000Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_single_model(test_data, \"Multinomial NB (tfidf)\", classify_bayes(classify_doc_bayes, clf, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
