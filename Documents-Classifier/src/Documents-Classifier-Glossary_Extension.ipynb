{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Índice<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-documentos\" data-toc-modified-id=\"Carga-de-documentos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Carga de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocesado\" data-toc-modified-id=\"Preprocesado-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preprocesado</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bigramas\" data-toc-modified-id=\"Bigramas-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Bigramas</a></span></li></ul></li></ul></li><li><span><a href=\"#Glosario\" data-toc-modified-id=\"Glosario-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-de-keywords\" data-toc-modified-id=\"Extracción-de-keywords-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extracción de keywords</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracción-propia\" data-toc-modified-id=\"Extracción-propia-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Extracción propia</a></span></li><li><span><a href=\"#Gensim\" data-toc-modified-id=\"Gensim-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Gensim</a></span></li><li><span><a href=\"#Kmeans\" data-toc-modified-id=\"Kmeans-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Kmeans</a></span></li></ul></li><li><span><a href=\"#Formación-del-glosario\" data-toc-modified-id=\"Formación-del-glosario-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Formación del glosario</a></span><ul class=\"toc-item\"><li><span><a href=\"#Automatizado\" data-toc-modified-id=\"Automatizado-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Automatizado</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificador\" data-toc-modified-id=\"Clasificador-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clasificador</a></span><ul class=\"toc-item\"><li><span><a href=\"#Carga-de-glosarios\" data-toc-modified-id=\"Carga-de-glosarios-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Carga de glosarios</a></span></li><li><span><a href=\"#Bigramas-de-test-data\" data-toc-modified-id=\"Bigramas-de-test-data-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Bigramas de test data</a></span></li><li><span><a href=\"#Modelos\" data-toc-modified-id=\"Modelos-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#TFIDF\" data-toc-modified-id=\"TFIDF-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>TFIDF</a></span></li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Word2Vec</a></span></li><li><span><a href=\"#Naive-Bayes\" data-toc-modified-id=\"Naive-Bayes-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Naive Bayes</a></span></li></ul></li></ul></li><li><span><a href=\"#Clasificación-de-documentos\" data-toc-modified-id=\"Clasificación-de-documentos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Clasificación de documentos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li><li><span><a href=\"#Clasificación\" data-toc-modified-id=\"Clasificación-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Clasificación</a></span></li></ul></li><li><span><a href=\"#Evaluación-de-modelos\" data-toc-modified-id=\"Evaluación-de-modelos-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluación de modelos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Funciones-auxiliares\" data-toc-modified-id=\"Funciones-auxiliares-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Funciones auxiliares</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:29.429580Z",
     "start_time": "2020-12-20T21:33:28.522581Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.summarization import keywords\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "# Operatos\n",
    "from operator import itemgetter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# statistics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# utils\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:29.433583Z",
     "start_time": "2020-12-20T21:33:29.430581Z"
    }
   },
   "outputs": [],
   "source": [
    "path_health = \"../documents/health\"\n",
    "path_politics = \"../documents/politics\"\n",
    "path_sports = \"../documents/sports\"\n",
    "path_technology = \"../documents/technology\"\n",
    "path_documents = \"../documents\"\n",
    "path_stopwords = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:29.439580Z",
     "start_time": "2020-12-20T21:33:29.434581Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_document(path):\n",
    "    return path.split(\"\\\\\")[-1], open(path,encoding='utf-8').read(), path.split(\"\\\\\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:29.927581Z",
     "start_time": "2020-12-20T21:33:29.440581Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = Parallel(n_jobs = -1)(delayed(load_document)(path) for path in glob.glob(path_documents+\"/*/*.txt\"))\n",
    "documents = pd.DataFrame(documents, columns=[\"doc_name\", \"text\", \"class\"])\n",
    "documents['text'] = documents['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:29.939581Z",
     "start_time": "2020-12-20T21:33:29.928581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health\n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health\n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health\n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health\n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:29.945581Z",
     "start_time": "2020-12-20T21:33:29.940580Z"
    }
   },
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\&)|(\\%)|(\\$)|(\\€)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)|(\\⁰)|(\\•)|(\\\\')\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def load_stopwords(path):\n",
    "    return [line.strip() for line in open(path_stopwords, encoding = \"utf-8\").readlines()]\n",
    "\n",
    "STOP_WORDS = set(load_stopwords(path_stopwords))\n",
    "\n",
    "def delete_stop_words(doc):\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token for token in tokens if token not in STOP_WORDS and len(token) > 2]\n",
    "    return clean\n",
    "\n",
    "def preprocess_document(document):\n",
    "    document = REPLACE_NO_SPACE.sub(NO_SPACE, document.lower())\n",
    "    document = REPLACE_WITH_SPACE.sub(SPACE, document)\n",
    "    # tokens = wordpunct_tokenize(document)\n",
    "    # tokens = delete_proper_nouns(tokens)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:30.453580Z",
     "start_time": "2020-12-20T21:33:29.946581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>health_1.txt</td>\n",
       "      <td>Aceptémoslo de una vez: perder peso de manera ...</td>\n",
       "      <td>health</td>\n",
       "      <td>aceptémoslo de una vez perder peso de manera r...</td>\n",
       "      <td>[aceptémoslo, perder, peso, rápida, indolora, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health_10.txt</td>\n",
       "      <td>Sin tiempo para hacer recuento de daños, irrum...</td>\n",
       "      <td>health</td>\n",
       "      <td>sin tiempo para hacer recuento de daños irrump...</td>\n",
       "      <td>[recuento, daños, irrumpe, ola, virus, golpear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health_11.txt</td>\n",
       "      <td>Mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>health</td>\n",
       "      <td>mucha gente intenta mostrar en las redes socia...</td>\n",
       "      <td>[gente, mostrar, redes, sociales, versión, fot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_12.txt</td>\n",
       "      <td>Una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>health</td>\n",
       "      <td>una faceta clave en la frenética lucha global ...</td>\n",
       "      <td>[faceta, clave, frenética, lucha, global, pfiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_13.txt</td>\n",
       "      <td>La curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>health</td>\n",
       "      <td>la curva de contagios de coronavirus se mantie...</td>\n",
       "      <td>[curva, contagios, coronavirus, mantiene, espa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_name                                               text   class  \\\n",
       "0   health_1.txt  Aceptémoslo de una vez: perder peso de manera ...  health   \n",
       "1  health_10.txt  Sin tiempo para hacer recuento de daños, irrum...  health   \n",
       "2  health_11.txt  Mucha gente intenta mostrar en las redes socia...  health   \n",
       "3  health_12.txt  Una faceta clave en la frenética lucha global ...  health   \n",
       "4  health_13.txt  La curva de contagios de coronavirus se mantie...  health   \n",
       "\n",
       "                                        preprocesado  \\\n",
       "0  aceptémoslo de una vez perder peso de manera r...   \n",
       "1  sin tiempo para hacer recuento de daños irrump...   \n",
       "2  mucha gente intenta mostrar en las redes socia...   \n",
       "3  una faceta clave en la frenética lucha global ...   \n",
       "4  la curva de contagios de coronavirus se mantie...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [aceptémoslo, perder, peso, rápida, indolora, ...  \n",
       "1  [recuento, daños, irrumpe, ola, virus, golpear...  \n",
       "2  [gente, mostrar, redes, sociales, versión, fot...  \n",
       "3  [faceta, clave, frenética, lucha, global, pfiz...  \n",
       "4  [curva, contagios, coronavirus, mantiene, espa...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[\"preprocesado\"] = documents[\"text\"].apply(lambda x: preprocess_document(x))\n",
    "documents[\"tokens\"] = documents[\"preprocesado\"].apply(lambda x: delete_stop_words(x))\n",
    "# documents[\"lematizado\"] = documents[\"preprocesado\"].apply(lambda x: lemmatize(x))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:30.463580Z",
     "start_time": "2020-12-20T21:33:30.454581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ==> 60 documents\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_health = documents[documents[\"class\"] == \"health\"].iloc[:15]\n",
    "train_politics = documents[documents[\"class\"] == \"politics\"].iloc[:15]\n",
    "train_sports = documents[documents[\"class\"] == \"sports\"].iloc[:15]\n",
    "train_technology = documents[documents[\"class\"] == \"technology\"].iloc[:15]\n",
    "\n",
    "train_data = pd.concat([train_health, train_politics, train_sports, train_technology])\n",
    "print(f\"Training data ==> {len(train_data)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:30.470583Z",
     "start_time": "2020-12-20T21:33:30.464580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data ==> 140 documents\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_health = documents[documents[\"class\"] == \"health\"].iloc[15:]\n",
    "test_politics = documents[documents[\"class\"] == \"politics\"].iloc[15:]\n",
    "test_sports = documents[documents[\"class\"] == \"sports\"].iloc[15:]\n",
    "test_technology = documents[documents[\"class\"] == \"technology\"].iloc[15:]\n",
    "\n",
    "test_data = pd.concat([test_health, test_politics, test_sports, test_technology])\n",
    "test_data.reset_index(inplace = True)\n",
    "print(f\"Testing data ==> {len(test_data)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:30.475581Z",
     "start_time": "2020-12-20T21:33:30.471581Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(documents, threshold):\n",
    "    token_ = [doc.split(\" \") for doc in documents]\n",
    "    bigram = Phrases(token_, min_count=1, threshold=threshold, delimiter=b' ')\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    bigram_token = []\n",
    "    for sent in token_:\n",
    "        for bigram in bigram_phraser[sent]:\n",
    "            if len(bigram.split(\" \")) > 1: # comprobamos que realmente es un bigrama\n",
    "                bigram_token.append(bigram) \n",
    "    return list(set(bigram_token))\n",
    "           \n",
    "def check_bigram(x, bigrams):\n",
    "    return [bigram for bigram in bigrams if x.find(bigram) != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:31.584582Z",
     "start_time": "2020-12-20T21:33:30.476580Z"
    }
   },
   "outputs": [],
   "source": [
    "bigrams_sports = get_bigrams(train_sports[\"preprocesado\"].values, 50)\n",
    "bigrams_health = get_bigrams(train_health[\"preprocesado\"].values, 50)\n",
    "bigrams_politics = get_bigrams(train_politics[\"preprocesado\"].values, 50)\n",
    "bigrams_technology = get_bigrams(train_technology[\"preprocesado\"].values, 50)\n",
    "bigrams = get_bigrams(train_data[\"preprocesado\"].values, 50)\n",
    "\n",
    "\n",
    "train_sports[\"bigrams\"] = train_sports[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_sports))\n",
    "train_health[\"bigrams\"] = train_health[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_health))\n",
    "train_politics[\"bigrams\"] = train_politics[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_politics))\n",
    "train_technology[\"bigrams\"] = train_technology[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams_technology))\n",
    "\n",
    "train_sports[\"tokens + bigrams\"] = train_sports[\"tokens\"] + train_sports[\"bigrams\"]\n",
    "train_health[\"tokens + bigrams\"] = train_health[\"tokens\"] + train_health[\"bigrams\"]\n",
    "train_politics[\"tokens + bigrams\"] = train_politics[\"tokens\"] + train_politics[\"bigrams\"]\n",
    "train_technology[\"tokens + bigrams\"] = train_technology[\"tokens\"] + train_technology[\"bigrams\"]\n",
    "\n",
    "train_data[\"bigrams\"] = train_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "train_data[\"tokens + bigrams\"] = train_data[\"tokens\"] + train_data[\"bigrams\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:31.588581Z",
     "start_time": "2020-12-20T21:33:31.585581Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords_dir = \"../documents/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:31.592584Z",
     "start_time": "2020-12-20T21:33:31.589581Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_tfidf_keywords(df, k):\n",
    "    tokens = df[\"tokens + bigrams\"].values\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    bow = [dictionary.doc2bow(doc) for doc in tokens]\n",
    "    tfidf = models.TfidfModel(bow)\n",
    "    bow_tfidf = tfidf[bow]\n",
    "    tfidf_dic = {dictionary.get(id): value for doc in bow_tfidf for id, value in doc}\n",
    "    tfidf_list = [k for k, v in sorted(tfidf_dic.items(), key=lambda item: item[1], reverse = True)]\n",
    "    return tfidf_list[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:31.746582Z",
     "start_time": "2020-12-20T21:33:31.593583Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_tfidf_health = get_k_tfidf_keywords(train_health, 250)\n",
    "keywords_tfidf_politics = get_k_tfidf_keywords(train_politics, 250)\n",
    "keywords_tfidf_sports = get_k_tfidf_keywords(train_sports, 250)\n",
    "keywords_tfidf_technology = get_k_tfidf_keywords(train_technology, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:31.751581Z",
     "start_time": "2020-12-20T21:33:31.747580Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(d1, d2, d3, d4):\n",
    "\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d1) & set(d4)\n",
    "    i4 = set(d2) & set(d3)\n",
    "    i5 = set(d3) & set(d4)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2,i3,i4,i5)))\n",
    "    for key in deleted:\n",
    "        try:\n",
    "            d1.remove(key)\n",
    "        except:\n",
    "            print(f\"D1 no tiene {key}\")\n",
    "        try:\n",
    "            d2.remove(key)\n",
    "        except:\n",
    "            print(f\"D2 no tiene {key}\")\n",
    "        try:\n",
    "            d3.remove(key)\n",
    "        except:\n",
    "            print(f\"D3 no tiene {key}\")\n",
    "        try:\n",
    "            d4.remove(key)\n",
    "        except:\n",
    "            print(f\"D4 no tiene {key}\")\n",
    "            \n",
    "    return d1, d2, d3, d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:31.761584Z",
     "start_time": "2020-12-20T21:33:31.752583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2 no tiene copa\n",
      "D4 no tiene copa\n",
      "D2 no tiene new\n",
      "D4 no tiene new\n",
      "D2 no tiene pantalla\n",
      "D3 no tiene pantalla\n",
      "D2 no tiene facebook\n",
      "D3 no tiene facebook\n",
      "D1 no tiene defensa\n",
      "D4 no tiene defensa\n",
      "D1 no tiene nacional\n",
      "D4 no tiene nacional\n",
      "D2 no tiene puedas\n",
      "D3 no tiene puedas\n",
      "D3 no tiene sanidad\n",
      "D4 no tiene sanidad\n",
      "D3 no tiene andalucía\n",
      "D4 no tiene andalucía\n",
      "D1 no tiene gente\n",
      "D2 no tiene capa\n",
      "D3 no tiene capa\n",
      "D3 no tiene atención\n",
      "D4 no tiene atención\n",
      "D1 no tiene récord\n",
      "D3 no tiene sociedad\n",
      "D4 no tiene sociedad\n",
      "D2 no tiene cara\n",
      "D3 no tiene cara\n",
      "D2 no tiene calor\n",
      "D3 no tiene calor\n",
      "D2 no tiene cualquiera\n",
      "D4 no tiene cualquiera\n",
      "D3 no tiene padres\n",
      "D4 no tiene padres\n",
      "D1 no tiene próxima\n",
      "D4 no tiene próxima\n",
      "D2 no tiene energía\n",
      "D4 no tiene energía\n",
      "D2 no tiene redes\n",
      "D4 no tiene redes\n",
      "D2 no tiene actividad\n",
      "D3 no tiene actividad\n",
      "D1 no tiene sede\n",
      "D2 no tiene sede\n",
      "D2 no tiene tipo\n",
      "D4 no tiene tipo\n",
      "D2 no tiene mar\n",
      "D3 no tiene mar\n"
     ]
    }
   ],
   "source": [
    "keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports, keywords_tfidf_technology = remove_duplicates(\n",
    "    keywords_tfidf_health, keywords_tfidf_politics, keywords_tfidf_sports, keywords_tfidf_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:31.766582Z",
     "start_time": "2020-12-20T21:33:31.762583Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_gensim_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens + bigrams\"].apply(lambda x: \" \".join(x))\n",
    "    data['joined'] = data.joined.astype(str)\n",
    "    data = \" \".join(data[\"joined\"].values)\n",
    "    return [key[0] for key in keywords(data, scores=True, words=k, pos_filter=('NNP', 'JJ', \"NNPS\", \"VB\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:33.505582Z",
     "start_time": "2020-12-20T21:33:31.767581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 no tiene baja\n",
      "D2 no tiene baja\n",
      "D2 no tiene niveles\n",
      "D3 no tiene niveles\n",
      "D1 no tiene final\n",
      "D2 no tiene unido\n",
      "D3 no tiene unido\n",
      "D1 no tiene acuerdo\n",
      "D2 no tiene manos\n",
      "D4 no tiene manos\n",
      "D3 no tiene social\n",
      "D4 no tiene social\n",
      "D2 no tiene problemas\n",
      "D3 no tiene problemas\n",
      "D2 no tiene efectos\n",
      "D3 no tiene efectos\n",
      "D2 no tiene productos\n",
      "D3 no tiene productos\n",
      "D3 no tiene persona\n",
      "D1 no tiene principios\n",
      "D4 no tiene principios\n",
      "D1 no tiene base\n",
      "D2 no tiene base\n",
      "D2 no tiene paises\n",
      "D3 no tiene paises\n",
      "D2 no tiene semana\n",
      "D2 no tiene tipos\n",
      "D3 no tiene tipos\n",
      "D1 no tiene relacion\n",
      "D2 no tiene relacion\n",
      "D2 no tiene necesarios\n",
      "D3 no tiene necesarios\n",
      "D3 no tiene decadas\n",
      "D4 no tiene decadas\n",
      "D1 no tiene equipos\n",
      "D2 no tiene equipos\n",
      "D2 no tiene puntos\n",
      "D1 no tiene kilometros\n",
      "D2 no tiene kilometros\n",
      "D1 no tiene jornada\n",
      "D2 no tiene jornada\n",
      "D2 no tiene movil\n",
      "D3 no tiene movil\n",
      "D4 no tiene grupo\n",
      "D3 no tiene tipo\n",
      "D1 no tiene metros\n",
      "D2 no tiene metros\n",
      "D3 no tiene casos\n",
      "D3 no tiene muerte\n",
      "D4 no tiene muerte\n",
      "D2 no tiene estudio\n",
      "D3 no tiene estudio\n",
      "D2 no tiene por\n",
      "D4 no tiene por\n",
      "D1 no tiene situaciones\n",
      "D2 no tiene situaciones\n",
      "D1 no tiene cabeza\n",
      "D4 no tiene cabeza\n",
      "D1 no tiene situacion\n",
      "D2 no tiene noviembre\n",
      "D4 no tiene noviembre\n",
      "D1 no tiene interesante\n",
      "D2 no tiene interesante\n",
      "D2 no tiene prueba\n",
      "D4 no tiene prueba\n",
      "D3 no tiene mujer\n",
      "D4 no tiene mujer\n",
      "D3 no tiene gente\n",
      "D3 no tiene decada\n",
      "D4 no tiene decada\n",
      "D1 no tiene resultados\n",
      "D2 no tiene resultados\n",
      "D3 no tiene padre\n",
      "D4 no tiene padre\n",
      "D2 no tiene del\n",
      "D4 no tiene del\n",
      "D3 no tiene semanas\n",
      "D1 no tiene acciones\n",
      "D4 no tiene acciones\n",
      "D2 no tiene unidos\n",
      "D3 no tiene unidos\n",
      "D2 no tiene moviles\n",
      "D3 no tiene moviles\n",
      "D2 no tiene mar\n",
      "D3 no tiene mar\n",
      "D1 no tiene pensar\n",
      "D2 no tiene pensar\n",
      "D2 no tiene facebook\n",
      "D3 no tiene facebook\n",
      "D2 no tiene especialmente\n",
      "D1 no tiene esta\n",
      "D2 no tiene pruebas\n",
      "D4 no tiene pruebas\n",
      "D3 no tiene mujeres\n",
      "D4 no tiene mujeres\n",
      "D3 no tiene pandemia\n",
      "D2 no tiene puede\n",
      "D3 no tiene puede\n",
      "D2 no tiene formas\n",
      "D3 no tiene formas\n",
      "D4 no tiene general\n",
      "D1 no tiene partidos\n",
      "D4 no tiene partidos\n",
      "D2 no tiene todos\n",
      "D3 no tiene todos\n",
      "D2 no tiene joven\n",
      "D4 no tiene joven\n",
      "D2 no tiene pasos\n",
      "D3 no tiene pasos\n",
      "D4 no tiene dificil\n",
      "D1 no tiene partido\n",
      "D4 no tiene partido\n",
      "D4 no tiene ano\n",
      "D1 no tiene dejar\n",
      "D4 no tiene dejar\n",
      "D1 no tiene plantilla\n",
      "D2 no tiene plantilla\n",
      "D2 no tiene punto\n",
      "D2 no tiene paso\n",
      "D3 no tiene paso\n",
      "D3 no tiene control\n",
      "D2 no tiene mejores\n",
      "D4 no tiene mejores\n",
      "D1 no tiene mercado\n",
      "D2 no tiene mercado\n",
      "D2 no tiene pais\n",
      "D3 no tiene pais\n",
      "D2 no tiene efecto\n",
      "D3 no tiene efecto\n",
      "D2 no tiene resultado\n",
      "D3 no tiene resultado\n",
      "D1 no tiene marcas\n",
      "D2 no tiene marcas\n",
      "D2 no tiene necesario\n",
      "D3 no tiene necesario\n",
      "D2 no tiene agua\n",
      "D3 no tiene agua\n",
      "D2 no tiene factores\n",
      "D3 no tiene factores\n",
      "D2 no tiene jovenes\n",
      "D4 no tiene jovenes\n",
      "D4 no tiene falta\n",
      "D2 no tiene cifra\n",
      "D3 no tiene cifra\n",
      "D2 no tiene lunes\n",
      "D3 no tiene lunes\n",
      "D1 no tiene defensa\n",
      "D4 no tiene defensa\n",
      "D3 no tiene medio\n",
      "D4 no tiene medio\n",
      "D1 no tiene equipo\n",
      "D2 no tiene equipo\n",
      "D1 no tiene publico\n",
      "D3 no tiene sociales\n",
      "D4 no tiene sociales\n",
      "D1 no tiene espana\n",
      "D2 no tiene seguro\n",
      "D4 no tiene seguro\n",
      "D3 no tiene dato\n",
      "D3 no tiene sin\n",
      "D4 no tiene sin\n",
      "D3 no tiene personas\n",
      "D2 no tiene hasta\n",
      "D4 no tiene hasta\n",
      "D4 no tiene numero\n",
      "D2 no tiene dias\n",
      "D3 no tiene medidas\n",
      "D1 no tiene cambios\n",
      "D1 no tiene cambio\n",
      "D2 no tiene cambio\n",
      "D2 no tiene experto\n",
      "D3 no tiene experto\n",
      "D1 no tiene marca\n",
      "D2 no tiene marca\n",
      "D3 no tiene padres\n",
      "D4 no tiene padres\n",
      "D2 no tiene horas\n",
      "D2 no tiene opcion\n",
      "D4 no tiene opcion\n",
      "D2 no tiene opciones\n",
      "D3 no tiene dia\n",
      "D1 no tiene principio\n",
      "D4 no tiene principio\n",
      "D3 no tiene centro\n",
      "D4 no tiene centro\n",
      "D4 no tiene ideas\n",
      "D1 no tiene meses\n",
      "D2 no tiene meses\n",
      "D4 no tiene forma\n",
      "D2 no tiene caso\n",
      "D3 no tiene caso\n"
     ]
    }
   ],
   "source": [
    "keywords_gensim_health = get_k_gensim_keywords(train_health, 250)\n",
    "keywords_gensim_politics = get_k_gensim_keywords(train_politics, 250)\n",
    "keywords_gensim_sports = get_k_gensim_keywords(train_sports, 250)\n",
    "keywords_gensim_technology = get_k_gensim_keywords(train_technology, 250)\n",
    "\n",
    "keywords_gensim_health, keywords_k_gensim_politics, keywords_k_gensim_sports, keywords_k_gensim_technology = remove_duplicates(\n",
    "    keywords_gensim_health, keywords_gensim_politics, keywords_gensim_sports, keywords_gensim_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:33.511581Z",
     "start_time": "2020-12-20T21:33:33.506582Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_k_kmeans_keywords(data, k):\n",
    "    data = data.copy()\n",
    "    data[\"joined\"] = data[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "    k_means_data = data[\"joined\"].values\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(k_means_data)\n",
    "    \n",
    "    model = KMeans(n_clusters=4, init='k-means++', max_iter=1000, n_init=1, random_state = 5, algorithm=\"full\")\n",
    "    model.fit(X)\n",
    "    \n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    keywords_kmeans_politics = [terms[ind] for ind in order_centroids[0, :k]]\n",
    "    keywords_kmeans_technology = [terms[ind] for ind in order_centroids[1, :k]]\n",
    "    keywords_kmeans_sports = [terms[ind] for ind in order_centroids[2, :k]]\n",
    "    keywords_kmeans_health = [terms[ind] for ind in order_centroids[3, :k]]\n",
    "    \n",
    "    return keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:33.694582Z",
     "start_time": "2020-12-20T21:33:33.514580Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D3 no tiene casos\n",
      "D4 no tiene casos\n",
      "D3 no tiene covid\n",
      "D4 no tiene covid\n",
      "D2 no tiene estudio\n",
      "D3 no tiene estudio\n",
      "D2 no tiene equipo\n",
      "D4 no tiene equipo\n",
      "D1 no tiene pablo\n",
      "D4 no tiene pablo\n",
      "D3 no tiene media\n",
      "D4 no tiene media\n",
      "D1 no tiene real\n",
      "D4 no tiene real\n",
      "D3 no tiene pandemia\n",
      "D4 no tiene pandemia\n",
      "D2 no tiene datos\n",
      "D3 no tiene datos\n",
      "D3 no tiene situación\n",
      "D4 no tiene situación\n",
      "D3 no tiene social\n",
      "D4 no tiene social\n",
      "D4 no tiene años\n",
      "D2 no tiene enfermedad\n",
      "D3 no tiene enfermedad\n",
      "D3 no tiene coronavirus\n",
      "D4 no tiene coronavirus\n",
      "D3 no tiene días\n",
      "D4 no tiene días\n",
      "D3 no tiene día\n",
      "D4 no tiene día\n",
      "D3 no tiene personas\n",
      "D4 no tiene personas\n",
      "D2 no tiene hielo\n",
      "D3 no tiene hielo\n",
      "D1 no tiene base\n",
      "D2 no tiene base\n",
      "D1 no tiene partido\n",
      "D4 no tiene partido\n",
      "D1 no tiene volver\n",
      "D2 no tiene volver\n",
      "D1 no tiene madrid\n",
      "D4 no tiene madrid\n",
      "D3 no tiene récord\n",
      "D4 no tiene récord\n",
      "D3 no tiene falta\n",
      "D4 no tiene falta\n",
      "D3 no tiene semana\n",
      "D4 no tiene semana\n",
      "D2 no tiene riesgo\n",
      "D3 no tiene riesgo\n",
      "D2 no tiene compañía\n",
      "D3 no tiene compañía\n",
      "D2 no tiene mercado\n",
      "D4 no tiene mercado\n",
      "D1 no tiene marca\n",
      "D4 no tiene marca\n",
      "D3 no tiene horas\n",
      "D4 no tiene horas\n",
      "D3 no tiene seguridad\n",
      "D4 no tiene seguridad\n",
      "D3 no tiene país\n",
      "D4 no tiene país\n",
      "D2 no tiene año\n",
      "D4 no tiene año\n",
      "D3 no tiene importante\n",
      "D4 no tiene importante\n",
      "D2 no tiene millones\n",
      "D4 no tiene millones\n",
      "D3 no tiene grupo\n",
      "D4 no tiene grupo\n",
      "D3 no tiene vida\n",
      "D4 no tiene vida\n",
      "D3 no tiene claro\n",
      "D4 no tiene claro\n",
      "D2 no tiene unidos\n",
      "D3 no tiene unidos\n",
      "D2 no tiene comida\n",
      "D3 no tiene comida\n",
      "D3 no tiene virus\n",
      "D4 no tiene virus\n",
      "D2 no tiene agua\n",
      "D3 no tiene agua\n",
      "D3 no tiene caso\n",
      "D4 no tiene caso\n"
     ]
    }
   ],
   "source": [
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology = get_k_kmeans_keywords(\n",
    "    train_data, 200)\n",
    "keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology = remove_duplicates(\n",
    "    keywords_kmeans_politics, keywords_kmeans_health, keywords_kmeans_sports, keywords_kmeans_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formación del glosario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:33.703581Z",
     "start_time": "2020-12-20T21:33:33.700581Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_relevant_keywords(d1, d2, d3):\n",
    "    i1 = set(d1) & set(d2)\n",
    "    i2 = set(d1) & set(d3)\n",
    "    i3 = set(d2) & set(d3)\n",
    "    \n",
    "    deleted = set(list(i1.union(i2).union(i3)))\n",
    "    return deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:33.713580Z",
     "start_time": "2020-12-20T21:33:33.705582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics ==>  {'izquierda', 'municipal', 'frases', 'ministro', 'fuerzas', 'contenido', 'jueves', 'violencias', 'dsn', 'utrera', 'vivienda', 'entrada', 'comunidades', 'marzo', 'clave', 'ayuntamiento', 'indicado', 'decisiones', 'libertad', 'familia', 'pnv', 'política', 'navidades', 'desahucio', 'azurmendi', 'obras', 'casa', 'terreno', 'militares', 'obligaciones', 'militar', 'control', 'notas', 'supremo', 'demasiado', 'texto', 'fiscal', 'tribunal', 'ferial', 'hora', 'penal', 'grupos', 'territorio', 'hija', 'sanitario', 'mal'}\n",
      "Sports ==>  {'michael jordan', 'adrian', 'marc', 'demostrar', 'ganar', 'jugador', 'causeur', 'tiro puerta', 'davis', 'escolta', 'juego', 'lucha', 'subastar', 'pau', 'sonrisa', 'thompson', 'ademar', 'alfredo', 'dortmund', 'serbio', 'wojnarowski', 'pelicans', 'pista', 'campeones', 'bla', 'atlético', 'pistons', 'nombre', 'thunder', 'jornet', 'virus', 'correr', 'last', 'presidente', 'pongan', 'situaciones', 'facu', 'reto', 'ricky', 'fichajes', 'detroit', 'distancia', 'tema', 'podio', 'gasol', 'par', 'curry', 'zapatilla', 'esperamos', 'alguien', 'entrenador', 'mcgee', 'sergio', 'anthony', 'galos', 'campazzo', 'dólares', 'registro', 'warriors', 'barcelona', 'vaccaro', 'registros', 'herning', 'campeonato', 'duro', 'dellavedova', 'raptors', 'club', 'zapatillas', 'espn', 'competir', 'carro', 'traspasos', 'discurso', 'nike', 'jode', 'atleta', 'pasa', 'orgulloso', 'rehabilitación', 'jugado', 'orleans', 'champions', 'hombre', 'regaló', 'pasar', 'informado', 'necesario', 'favorito', 'actuales campeones', 'pelearla', 'goleador', 'haaland', 'temporadas', 'klay', 'balonmano', 'bolt', 'carrera', 'obligación', 'heat', 'kolding', 'gustaría', 'liebres', 'noruega', 'laprovittola', 'meta', 'hablar', 'lakers', 'saunders', 'goles', 'suns', 'barça', 'franquicia', 'leyenda', 'lesión', 'marc gasol', 'agente libre', 'grizzlies', 'candidato', 'madridista', 'queda', 'wolves', 'estrella', 'dance', 'sonny', 'laso'}\n",
      "Health ==>  {'alergias', 'darte', 'medicamentos', 'voluntarios', 'belleza', 'enfermera', 'asturias', 'microbios', 'expertos', 'desayuno', 'incidencia', 'tuberculosis', 'hiperinmune', 'fruta', 'sanidad', 'dolor', 'pecho', 'suicidios', 'elemento', 'placa', 'tasa', 'vacuna', 'alergia', 'notificados', 'placebo', 'españa', 'dosis', 'contagios', 'positivos', 'león', 'castilla', 'escuela', 'vasco', 'fallecidos', 'donante', 'ataque', 'miocardio', 'suicidio', 'malo', 'personalidad', 'ensayo', 'sal', 'bebes', 'centro', 'higiene', 'mes', 'infectados', 'plasma', 'aumentar', 'plato'}\n",
      "Technology ==>  {'conocido', 'normas', 'lanzados', 'twitter', 'sofia', 'tuit', 'instagram', 'eventos', 'molecular', 'operaciones', 'destacó', 'bridenstine', 'spacex', 'foxtrot', 'astronautas', 'emisiones', 'ejército', 'camino', 'programa', 'helicóptero', 'basura', 'prohíbe', 'apps', 'tuits', 'ribas', 'rotores', 'tirar', 'helicópteros', 'herramienta', 'hayne', 'trampas', 'online', 'contar', 'claves', 'modelo', 'zataca', 'aterrizaje', 'prueba', 'cohete', 'confinamiento', 'snapchat', 'gracias', 'web', 'marino', 'registrados', 'sonido', 'artemis', 'señalado', 'nave', 'musk', 'seguridad', 'secreto', 'restaurante', 'eei', 'depósitos', 'idealmente', 'aplicaciones', 'luna', 'carretera', 'mejoras', 'versión', 'boeing', 'chinook', 'destruido', 'transportará', 'co₂', 'combustible', 'superficie', 'combustibles', 'tierra', 'capacidades', 'nasa', 'auriculares', 'productividad', 'app', 'marketing', 'texas', 'rotor', 'frías', 'exploración', 'espacial', 'ejemplares', 'hora', 'espaciales', 'ayudas', 'identificados', 'digital', 'aeroespacial', 'webs', 'longitud', 'das', 'starship', 'cree', 'navarra', 'marte', 'lunar', 'objetos', 'hielo', 'veridas', 'toneladas', 'espacio', 'ccus', 'pretende', 'versiones', 'accesible', 'modernización', 'accesos', 'lunares', 'acuerdos', 'explotó', 'compartir', 'evento', 'bloque', 'respuestas', 'carreteras'}\n"
     ]
    }
   ],
   "source": [
    "relevant_keywords_politics = check_relevant_keywords(keywords_kmeans_politics, keywords_gensim_politics, keywords_tfidf_politics)\n",
    "relevant_keywords_health = check_relevant_keywords(keywords_kmeans_health, keywords_gensim_health, keywords_tfidf_health)\n",
    "relevant_keywords_sports = check_relevant_keywords(keywords_kmeans_sports, keywords_gensim_sports, keywords_tfidf_sports)\n",
    "relevant_keywords_technology = check_relevant_keywords(keywords_kmeans_technology, keywords_gensim_technology, keywords_tfidf_technology)\n",
    "\n",
    "print(\"Politics ==> \", relevant_keywords_politics)\n",
    "print(\"Sports ==> \", relevant_keywords_sports)\n",
    "print(\"Health ==> \", relevant_keywords_health)\n",
    "print(\"Technology ==> \", relevant_keywords_technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de glosarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:33.719583Z",
     "start_time": "2020-12-20T21:33:33.714581Z"
    }
   },
   "outputs": [],
   "source": [
    "path_keys_health = \"../keywords/keys_health.txt\"\n",
    "path_keys_sports = \"../keywords/keys_sports.txt\"\n",
    "path_keys_politics = \"../keywords/keys_politics.txt\"\n",
    "path_keys_technology = \"../keywords/keys_technology.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:33.725582Z",
     "start_time": "2020-12-20T21:33:33.720581Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_health = [key.strip() for key in open(path_keys_health, encoding=\"utf-8\").readlines()]\n",
    "keys_sports = [key.strip() for key in open(path_keys_sports, encoding=\"utf-8\").readlines()]\n",
    "keys_politics = [key.strip() for key in open(path_keys_politics, encoding=\"utf-8\").readlines()]\n",
    "keys_technology = [key.strip() for key in open(path_keys_technology, encoding=\"utf-8\").readlines()]\n",
    "\n",
    "keys_dic = {-1: \"unknown\", 0: \"health\", 1: \"sports\", 2: \"politics\", 3: \"technology\"}\n",
    "inverted_keys_dic = {\"unknown\": -1, \"health\": 0, \"sports\": 1, \"politics\": 2, \"technology\": 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigramas de test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.206581Z",
     "start_time": "2020-12-20T21:33:33.726582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>tokens + bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>health_23.txt</td>\n",
       "      <td>Hace unos días Alejandro Díez, madrileño de 24...</td>\n",
       "      <td>health</td>\n",
       "      <td>hace unos días alejandro díez madrileño de  añ...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "      <td>[hace unos, países europeos, me dijo, contar c...</td>\n",
       "      <td>[días, alejandro, díez, madrileño, años, levan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>health_24.txt</td>\n",
       "      <td>Casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>health</td>\n",
       "      <td>casi todos los planes contra el coronavirus un...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "      <td>[qué punto, segunda ola, sobre todo, ni siquie...</td>\n",
       "      <td>[planes, coronavirus, pase, peor, basan, inmun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>health_25.txt</td>\n",
       "      <td>Un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>health</td>\n",
       "      <td>un correcto descanso nocturno no sólo es impor...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "      <td>[más allá, new york, nuestro cuerpo, ha explic...</td>\n",
       "      <td>[correcto, descanso, nocturno, importante, sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>health_26.txt</td>\n",
       "      <td>Los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>health</td>\n",
       "      <td>los problemas de sueño son cada vez más frecue...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "      <td>[sin embargo, hemos hecho, se trata, mucha gen...</td>\n",
       "      <td>[problemas, sueño, frecuentes, sociedad, llega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>health_27.txt</td>\n",
       "      <td>El estrés de la rutina diaria, las preocupacio...</td>\n",
       "      <td>health</td>\n",
       "      <td>el estrés de la rutina diaria las preocupacion...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "      <td>[más allá, estudio publicado, al final, sin em...</td>\n",
       "      <td>[estrés, rutina, diaria, preocupaciones, labor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>195</td>\n",
       "      <td>technology_50.txt</td>\n",
       "      <td>Amnistía Internacional acusa a empresas europe...</td>\n",
       "      <td>technology</td>\n",
       "      <td>amnistía internacional acusa a empresas europe...</td>\n",
       "      <td>[amnistía, internacional, acusa, empresas, eur...</td>\n",
       "      <td>[por ejemplo, cuenta bancaria, sobre todo, uni...</td>\n",
       "      <td>[amnistía, internacional, acusa, empresas, eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>196</td>\n",
       "      <td>technology_6.txt</td>\n",
       "      <td>Hoy, gran parte de las cosas que hacemos diari...</td>\n",
       "      <td>technology</td>\n",
       "      <td>hoy gran parte de las cosas que hacemos diaria...</td>\n",
       "      <td>[diariamente, publicamos, redes, sociales, all...</td>\n",
       "      <td>[por ejemplo, este modo, hasta dónde, te da, s...</td>\n",
       "      <td>[diariamente, publicamos, redes, sociales, all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>197</td>\n",
       "      <td>technology_7.txt</td>\n",
       "      <td>Las fiestas que están por venir serán radicalm...</td>\n",
       "      <td>technology</td>\n",
       "      <td>las fiestas que están por venir serán radicalm...</td>\n",
       "      <td>[fiestas, venir, radicalmente, distintas, cono...</td>\n",
       "      <td>[por debajo, sobre todo, contar con, si quiere...</td>\n",
       "      <td>[fiestas, venir, radicalmente, distintas, cono...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>198</td>\n",
       "      <td>technology_8.txt</td>\n",
       "      <td>La tecnología ha cambiado en poco tiempo nuest...</td>\n",
       "      <td>technology</td>\n",
       "      <td>la tecnología ha cambiado en poco tiempo nuest...</td>\n",
       "      <td>[tecnología, cambiado, forma, comprender, mund...</td>\n",
       "      <td>[ha crecido, están presentes, todas las, cada ...</td>\n",
       "      <td>[tecnología, cambiado, forma, comprender, mund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>199</td>\n",
       "      <td>technology_9.txt</td>\n",
       "      <td>Cada vez se llevan a cabo un mayor número de c...</td>\n",
       "      <td>technology</td>\n",
       "      <td>cada vez se llevan a cabo un mayor número de c...</td>\n",
       "      <td>[llevan, cabo, número, compras, internet, estu...</td>\n",
       "      <td>[respecto al, sobre todo, inteligencia artific...</td>\n",
       "      <td>[llevan, cabo, número, compras, internet, estu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index           doc_name  \\\n",
       "0       15      health_23.txt   \n",
       "1       16      health_24.txt   \n",
       "2       17      health_25.txt   \n",
       "3       18      health_26.txt   \n",
       "4       19      health_27.txt   \n",
       "..     ...                ...   \n",
       "135    195  technology_50.txt   \n",
       "136    196   technology_6.txt   \n",
       "137    197   technology_7.txt   \n",
       "138    198   technology_8.txt   \n",
       "139    199   technology_9.txt   \n",
       "\n",
       "                                                  text       class  \\\n",
       "0    Hace unos días Alejandro Díez, madrileño de 24...      health   \n",
       "1    Casi todos los planes contra el coronavirus un...      health   \n",
       "2    Un correcto descanso nocturno no sólo es impor...      health   \n",
       "3    Los problemas de sueño son cada vez más frecue...      health   \n",
       "4    El estrés de la rutina diaria, las preocupacio...      health   \n",
       "..                                                 ...         ...   \n",
       "135  Amnistía Internacional acusa a empresas europe...  technology   \n",
       "136  Hoy, gran parte de las cosas que hacemos diari...  technology   \n",
       "137  Las fiestas que están por venir serán radicalm...  technology   \n",
       "138  La tecnología ha cambiado en poco tiempo nuest...  technology   \n",
       "139  Cada vez se llevan a cabo un mayor número de c...  technology   \n",
       "\n",
       "                                          preprocesado  \\\n",
       "0    hace unos días alejandro díez madrileño de  añ...   \n",
       "1    casi todos los planes contra el coronavirus un...   \n",
       "2    un correcto descanso nocturno no sólo es impor...   \n",
       "3    los problemas de sueño son cada vez más frecue...   \n",
       "4    el estrés de la rutina diaria las preocupacion...   \n",
       "..                                                 ...   \n",
       "135  amnistía internacional acusa a empresas europe...   \n",
       "136  hoy gran parte de las cosas que hacemos diaria...   \n",
       "137  las fiestas que están por venir serán radicalm...   \n",
       "138  la tecnología ha cambiado en poco tiempo nuest...   \n",
       "139  cada vez se llevan a cabo un mayor número de c...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [días, alejandro, díez, madrileño, años, levan...   \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...   \n",
       "2    [correcto, descanso, nocturno, importante, sen...   \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...   \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...   \n",
       "..                                                 ...   \n",
       "135  [amnistía, internacional, acusa, empresas, eur...   \n",
       "136  [diariamente, publicamos, redes, sociales, all...   \n",
       "137  [fiestas, venir, radicalmente, distintas, cono...   \n",
       "138  [tecnología, cambiado, forma, comprender, mund...   \n",
       "139  [llevan, cabo, número, compras, internet, estu...   \n",
       "\n",
       "                                               bigrams  \\\n",
       "0    [hace unos, países europeos, me dijo, contar c...   \n",
       "1    [qué punto, segunda ola, sobre todo, ni siquie...   \n",
       "2    [más allá, new york, nuestro cuerpo, ha explic...   \n",
       "3    [sin embargo, hemos hecho, se trata, mucha gen...   \n",
       "4    [más allá, estudio publicado, al final, sin em...   \n",
       "..                                                 ...   \n",
       "135  [por ejemplo, cuenta bancaria, sobre todo, uni...   \n",
       "136  [por ejemplo, este modo, hasta dónde, te da, s...   \n",
       "137  [por debajo, sobre todo, contar con, si quiere...   \n",
       "138  [ha crecido, están presentes, todas las, cada ...   \n",
       "139  [respecto al, sobre todo, inteligencia artific...   \n",
       "\n",
       "                                      tokens + bigrams  \n",
       "0    [días, alejandro, díez, madrileño, años, levan...  \n",
       "1    [planes, coronavirus, pase, peor, basan, inmun...  \n",
       "2    [correcto, descanso, nocturno, importante, sen...  \n",
       "3    [problemas, sueño, frecuentes, sociedad, llega...  \n",
       "4    [estrés, rutina, diaria, preocupaciones, labor...  \n",
       "..                                                 ...  \n",
       "135  [amnistía, internacional, acusa, empresas, eur...  \n",
       "136  [diariamente, publicamos, redes, sociales, all...  \n",
       "137  [fiestas, venir, radicalmente, distintas, cono...  \n",
       "138  [tecnología, cambiado, forma, comprender, mund...  \n",
       "139  [llevan, cabo, número, compras, internet, estu...  \n",
       "\n",
       "[140 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"bigrams\"] = test_data[\"preprocesado\"].apply(lambda x: check_bigram(x, bigrams))\n",
    "test_data[\"tokens + bigrams\"] = test_data[\"tokens\"] + test_data[\"bigrams\"]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.209582Z",
     "start_time": "2020-12-20T21:33:34.207581Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics, keys_technology]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.216583Z",
     "start_time": "2020-12-20T21:33:34.209582Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(glossary for glossary in glossaries)\n",
    "dictionary.save('keys.dict')  # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.221581Z",
     "start_time": "2020-12-20T21:33:34.217580Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \n",
    "    def __init__(self, docs, dictionary):\n",
    "        self.docs = docs\n",
    "        self.dict = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.docs:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dict.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.226584Z",
     "start_time": "2020-12-20T21:33:34.222580Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = MyCorpus(glossaries, dictionary)\n",
    "corpora.MmCorpus.serialize(\"keys.mm\", bow, metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.235583Z",
     "start_time": "2020-12-20T21:33:34.227581Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "index_temp = get_tmpfile(\"index\")\n",
    "index = Similarity(index_temp, bow, num_features=len(dictionary))  # create index\n",
    "index.save(\"keys.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.239581Z",
     "start_time": "2020-12-20T21:33:34.236582Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tfidf = models.TfidfModel(bow,smartirs=\"lpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.244581Z",
     "start_time": "2020-12-20T21:33:34.240580Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_document_tfidf(model, dictionary, bow, index, documents, i, verbose = False):\n",
    "    \"\"\"\n",
    "    Given a specific document, computes the ranking of the classes and returns the current class, \n",
    "    the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    document = documents.iloc[i]\n",
    "    pq = document[\"tokens + bigrams\"]\n",
    "    vq = dictionary.doc2bow(pq)\n",
    "    qtfidf = model[vq]\n",
    "    sim = index[qtfidf]\n",
    "\n",
    "    ranking = sorted(enumerate(sim), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "            \n",
    "    \n",
    "    return [i, get_info_document(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.248581Z",
     "start_time": "2020-12-20T21:33:34.245581Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.sum(sim) == 0.0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        tfidf_scores = np.array(sim)\n",
    "        probabilities = tfidf_scores / np.sum(tfidf_scores)\n",
    "    \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.255582Z",
     "start_time": "2020-12-20T21:33:34.249581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> ¿Es la App Store un monopolio? El plan de Apple para evitar otra multa millonaria\n",
      "EEUU, donde Tim Co\n",
      "[technology] ==> 0.182\n",
      "[sports] ==> 0.155\n",
      "[health] ==> 0.144\n",
      "[politics] ==> 0.026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[110,\n",
       " {'current_class': 3,\n",
       "  'predicted_class': 3,\n",
       "  'probabilities': array([0.28439137, 0.30552545, 0.05092091, 0.3591622 ], dtype=float32)}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_document_tfidf(model_tfidf, dictionary, bow, index, test_data, 110, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.258580Z",
     "start_time": "2020-12-20T21:33:34.256580Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_tfidf(function, model, dictionary, bow, index, data):\n",
    "    def classify(doc_i):\n",
    "        return function(model, dictionary, bow, index, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.308580Z",
     "start_time": "2020-12-20T21:33:34.259581Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries = [keys_health, keys_sports, keys_politics, keys_technology]\n",
    "model_w2v = models.Word2Vec(sentences = glossaries, window = 5, workers = 12, min_count = 1, seed=50)\n",
    "\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.313583Z",
     "start_time": "2020-12-20T21:33:34.309580Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max_min(model):\n",
    "    vocab = model.wv.vocab\n",
    "    \n",
    "    maxs = []\n",
    "    mins = []\n",
    "    \n",
    "    for key in vocab:\n",
    "        maxs.append(max(model.wv[key]))\n",
    "        mins.append(min(model.wv[key]))\n",
    "    return max(maxs), min(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.319581Z",
     "start_time": "2020-12-20T21:33:34.314580Z"
    }
   },
   "outputs": [],
   "source": [
    "MAXI, MINI = get_max_min(model_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.324582Z",
     "start_time": "2020-12-20T21:33:34.320580Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_from_document(model, document):\n",
    "    embeddings = []\n",
    "    \n",
    "    for word in document:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else: # no está en el vocab\n",
    "            embeddings.append(np.random.uniform(low = MINI, high = MAXI, size = 100))\n",
    "    \n",
    "    return np.mean(embeddings, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.329581Z",
     "start_time": "2020-12-20T21:33:34.325582Z"
    }
   },
   "outputs": [],
   "source": [
    "glossaries_vector = [get_embeddings_from_document(model_w2v, glossary) for glossary in glossaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.333582Z",
     "start_time": "2020-12-20T21:33:34.330581Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_info_document_w2v(document, ranking, sim):\n",
    "    \"\"\"\n",
    "    Given a ranking of classes, returns the current class, the predicted class and the probabilities for each class.\n",
    "    \n",
    "    \"\"\"\n",
    "    current_class = inverted_keys_dic[document[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(sim) == 0:\n",
    "        predicted_class = -1\n",
    "        probabilities = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "        w2v_scores = np.array(sim, dtype = \"float32\")\n",
    "        probabilities = (w2v_scores - w2v_scores.min()) / (w2v_scores.max() - w2v_scores.min()) \n",
    "        probabilities /= np.sum(probabilities)\n",
    "\n",
    "        \n",
    "    return {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.340581Z",
     "start_time": "2020-12-20T21:33:34.334580Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_w2v(glossaries_vector, model, documents, i, verbose = False):\n",
    "    document = documents.iloc[i]\n",
    "    doc_vector = get_embeddings_from_document(model, document[\"tokens + bigrams\"])\n",
    "\n",
    "    ranking = [[i, cosine_similarity(np.array(doc_vector).reshape(1,-1), np.array(glossary).reshape(1,-1)).item()] \n",
    "               for i, glossary in enumerate(glossaries_vector)]\n",
    "    \n",
    "    sim = [rank[1] for rank in ranking] \n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + document[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, get_info_document_w2v(document, ranking, sim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.352580Z",
     "start_time": "2020-12-20T21:33:34.341581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> El riesgo financiero y sanitario, el aumento de la conectividad y la automatización son algunos de l\n",
      "Scores:\n",
      "[health] ==> 0.136\n",
      "[sports] ==> 0.101\n",
      "[technology] ==> -0.036\n",
      "[politics] ==> -0.073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[118,\n",
       " {'current_class': 3,\n",
       "  'predicted_class': 0,\n",
       "  'probabilities': array([0.4965499 , 0.41370732, 0.        , 0.08974276], dtype=float32)}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_doc_w2v(glossaries_vector, model_w2v, test_data, 118, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.358581Z",
     "start_time": "2020-12-20T21:33:34.355580Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_w2v(function, glossaries_vector, model, data):\n",
    "    def classify(doc_i):\n",
    "        return function(glossaries_vector, model, data, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.365583Z",
     "start_time": "2020-12-20T21:33:34.360580Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_doc_bayes(classifier, documents, i, verbose = False):\n",
    "    doc = documents.iloc[i]\n",
    "    \n",
    "    y_pred = classifier.predict_proba([doc[\"preprocesado\"]])[0]\n",
    "    \n",
    "    ranking = [[i,prob] for i,prob in enumerate(y_pred)]\n",
    "    \n",
    "    probabilities = y_pred\n",
    "    \n",
    "    ranking.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    current_class = inverted_keys_dic[doc[\"class\"]]\n",
    "    \n",
    "    if np.count_nonzero(y_pred == 1/4) > 2 or np.count_nonzero(y_pred == 1/2) > 1:\n",
    "        predicted_class = -1\n",
    "    else:\n",
    "        predicted_class = ranking[0][0]\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Document ==> \" + doc[\"text\"][:100])\n",
    "        print(\"Scores:\")\n",
    "        for doc, score in ranking:\n",
    "            cat = keys_dic[doc]\n",
    "            print(f\"[{cat}] ==> %.3f\" % round(score,3))\n",
    "    \n",
    "    return [i, {\"current_class\": current_class, \"predicted_class\": predicted_class, \n",
    "            \"probabilities\": probabilities}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.370580Z",
     "start_time": "2020-12-20T21:33:34.366581Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_bayes(function, clf, documents):\n",
    "    def classify(doc_i):\n",
    "        return function(clf, documents, doc_i)\n",
    "    return classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.379581Z",
     "start_time": "2020-12-20T21:33:34.371581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None,\n",
       "                             fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossaries_joined = [\" \".join(gloss) for gloss in glossaries]\n",
    "\n",
    "X_train = glossaries_joined\n",
    "y_train = [0,1,2, 3]\n",
    "\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', BernoulliNB(alpha=1e-1))])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.388580Z",
     "start_time": "2020-12-20T21:33:34.380582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ==> ¿Es la App Store un monopolio? El plan de Apple para evitar otra multa millonaria\n",
      "EEUU, donde Tim Co\n",
      "Scores:\n",
      "[technology] ==> 0.917\n",
      "[sports] ==> 0.083\n",
      "[health] ==> 0.000\n",
      "[politics] ==> 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[110,\n",
       " {'current_class': 3,\n",
       "  'predicted_class': 3,\n",
       "  'probabilities': array([6.26056470e-05, 8.33281162e-02, 3.88731812e-10, 9.16609278e-01])}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_doc_bayes(clf, test_data, 110, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.392580Z",
     "start_time": "2020-12-20T21:33:34.389580Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify_documents(test_data, classify):\n",
    "    \"\"\"\n",
    "    Classifies the documents given a specific classification function.\n",
    "    \n",
    "    \"\"\"\n",
    "    test_data = test_data.copy()\n",
    "    \n",
    "    infos = [classify(i) for i in range(len(test_data))]\n",
    "    data = fill_test_data(test_data, infos)\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.399580Z",
     "start_time": "2020-12-20T21:33:34.393581Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_test_data(test_data, infos):\n",
    "    \"\"\"\n",
    "    Auxiliary function to fill the dataframe with info about the classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    current_class = pd.Series([info[1][\"current_class\"] for info in infos])\n",
    "    predicted_class = pd.Series([info[1][\"predicted_class\"] for info in infos])\n",
    "    p_health = pd.Series([info[1][\"probabilities\"][0] for info in infos])\n",
    "    p_sports = pd.Series([info[1][\"probabilities\"][1] for info in infos])\n",
    "    p_politics = pd.Series([info[1][\"probabilities\"][2] for info in infos])\n",
    "    p_technology = pd.Series([info[1][\"probabilities\"][3] for info in infos])\n",
    "\n",
    "\n",
    "    data[\"current_class\"] = current_class\n",
    "    data[\"predicted_class\"] = predicted_class\n",
    "    data[\"p_health\"] = p_health\n",
    "    data[\"p_sports\"] = p_sports\n",
    "    data[\"p_politics\"] = p_politics\n",
    "    data[\"p_technology\"] = p_technology\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.403581Z",
     "start_time": "2020-12-20T21:33:34.400580Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filename(df):\n",
    "    \"\"\"\n",
    "    Computes the filename for each document based on the performed classification.\n",
    "    \n",
    "    \"\"\"\n",
    "    confidence = \"%.3f\" % df[\"confidence\"]\n",
    "    current_class = df[\"class\"]\n",
    "    predicted_class = df[\"predicted_class_name\"]\n",
    "    correct = current_class == predicted_class\n",
    "    name = df[\"doc_name\"].split(\".\")[0]\n",
    "    \n",
    "    return f\"../classification/{predicted_class}/{confidence}_{name}-{correct}-{current_class}-{predicted_class}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.407581Z",
     "start_time": "2020-12-20T21:33:34.404580Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_file(path, content):\n",
    "    \"\"\"\n",
    "    Writes a file given its path and content.\n",
    "    \n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.413581Z",
     "start_time": "2020-12-20T21:33:34.408582Z"
    }
   },
   "outputs": [],
   "source": [
    "def move_files(data, tables = False):\n",
    "    \"\"\"\n",
    "    Moves the files to their corresponding new directory after classification is done.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    classes = [[0, \"p_health\"], [1, \"p_sports\"], [2, \"p_politics\"], [3, \"p_technology\"]]\n",
    "\n",
    "    for cl in classes:\n",
    "        docs = data[data[\"current_class\"] == cl[0]]\n",
    "        docs = docs.sort_values(by=[cl[1]], ascending=False)\n",
    "        docs[\"predicted_class_name\"] = docs[\"predicted_class\"].apply(lambda x : keys_dic[x])\n",
    "        docs[\"confidence\"] = docs[[\"p_health\", \"p_sports\", \"p_politics\"]].max(axis=1)\n",
    "        docs[\"file\"] = docs.apply(lambda x: get_filename(x), axis=1)\n",
    "        docs.apply(lambda row: write_file(row[\"file\"], row[\"text\"]), axis = 1)\n",
    "        if tables:\n",
    "            # tabla para la memoria\n",
    "            docs = docs[[\"doc_name\", \"class\", \"p_health\", \"p_sports\", \"p_politics\", \"p_technology\", \"predicted_class_name\"]]\n",
    "            docs = docs.rename(columns = {\"predicted_class_name\": \"predicted_class\"})\n",
    "            print(docs.to_latex(bold_rows = True, float_format=\"%.2f\", column_format = \"lllllll\", index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:34.417584Z",
     "start_time": "2020-12-20T21:33:34.414581Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute(test_data, classification_function, move = True, tables = False):\n",
    "    \"\"\"\n",
    "    General function that classifies the documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = test_data.copy()\n",
    "    print(\"############################################################\")\n",
    "    print(\"Starting document´s classification...\")\n",
    "    data = classify_documents(data, classification_function)\n",
    "    sleep(1)\n",
    "    print(\"Document´s classification done...\")\n",
    "    if move:\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        sleep(1)\n",
    "        print(\"Moving files to the correct directories...\")\n",
    "        move_files(data, tables)\n",
    "        sleep(1)\n",
    "        print(\"Files moved.\")\n",
    "    print(\"############################################################\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:37.555486Z",
     "start_time": "2020-12-20T21:33:34.418580Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Starting document´s classification...\n",
      "Document´s classification done...\n",
      "-----------------------------------------------------------\n",
      "Moving files to the correct directories...\n",
      "Files moved.\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "data = execute(test_data, classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))\n",
    "#data = execute(test_data, classify_w2v(classify_doc_w2v, glossaries_vector, model_w2v, test_data))\n",
    "#data = execute(test_data, classify_bayes(classify_doc_bayes, clf, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:37.560485Z",
     "start_time": "2020-12-20T21:33:37.556485Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_single_model(data, model, classify_function):\n",
    "    \"\"\"\n",
    "    Function that evaluates the performance of a specific model.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    print(\"#######################################################\")\n",
    "    print(\"Evaluating \"+ model + \"...\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    data = classify_documents(data, classify_function)\n",
    "    \n",
    "    y_true = data[\"current_class\"]\n",
    "    y_pred = data[\"predicted_class\"]\n",
    "    original = len(y_pred)\n",
    "    y_pred = data[data[\"predicted_class\"] != -1][\"predicted_class\"]\n",
    "    unknown = original - len(y_pred)\n",
    "    y_true = y_true.loc[y_pred.index]\n",
    "        \n",
    "    print(classification_report(y_true, y_pred, target_names=[\"health\", \"sports\", \"politics\", \"technology\"]))\n",
    "    print(\"Confusion matrix ==>\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    print(f\"{unknown} documents couldn´t been classified.\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"Model evaluated\")\n",
    "    print(\"#######################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:37.644998Z",
     "start_time": "2020-12-20T21:33:37.561488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating Vector Space Model...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.71      0.86      0.78        35\n",
      "      sports       0.75      0.84      0.79        32\n",
      "    politics       0.97      0.88      0.92        34\n",
      "  technology       0.96      0.74      0.84        35\n",
      "\n",
      "    accuracy                           0.83       136\n",
      "   macro avg       0.85      0.83      0.83       136\n",
      "weighted avg       0.85      0.83      0.83       136\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[30  4  0  1]\n",
      " [ 4 27  1  0]\n",
      " [ 4  0 30  0]\n",
      " [ 4  5  0 26]]\n",
      "4 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_model(test_data, \"Vector Space Model\", classify_tfidf(classify_document_tfidf, model_tfidf, dictionary, bow, index, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:38.028035Z",
     "start_time": "2020-12-20T21:33:37.645999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating Word2Vec...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.53      0.51      0.52        35\n",
      "      sports       0.45      0.57      0.51        35\n",
      "    politics       0.56      0.54      0.55        35\n",
      "  technology       0.57      0.46      0.51        35\n",
      "\n",
      "    accuracy                           0.52       140\n",
      "   macro avg       0.53      0.52      0.52       140\n",
      "weighted avg       0.53      0.52      0.52       140\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[18  7  5  5]\n",
      " [ 4 20  5  6]\n",
      " [ 7  8 19  1]\n",
      " [ 5  9  5 16]]\n",
      "0 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_model(test_data, \"Word2Vec\", classify_w2v(classify_doc_w2v, glossaries_vector, model_w2v, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-20T21:33:38.211046Z",
     "start_time": "2020-12-20T21:33:38.029036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Evaluating Bernoulli NB (tfidf)...\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.89      0.69      0.77        35\n",
      "      sports       0.72      0.80      0.76        35\n",
      "    politics       0.88      0.83      0.85        35\n",
      "  technology       0.73      0.86      0.79        35\n",
      "\n",
      "    accuracy                           0.79       140\n",
      "   macro avg       0.80      0.79      0.79       140\n",
      "weighted avg       0.80      0.79      0.79       140\n",
      "\n",
      "Confusion matrix ==>\n",
      "[[24  5  2  4]\n",
      " [ 0 28  2  5]\n",
      " [ 3  1 29  2]\n",
      " [ 0  5  0 30]]\n",
      "0 documents couldn´t been classified.\n",
      "-------------------------------------------------------\n",
      "Model evaluated\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "evaluate_single_model(test_data, \"Bernoulli NB (tfidf)\", classify_bayes(classify_doc_bayes, clf, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
